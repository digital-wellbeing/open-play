---
title: |
  A longitudinal dataset of multi-platform video game digital trace data and psychological assessments
author:
  - name: Nick Ballou
    corresponding: false
    orcid: 0000-0003-4126-0696
    email: nick@nickballou.com
    affiliations:
      - ref: 1
    # roles:
    #   - conceptualization
    #   - data curation
    #   - methodology
    #   - formal analysis
    #   - writing

  - name: Tamás Andrei Földes
    corresponding: false
    orcid: 0000-0002-0623-9149
    email: tamas.foldes@oii.ox.ac.uk
    affiliations:
      - ref: 1
    # roles:
    #   - conceptualization
    #   - data curation
    #   - methodology
    #   - formal analysis
    #   - writing

  - name: Matti Vuorre
    corresponding: false
    orcid: 0000-0001-5052-066X
    email: mjvuorre@uvt.nl
    affiliations:
      - ref: 2
      - ref: 1
    roles:
      - conceptualization
      - methodology
      - funding acquisition
      - formal analysis
      - supervision
      - writing
      - editing

  - name: Thomas Hakman
    corresponding: false
    orcid: 0009-0009-8292-2482
    email: nick@nickballou.com
    affiliations:
      - ref: 1
    # roles:
    #   - data curation
    #   - validation
    #   - editing

  - name: Kristoffer Magnusson
    corresponding: false
    orcid: 0000-0003-0713-0556
    affiliations:
      - ref: 3
      - ref: 1
    # roles:
    #   - methodology
    #   - editing

  - name: Andrew K Przybylski
    corresponding: true
    email: andy.przybylski@oii.ox.ac.uk
    orcid: 0000-0001-5547-2185
    affiliations:
      - ref: 1
    # roles:
    #   - conceptualization
    #   - funding acquisition
    #   - project administration
    #   - editing
affiliations:
  - id: 1
    name: University of Oxford
    department: Oxford Internet Institute
  - id: 2
    name: Tilburg University
    department: Department of Social Psychology
  - id: 3
    name: Karolinska Institute
bibliography: references.bib
csl: https://www.zotero.org/styles/apa
keywords: [dataset, video games, well-being, mental health, trace data, attention]
editor_options:
  chunk_output_type: console
  markdown:
    wrap: none
knitr:
  opts_chunk:
    echo: false
    cache: true
    warning: false
    message: false
    fig-width: 6
    fig-asp: 0.618
    out-width: 80%
    fig-align: center
code-links: repo
format:
  html:
    theme:
      light: united
      dark: darkly
    toc: true
    toc-depth: 2
    fontsize: 13pt
    other-links:
      - text: Stage 1 Registered Report
        href: https://osf.io/pb5nu
        icon: file-pdf
  preprint-typst:
    wordcount: true
    theme: jou
    citeproc: true
    mainfont: "Liberation Serif"
    sansfont: "Liberation Sans"
    include-before-body:
      - text: "#show <refs>: set par(hanging-indent: 1.4em)"
  docx: default
---

```{r}
#| label: libraries
#| cache: false

library(tidyverse)
library(lubridate)
library(jsonlite)
library(qualtRics)
library(sjlabelled)
library(hms)
library(openxlsx)
library(scales)
library(ggsankey)
library(ggrepel)
library(ggstream)
library(patchwork)
library(tinytable)
library(sparkline)
library(Hmisc)

# Load custom functions
source("R/helpers.R")

set.seed(8675309)
options(scipen = 999)

# Configure tinytable
options(tinytable_tt_theme_placement = "after")
options(tinytable_print_output = NULL)
options(tinytable_tt_digits = 2)

# For HTML, use portable mode to embed images inline (avoids external file dependencies)
# For other formats, we'll exclude plot columns entirely to avoid path issues
if (knitr::is_html_output()) {
  options(tinytable_html_portable = TRUE)
}

theme_set(theme_minimal())
theme_update(
  strip.background = element_rect(fill = "black"),
  strip.text = element_text(color = "white", size = 10),
  panel.grid.minor = element_blank(),
  panel.border = element_rect(
    colour = "black",
    fill = NA,
    linewidth = 1
  ),
)
```

```{r}
#| label: data-read

intake <- read_csv("data/clean/intake.csv.gz")
diary <- read_csv("data/clean/diary.csv.gz")
panel <- read_csv("data/clean/panel.csv.gz")
nintendo <- read_csv("data/clean/nintendo.csv.gz")
steam <- read_csv("data/clean/steam.csv.gz")
xbox <- read_csv("data/clean/xbox.csv.gz")
simon <- read_csv("data/clean/simon.csv.gz")
ethnicity_us <- read_csv("data/clean/ethnicity_us.csv.gz")
ethnicity_uk <- read_csv("data/clean/ethnicity_uk.csv.gz")
```

```{r}
#| label: summary-stats

n_participants <- print_num(length(unique(panel$pid)))
n_daily <- print_num(nrow(diary))
n_biweekly <- print_num(nrow(panel))
n_sessions <- print_num(nrow(nintendo) + nrow(xbox) + nrow(steam))
n_hours <- print_num(
  (sum(nintendo$duration, na.rm = TRUE) +
    sum(xbox$duration, na.rm = TRUE) +
    sum(steam$minutes, na.rm = TRUE)) /
    60
)

n_platforms <- table(
  str_count(
    intake$linked_platforms[!is.na(intake$linked_platforms)],
    ","
  ) +
    1
)

third_party_prop <- paste0(
  print_num((1 - median(intake$nintendo_first_party_prop, na.rm = T)) * 100, 1),
  "%"
)
```

```{r}
#| label: aggregate-session-hourly-daily

# merge with the local time zone offsets from intake
tz_map <- intake |>
  transmute(pid = as.character(pid), off = offset_secs(local_timezone)) |>
  distinct(pid, .keep_all = TRUE)

# aggregate data at each level of granularity (session, hourly, daily)
# --- 1) SESSION-LEVEL (Nintendo + Xbox) ------------------------------------
sessions_all <- bind_rows(
  xbox |> 
    transmute(
      pid = as.character(pid),
      platform = "Xbox",
      start_utc = session_start,
      end_utc = session_end
    ),
  nintendo |> 
    transmute(
      pid = as.character(pid),
      platform = "Nintendo",
      start_utc = session_start,
      end_utc = session_end
    )
) |> 
  left_join(tz_map, by = "pid") |> 
  mutate(
    start_local = start_utc + off,
    end_local = end_utc + off,
    duration_min = as.numeric(difftime(end_utc, start_utc, units = "mins"))
  ) |> 
  filter(
    !is.na(start_utc),
    !is.na(end_utc),
    end_utc > start_utc,
    duration_min >= 1
  )

# --- 2) HOURLY (Nintendo + Xbox expanded) ----------------------------------
# expand sessions into local-hour bins, compute overlap minutes, and add UTC hour
hourly_from_sessions <- sessions_all |> 
  filter(!is.na(off)) |>
  mutate(
    h0_local = floor_date(start_local, "hour"),
    h1_local = floor_date(end_local - seconds(1), "hour"),
    n_hours = as.integer(difftime(h1_local, h0_local, units = "hours")) + 1
  ) |> 
  tidyr::uncount(n_hours, .remove = FALSE, .id = "k") |>
  mutate(
    hour_start_local = h0_local + hours(k - 1),
    minutes = pmax(
      0,
      as.numeric(difftime(
        pmin(end_local, hour_start_local + hours(1)),
        pmax(start_local, hour_start_local),
        units = "mins"
      ))
    ),
    hour_start_utc = hour_start_local - off
  ) |>
  select(pid, platform, hour_start_local, hour_start_utc, minutes) |> 
  group_by(pid, platform, hour_start_local, hour_start_utc) |> 
  summarise(minutes = sum(minutes, na.rm = TRUE), .groups = "drop")

hourly_from_steam <- steam |>
  select(pid, datetime_hour_start, minutes) |>
  mutate(pid = as.character(pid)) |>
  left_join(tz_map, by = "pid") |>
  transmute(
    pid,
    platform = "Steam",
    hour_start_utc = datetime_hour_start,
    hour_start_local = datetime_hour_start + off,
    minutes
  ) |> 
  group_by(pid, platform, hour_start_local, hour_start_utc) |> 
  summarise(minutes = sum(minutes, na.rm = TRUE), .groups = "drop")

hourly_all <- bind_rows(hourly_from_sessions, hourly_from_steam)

# --- 3) DAILY (Nintendo + Xbox + Steam; collapse hourly to days) -----------
daily_all <- hourly_all |>
  mutate(
    day_local = as.Date(hour_start_local),
  ) |>
  group_by(pid, platform, day_local) |>
  summarise(minutes = sum(minutes, na.rm = TRUE), .groups = "drop")

# --- (optional) MOBILE PLACEHOLDERS ----------------------------------------
# When iOS/Android daily datasets are ready, bind_rows() their daily tables here.
# ios_daily <- ios_df |>
#   transmute(pid = as.character(pid), platform = "iOS",
#             day_local = <local_date>, day_utc = <utc_date>, minutes = <minutes>)
# android_daily <- android_df |>
#   transmute(pid = as.character(pid), platform = "Android",
#             day_local = <local_date>, day_utc = <utc_date>, minutes = <minutes>)
# daily_all <- bind_rows(daily_all, ios_daily, android_daily)

telemetry_spans <- hourly_all |>
  group_by(pid, platform) |>
  summarise(
    telemetry_start = min(hour_start_local, na.rm = TRUE),
    telemetry_end = max(hour_start_local, na.rm = TRUE) + hours(1), # end of last hour bin
    week = floor_date(telemetry_end, "week"),
    .groups = "drop"
  )
```

---
abstract: |
  A major limitation to understanding digital technology use, and its potential psychological consequences, is the lack of sufficiently detailed, multidimensional, and accurate data. We present a dataset of `{r} n_participants` individuals' video game play telemetry data from Nintendo Switch, Steam, and Xbox, paired with psychological assessments across multiple dimensions of mental health, motivations, well-being, and cognitive ability. The data were collected over 12 weeks that included fourteen daily assessments, six biweekly surveys, three biweekly cognitive tests, and play telemetry for X months. The data include X hours of video game play across X titles, X responses to X survey instruments, and X attention ability measures to facilitate examining longitudinal associations between play behaviors and psychological functioning. Data and codebook are available under a {{< meta data-license >}} license at {{< meta data-url >}}.
---

# Introduction

Digital trace data---behavioral logs automatically collected by digital devices and online platforms---are necessary to better understand technology use and its psychological and health effects [@BurgessEtAl2024Potential; @Freelon2014Interpretation; @GriffioenEtAl2020improveda]. Self-reports of technology use do not accurately reflect objective technology use [@ParryEtAl2021systematic; @sewallRoleDepressionDiscrepancy2021], and are unsuitable for examining many phenomena of interest, such as seasonal patterns over long temporal horizons; high-frequency behavioral analysis at the level of hours, minutes, or even seconds; historical content analysis; and others due to limited accuracy and temporal resolution. Moreover, when technology use's relations to psychological survey instruments are of interest, digital trace data removes the possibility of  common methods bias.

To combat these issues, digital trace data is increasingly used in studies of smartphone and social media use [e.g. @sewallDoesObjectivelyMeasured2022], but less so in studies of video game play---one of the world's foremost leisure activities [@Ofcom2023Online; @EntertainmentSoftwareAssociation20242024]. Trace data in video games primarily falls under the field of game analytics or data science, but (1) the majority of these studies involve the use of proprietary data that is not publicly shared, and (2) the focus of this field tends to be more on player behavior metrics with clear industry value rather than academic inquiry for the common good.

Where trace data has been used in video games studies, it has largely been limited to a few games [@VuorreEtAl2022Time; @JohannesEtAl2021Video; @LarrieuEtAl2023How; @PerryEtAl2018onlineonly] or single platform [@BallouEtAl2023Gaming; @BallouEtAl2025Perceived]. However, players commonly play many games on multiple platforms. Moreover, the scope of psychological attributes in these studies tends to be limited to e.g. a small set of well-being outcomes.

## Current dataset

Here, we present a longitudinal dataset of digital trace data across multiple gaming platforms consisting of `r n_participants` participants, `r n_daily` daily surveys, `r n_biweekly` surveys, and a total of `r n_hours` of gameplay distributed across `r n_sessions`. Digital trace data was sourced for five distinct platforms—Xbox, Nintendo, Steam, iOS, and Android—through distinct piplines detailed below. The data collection methods were preregistered as part of a Stage 1 registered report (<https://osf.io/pb5nu>).

The dataset is openly available under a {{< meta data-license >}} license at {{< meta data-url >}} for unrestricted reuse. @tbl-overview shows a high-level overview of the dataset.

```{r}
#| label: tbl-overview
#| tbl-cap: "Overview of the dataset and sources."

# ===== tuning knobs =====
CAP_PROB <- 0.99 # winsorize at global 99th percentile (after per-dataset caps)
USE_LOG <- TRUE
THUMB_W <- 400
THUMB_H <- 200
LINEWIDTH <- 1.0

# ---- helpers ----
# Get PIDs of participants who completed at least one survey
survey_pids <- union(
  panel |> mutate(pid = as.character(pid)) |> pull(pid) |> unique(),
  diary |> mutate(pid = as.character(pid)) |> pull(pid) |> unique()
)

avg_days_platform <- function(pltf, pids_filter = NULL) {
  data <- daily_all |>
    filter(platform == pltf)
  
  if (!is.null(pids_filter)) {
    data <- data |> filter(pid %in% pids_filter)
  }
  
  data |>
    group_by(pid) |>
    summarise(days = sum(minutes > 0, na.rm = TRUE), .groups = "drop") |>
    summarise(avg = mean(days, na.rm = TRUE), .groups = "drop") |>
    pull(avg) %||%
    NA_real_
}

total_hours <- function(df) {
  mm <- if ("duration" %in% names(df)) {
    df$duration
  } else if ("minutes" %in% names(df)) {
    df$minutes
  } else {
    NULL
  }
  if (is.null(mm)) NA_real_ else sum(mm, na.rm = TRUE) / 60
}

# Cap events per participant at a per-dataset ceiling
events_per_participant <- function(df, pid, cap = Inf) {
  df |>
    count({{ pid }}, name = "events") |>
    mutate(events = pmin(events, cap)) |>
    pull(events)
}

summ_basic <- function(df, pid, label, avg_days = NA_real_) {
  by_pid <- df |> count({{ pid }}, name = "events")
  tibble(
    `Data type` = label,
    Participants = n_distinct(pull(df, {{ pid }})),
    Events = nrow(df),
    Hours = total_hours(df),
    `Avg events / participants` = mean(by_pid$events, na.rm = TRUE),
    `Avg active days` = avg_days,
    Sparklines = "" # plot_tt() will draw mini-plots here
  )
}

# ---- table rows (order preserved) ----
# Filter telemetry to only include participants who completed at least one survey
steam_survey <- steam |> mutate(pid = as.character(pid)) |> filter(pid %in% survey_pids)
nintendo_survey <- nintendo |> mutate(pid = as.character(pid)) |> filter(pid %in% survey_pids)
xbox_survey <- xbox |> mutate(pid = as.character(pid)) |> filter(pid %in% survey_pids)

overview <- bind_rows(
  summ_basic(intake, pid, "Intake survey", avg_days = 1),
  summ_basic(
    diary,
    pid,
    "Daily surveys",
    avg_days = nrow(diary) / n_distinct(diary$pid)
  ),
  summ_basic(
    panel,
    pid,
    "Biweekly surveys",
    avg_days = nrow(panel) / n_distinct(panel$pid)
  ),
  summ_basic(steam_survey, pid, "Steam", avg_days = avg_days_platform("Steam", survey_pids)),
  summ_basic(
    nintendo_survey,
    pid,
    "Nintendo",
    avg_days = avg_days_platform("Nintendo", survey_pids)
  ),
  summ_basic(xbox_survey, pid, "Xbox", avg_days = avg_days_platform("Xbox", survey_pids))
) |>
  mutate(
    Participants = number(
      Participants,
      accuracy = 1,
      big.mark = ","
    ),
    Events = number(
      Events,
      accuracy = 1,
      big.mark = ",",
      scale_cut = cut_long_scale()
    ),
    Hours = number(
      Hours,
      accuracy = 1,
      big.mark = ",",
      scale_cut = cut_long_scale()
    ),
    `Avg events / participants` = number(
      `Avg events / participants`,
      accuracy = 0.1,
      big.mark = ",",
      scale_cut = cut_long_scale()
    ),
    `Avg active days` = number(
      `Avg active days`,
      accuracy = 0.1,
      big.mark = ",",
      scale_cut = cut_long_scale()
    )
  )

plot_data <- list(
  events_per_participant(intake, pid, cap = 1),
  events_per_participant(diary, pid, cap = 30),
  events_per_participant(panel, pid, cap = 6),
  events_per_participant(steam_survey, pid, cap = Inf),
  events_per_participant(nintendo_survey, pid, cap = Inf),
  events_per_participant(xbox_survey, pid, cap = Inf)
)

# ---- winsorize globally, then log-scale if requested ----
all_events <- unlist(plot_data, use.names = FALSE)
cap_global <- quantile(all_events, CAP_PROB, na.rm = TRUE)
plot_data_w <- lapply(plot_data, \(v) pmin(v, cap_global))

xf <- if (USE_LOG) \(x) log1p(x) else identity
xmax_raw <- max(c(1, cap_global), na.rm = TRUE)
xmax_t <- xf(xmax_raw)

spark_density <- function(d, xmax_t, ...) {
  d <- data.frame(x = xf(d))
  ggplot(d, aes(x = x)) +
    geom_density(
      linewidth = LINEWIDTH,
      fill = "grey60",
      alpha = 0.35,
      adjust = 1
    ) +
    scale_x_continuous(
      limits = c(0, xmax_t),
      expand = c(0, 0),
      breaks = c(0, xmax_t),
      labels = c("0", number(xmax_raw, accuracy = 1, big.mark = ","))
    ) +
    labs(x = NULL, y = NULL) +
    theme_minimal(base_size = 8) +
    theme(
      panel.grid = element_blank(),
      axis.text.y = element_blank(),
      axis.ticks.y = element_blank(),
      plot.margin = margin(1, 1, 1, 1)
    )
}

# If you want console preview reliability:
# options(tinytable_inline_images = TRUE)

tt(
  overview |> select(-Sparklines),
  notes = paste0(
    # "Sparklines: per-participant events. Per-dataset caps applied (see code). ",
    "Winsorized at ",
    round(CAP_PROB * 100),
    "th percentile"
    # if (USE_LOG) "; x-axis uses log1p scale." else "."
  )
) |>
  style_tt(fontsize = 0.8) |>
  format_tt(replace = "")

# Removed until makes sense
# plot_tt(
#   j = which(names(overview) == "Sparklines"),
#   fun = spark_density,
#   data = plot_data_w,
#   xmax_t = xmax_t,
#   width_plot = THUMB_W,
#   height_plot = THUMB_H
# )

```

# Method

## Design

```{r}
#| label: fig-sankey
#| fig-cap: Participant flow during study intake.
#| fig-asp: 0.37

lvl <- c(
  "Screened",
  "Ineligible",
  "No recent trace data",
  "Did not begin surveys",
  "Linked ≥1 account",
  "Recent trace data",
  "Began surveys"
)

node_counts <- intake |>
  transmute(
    stage1 = "Screened",
    stage2 = ifelse(is.na(linked_platforms), "Ineligible", "Linked ≥1 account"),
    stage3 = ifelse(qualified, "Recent trace data", "No recent trace data"),
    stage4 = ifelse(
      pid %in% panel$pid,
      "Began surveys",
      "Did not begin surveys"
    )
  ) |>
  pivot_longer(everything(), names_to = "stage", values_to = "node") |>
  filter(!is.na(node)) |>
  count(node, name = "n")

sankey_data <- intake |>
  transmute(
    Screening = "Screened",
    Linking = ifelse(
      is.na(linked_platforms),
      "Ineligible",
      "Linked ≥1 account"
    ),
    `Account Validation` = ifelse(
      qualified,
      "Recent trace data",
      "No recent trace data"
    ),
    Surveys = ifelse(
      pid %in% panel$pid,
      "Began surveys",
      "Did not begin surveys"
    )
  ) |>
  make_long(Screening, Linking, `Account Validation`, Surveys) |>
  left_join(node_counts, by = "node") |>
  mutate(
    node = factor(node, levels = rev(lvl)),
    next_node = factor(next_node, levels = rev(lvl)),
    # build a plotmath expression: bold("Label")~"\n"~Count
    node_label = sprintf('atop(bold("%s"), "%s")', node, n)
  )

# 2. plot with parse = TRUE
ggplot(
  sankey_data,
  aes(
    x = x,
    next_x = next_x,
    node = node,
    next_node = next_node,
    fill = factor(node),
    label = node_label
  )
) +
  geom_sankey(flow.alpha = .6, node.color = "gray30") +
  # pass parse = TRUE into the label layer
  geom_sankey_label(parse = TRUE, size = 3, color = "white", fill = "gray40") +
  scale_fill_viridis_d(drop = FALSE) +
  theme_sankey() +
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.x = element_blank()
  ) +
  scale_y_continuous(expand = expansion(c(0.2, 0.1))) +
  scale_fill_grey(drop = FALSE, start = 1, end = 0)
```

The study consisted of four stages (@fig-sankey).

### Stage 1: Screening

In the first stage, we screened participants in order to find people aged 18-40 who (1) self-report playing video games, (2) self-report that at least 50% of their total video game play takes place on the platforms included in the study, and (3) were willing to link their gaming accounts to provide digital trace data. We screened participants from two panel sources: PureProfile and Prolific.

Participants were recruited under an initial set of ethnicity‐based quotas designed to mirror the general population’s demographic composition. After we reached approximately 50% of our target sample under quota constraints and found that further quota‐eligible recruits were scarce, we suspended the quotas for the remainder of data collection; all subsequent participants were enrolled on a first‐come, first‐served basis. Final sample characteristic reflect both quota‐driven and open‐enrollment phases (see below).

### Stage 2: Account Linking

Participants who were deemed eligible during screening proceeded directly to an account linking survey wherein they provided details of the gaming platforms they actively use. For UK participants, this includes Nintendo Switch, Steam, Android and iOS. For US participants, this includes the same four alongside Xbox. Details of how participants linked each type of account are shown in @tbl-platforms.

::: {.place arguments='top, scope: "parent", float: true'}

```{r}
#| label: tbl-platforms
#| tbl-cap: "Platform Details"

tibble(
  Platform = c(
    "Nintendo",
    "Xbox (US only)",
    "Steam",
    "iOS",
    "Android"
  ),
  `Data Source` = c(
    "Data-sharing agreements with Nintendo of America (US) and Nintendo of Europe (UK)",
    "Data-sharing agreement with Microsoft",
    "Custom web app (Gameplay.Science)",
    "iOS Screen Time Screenshots",
    "Digital Wellbeing Screenshots"
  ),
  `Account Linking Process` = c(
    "Participants share an identifier contained within a QR code on Nintendo web interface. Nintendo of America/Europe uses this identifier to retrieve gameplay data and share it with the research team.",
    "Participants consent to data sharing by opting in to the study on Xbox Insiders with their Xbox account. Microsoft retrieved gameplay data for all consented accounts, and shares it with the research team in pseudonymized form.",
    "Participants sign up for Gameplay.Science (https://gameplay.science), an o pen-source platform for tracking Steam gameplay. Participants consent to have their gameplay data monitored for the duration of the study. Their Steam ID is authenticated using the official Steam authentication API (OpenID).",
    "In each biweekly survey, participants submitted screenshots from the built-in iOS Screen Time app. These show details of the previous 3 weeks' of gaming app use (what games were played and for how long). Data was extracted using OCR.",
    "In each biweekly survey, participants submitted screenshots from the Digital Screen Time app, if available on their Android OS. These show details of the previous 3 weeks' of phone use (what app categories are used and for how long). Data was extracted using OCR."
  ),
  `Type of Data Collected` = c(
    "Session records (what game was played, at what time, for how long) for 1st party games (games published in whole or in part by Nintendo, but not by third party publishers such as Electronic Arts).",
    "Session records (what game was played, at what time, for how long). The name of the game replaced with a random persistent identifier for all third-party games (i.e., those not published by Xbox Game Studios), but genre(s) and age ratings are shared.",
    "Incremental playtime per game (every hour, the total time spent playing during the previous hour)",
    "Total weekly playtime per game (e.g., 2 hours on game X, 5 hours on game Y)",
    "Total weekly playtime per game (e.g., 2 hours on game X, 5 hours on game Y)"
  )
) |>
  tt(
    escape = TRUE,
    notes = list(
      "a" = list(
        i = 1,
        j = 3,
        text = "See https://accounts.nintendo.com/qrcode."
      ),
      "b" = list(
        i = 1,
        j = 4,
        text = "In previous research, Nintendo-published games accounted for 65% of Switch playtime [\\@BallouEtAl2025Perceived]."
      ),
      "c" = list(
        i = 2,
        j = 3,
        text = "See https://support.xbox.com/en-US/help/account-profile/manage-account/guide-to-insider-program."
      )
    )
  ) |>
  style_tt(fontsize = 0.8)
```

:::

### Stage 3: Account Validation

After players completed the account linking process, we checked each account for evidence of valid gaming—specifically, records of active gameplay sessions on one or more of Steam, Xbox, and Nintendo within the 2 weeks before survey completion. Participants who did not have recent, valid telemetry on any console platform were excluded from the rest of the study.

### Stage 4: Surveys

```{r}
#| label: fig-design
#| fig-cap: "Survey administration schedule across the 12-week study period. Participants completed biweekly surveys (orange) every two weeks, with US participants additionally completing daily surveys (blue) for the first 30 days. Cognitive tests (green) were administered during biweekly surveys at weeks 1, 5, and 9. Gray circles indicate days with no scheduled surveys. Retention percentages show the proportion of baseline participants (N=1,980) who were still active at each measurement week (defined as having completed either a daily diary or biweekly survey at any time after that week)."

knitr::include_graphics("figures/design.svg")
```

Eligible participants were invited to complete 6 waves of biweekly surveys, one every two weeks (@fig-design). US participants were additionally invited to complete daily surveys for 30 days, concurrently with the first biweekly surveys. During waves 1, 3, and 5, a cognitive task was also administered within the biweekly survey.

Daily survey links were sent every day at 2pm local time for the participant and remained available until 3am. Biweekly survey links were sent every second week from the first day of the study at 12pm and remained available for 96 hours.

## Participants

```{r}
#| label: participant-gender

gender_counts <- intake |>
  filter(country != "OTHER") |>
  mutate(
    gender = ifelse(gender %in% c("Man", "Woman"), gender, "Other")
  ) |>
  group_by(qualified) |>
  count(gender) |>
  mutate(
    prop = paste0(100 * round(prop.table(n), 3), "%")
  )

gender_text_all <- with(
  gender_counts |> group_by(gender) |> summarize(n = sum(n), prop = prop[1]),
  paste0(gender, " (", n, ", ", prop, ")", collapse = "; "),
)

gender_text_qualified <- with(
  gender_counts |> filter(qualified),
  paste0(gender, " (", n, ", ", prop, ")", collapse = "; "),
)

```

```{r}
#| label: fig-representative

ethnicity_survey <- intake |>
  mutate(
    ethnicity = case_match(
      ethnicity,
      c("Other ethnic group", "Others", "Some Other Race alone") ~ "Other",
      "White alone" ~ "White",
      c(
        "Black or African American alone",
        "Black, African, Caribbean or Black British",
        "Black or African American alone"
      ) ~
        "Black",
      "Native Hawaiian and Other Pacific Islander alone" ~
        "Native Hawaiian and Other Pacific Islander",
      "American Indian and Alaska Native alone" ~
        "American Indian and Alaska Native",
      c("Asian", "Asian alone", "Asian or Asian British") ~ "Asian",
      c("Mixed or multiple ethnic groups", "Mixed") ~ "Two or More Races",
      .default = ethnicity
    ),
  ) |>
  filter(
    !country %in% c("OTHER", NA) &
      !ethnicity %in% c("Prefer not to say", NA) &
      !(country == "UK" &
        ethnicity %in%
          c(
            "American Indian and Alaska Native alone",
            "Native Hawaiian and Other Pacific Islander alone"
          ))
  ) |>
  group_by(country, ethnicity) |> 
  summarize(
    survey_count = n(),
    gamers_count = sum(!is.na(used_platforms), na.rm = TRUE),
    qualified_count = sum(qualified, na.rm = TRUE),
    survey_moe = 1.96 * sqrt(survey_count * (1 - survey_count / n())) / n(),
    gamers_moe = 1.96 * sqrt(gamers_count * (1 - gamers_count / n())) / n(),
    qualified_moe = 1.96 *
      sqrt(qualified_count * (1 - qualified_count / n())) /
      n()
  ) |>
  group_by(country) |>
  mutate(
    survey_prop = survey_count / sum(survey_count),
    gamers_prop = gamers_count / sum(gamers_count),
    qualified_prop = qualified_count / sum(qualified_count),
  )

# Join the census and survey summaries
ethnicity_combined <- ethnicity_survey |>
  ungroup() |>
  left_join(
    bind_rows(ethnicity_us, ethnicity_uk),
    by = c("ethnicity" = "label", "country")
  ) |>
  pivot_longer(
    cols = c(gen_prop, survey_prop, gamers_prop, qualified_prop),
    names_to = "group",
    values_to = "value"
  ) |>
  filter(!is.na(value) & !is.na(gen_pop)) |>
  mutate(
    group = recode(
      group,
      gen_prop = "General Population",
      survey_prop = "Screened Participants",
      gamers_prop = "Self-reported Video Game Players",
      qualified_prop = "Qualified Participants"
    ),
    group = fct_relevel(
      group,
      c(
        "General Population",
        "Screened Participants",
        "Self-reported Video Game Players",
        "Qualified Participants"
      )
    ),
    ethnicity = fct_relevel(
      ethnicity,
      c(
        "White",
        "Black",
        "Asian",
        "Two or More Races",
        "American Indian and Alaska Native",
        "Native Hawaiian and Other Pacific Islander",
        "Other"
      )
    )
  )

ggplot(ethnicity_combined, aes(x = group, y = value, fill = ethnicity)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(
    x = "",
    y = "Proportion",
    fill = "Ethnicity",
  ) +
  facet_wrap(~country, scales = "free_x") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  geom_text(
    aes(
      label = ifelse(
        value < 0.01,
        "",
        scales::label_percent(accuracy = 1)(value)
      ),
      ,
      family = "serif"
    ),
    position = position_stack(vjust = 0.5),
    color = "white"
  ) +
  scale_fill_viridis_d(
    option = "magma",
    begin = .1,
    end = .8,
    labels = function(x) str_wrap(x, width = 25)
  )


```

Our final sample consists of `r nrow(intake[intake$qualified,])` qualified participants, selected from a pool of `r nrow(intake)` screened participants. Of the `r nrow(intake[intake$qualified,])` with recent telemetry, `r length(unique(panel$pid))` also completed at least one survey.

Our screening sample was roughly representative of the general population of the US and UK by ethnicity (@fig-representative) and gender identity: `r gender_text_all`.

The sample of qualified participants is less representative of the general population, containing more men and non-binary participants: `r gender_text_qualified`, as well as fewer black participants (@fig-representative). However, the demographics of console video game players are not well-understood; it is likely that our sample more closely represents this population than the general population.

## Ethics and Compensation

This study received ethical approval from the Social Sciences and Humanities Inter-Divisional Research Ethics Committee at the University of Oxford (OII_CIA_23_107). All participants provided informed consent at the start of the study, including consent to their data being shared openly for reanalysis.

Prolific participants were paid at a rate of £12/hour for all study components, which equates to: £0.20 for a 1-minute screening, £2 for the 10-minute intake survey (plus £5 for linking at least one account with recent data), £0.80 for each 4-minute daily survey, and £2 for each 10-minute biweekly survey. Participants received £10 bonus payments for completing at least 24 out of 30 daily surveys and/or 5 out of 6 biweekly surveys.

## Demographic measures

We collected the following demographic variables:

-   age
-   gender

For all participants who were eligible based on self-reported video game play (but who did not always successfully linked accounts), we additionally collected the following

-   ethnicity
-   educational attainment
-   employment status
-   height and weight
-   self-identified neurodivergence (e.g., ASD, ADHD, dyslexia)
-   political party affiliation
-   diagnosed neurodivergence
-   marital status
-   caretaking responsibilities (children, family members)
-   postal geography (general area only; first three digits of the five-digit US ZIP Code; UK outward code)

## Self-report measures

### Trait or traitlike measures

```{r}
#| label: tbl-trait-measures
#| tbl-cap: "Trait or traitlike measures assessed only once"

tibble(
  Construct = c(
    "Chronotype",
    "Big 5 Personality",
    "Player Trait Typology",
    "Gaming Disorder Symptoms"
  ),
  Measure = c(
    "Munich Chronotype Questionnaire [\\@RoennebergEtAl2003Life]",
    "BFI-2-XS [\\@SotoJohn2017short]",
    "Trojan Player Typology [\\@KahnEtAl2015trojan]",
    "Gaming Disorder Test [\\@PontesEtAl2019measurement]"
  ),
  `Example Item` = c(
    "*I go to bed at...*",
    "*I am someone who...is compassionate, has a soft heart.*",
    "*It's important to me to play with a tightly knit group.*",
    "*In the past 3 months...I have had difficulties controlling my gaming activity*."
  ),
  `Response Format` = c(
    "Times and numbers of minutes",
    "5-pt Likert scale from 1 (Disagree strongly) to 5 (Agree strongly)",
    "5-pt Likert scale from 1 (Strongly disagree) to 5 (Strongly agree)",
    "5-pt Likert scale from 1 (Never) to 5 (Very often)"
  )
) |>
  tt(
    notes = list(
      "a" = list(
        i = 4,
        j = 1,
        text = "Measured twice, at biweekly waves 1 and 6."
      )
    )
  ) |>
  style_tt(fontsize = 0.8)
```

### Daily measures

```{r}
#| label: tbl-daily
#| tbl-cap: "Self-report measures in the daily surveys"

tibble(
  Construct = c(
    "Basic psychological need satisfaction and frustration - life in general",
    "Basic psychological need satisfaction and frustration - video games",
    "Life satisfaction",
    "Affective valence",
    "Sleep Quality",
    "Stressors",
    "Self-reported displacement"
  ),
  Measure = c(
    "Basic Psychological Need Satisfaction and Frustration Scale [\\@ChenEtAl2015Basic], brief version [\\@MartelaRyan2024Assessing]",
    "Basic Needs in Games scale [\\@BallouEtAl2024Basic], brief session-level version",
    "Cantril Self-anchoring Scale [\\@Cantril1965pattern], daily version",
    "ad hoc",
    "Sleep quality item (Item 9) from Consensus Sleep Diary [\\@CarneyEtAl2012Consensus]",
    "Daily Inventory of Stressful Events [\\@AlmeidaEtAl2002Daily], modified for digital delivery",
    "ad hoc"
  ),
  `Example Item` = c(
    "*In the last 24 hours...I was able to do things I really want and value in life.*",
    "*In my most recent session of X...I felt disappointed with my performance.*",
    "*I was satisfied with my life today.*",
    "How are you feeling right now?",
    "How do you rate the quality of your sleep?",
    "*\\[In the last 24 hours\\], what kinds of stressful event(s) occurred? \\[Participant selects among 7 options, including e.g. argument or disagreement\\]*",
    "*Think back to your most recent gaming session. If you hadn't played a game, what would you most likely have done instead?*"
  ),
  `Response format` = c(
    "7-pt Likert scale from 1 (very strongly disagree) to 7 (very strongly agree)",
    "7-pt Likert scale from 1 (very strongly disagree) to 7 (very strongly agree)",
    "Visual Analogue Scale from 1 (Strongly disagree) to 100 (Strongly agree)",
    "Visual Analogue Scale from 1 (very bad) to 100 (very good)",
    "5-pt Likert scale from 1 (very poor) to 5 (very good)",
    "Yes/No, followed by a 4-pt Likert scale from 1 (Not at all stressful) to 4 (Very stressful)",
    "Open response"
  )
) |>
  tt(escape = TRUE) |>
  style_tt(fontsize = 0.8)
```

### Biweekly measures

```{r}
#| label: tbl-biweekly
#| tbl-cap: "Self-report measures in the biweekly surveys"

tibble(
  Construct = c(
    "General Mental Wellbeing",
    "Depression symptoms",
    "Life satisfaction",
    "Basic psychological need satisfaction and frustration - video games",
    "Subjective displacement"
  ),
  Measure = c(
    "Warwick-Edinburgh Mental Wellbeing Scale [\\@TennantEtAl2007warwickedinburgh]",
    "PROMIS Short Form 8a Adult Depression Scale [\\@PilkonisEtAl2011item]",
    "Cantril Self-anchoring Scale [\\@Cantril1965pattern]",
    "Basic Needs in Games scale [\\@BallouEtAl2024Basic], gaming in general version",
    "ad hoc"
  ),
  `Example Item` = c(
    "*I've been feeling optimistic about the future*",
    "*In the past 7 days...I felt that I had nothing to look forward to.*",
    "*On which step of \\[a ladder from 0 to 10 representing the best possible life\\] would you say you personally feel you stood over the past two weeks?*",
    "*When playing video games during the last 2 weeks...I could play in the way I wanted.*",
    "*Over the last two weeks, to what extent has the time you spend playing video games influenced the following areas of your life? \\[...\\] Work/school performance*"
  ),
  `Response format` = c(
    "5-pt Likert scale from 1 (none of the time) to 5 (all of the time)",
    "5-pt Likert scale from 1 (Never) to 5 (Always)",
    "10-pt unlabeled scale from 0 to 10",
    "7-pt Likert scale from 1 (very strongly disagree) to 7 (very strongly agree)",
    "7-pt Likert scale from 1 (greatly interfered) to 7 (greatly supported)"
  )
) |>
  tt() |>
  style_tt(fontsize = 0.8)
```

### Monthly measures

```{r}
#| label: tbl-monthly
#| tbl-cap: "Self-report measures in alternating biweekly surveys"

tibble(
  Construct = c(
    "Sleep quality",
    "Daytime sleepiness",
    "Harms and benefits of gaming"
  ),
  Measure = c(
    "Pittsburgh Sleep Quality Index [\\@BuysseEtAl1989Pittsburgh]",
    "Epworth Sleepiness Scale [\\@Johns1991New]",
    "2 free text questions"
  ),
  `Example Item` = c(
    "*During the past month, what time have you usually gotten up in the morning?*",
    "*How likely are you to doze off or fall asleep in the following situations, in comparison to feeling just tired? \\[...\\] Watching TV*",
    "Do you feel that gaming is sometimes a problem for you? Please describe."
  ),
  `Response format` = c(
    "Various",
    "4-pt Likert scale from 1 (No chance of dozing) to 4 (High chance of dozing)",
    "Open text"
  )
) |>
  tt() |>
  style_tt(fontsize = 0.8)
```

### Self-reported play

*Social context of play*: Participants reported which types of social play they engaged in during the last 24 hours (single-player games only, multiplayer with real-world friends, multiplayer with online-only friends, multiplayer with strangers). Participants could select more than one option.

*Self-reported Playtime*: In each biweekly survey, participants estimated the time they spent playing games on platforms they had linked during the study (e.g., excluding other platforms such as Playstation) in each of the following periods: last 24 hours, last 7 days, and last 14 days.

*Self-reported recent sessions:* In each biweekly survey, participants reported details of at least 1 and up to 3 of their most recent gaming sessions (game, date, and start/end time).

## Digital Trace Data

As described above, we collected video game play data from five platforms: Xbox, Nintendo Switch, Steam, iOS, and Android (full details in @tbl-platforms). To recap, on Xbox and Nintendo, we have *session-level* data, characterized by the following fields: a game ID (Xbox) or title (Nintendo), a start and end time, and genre(s). On Steam we have *hourly aggregates* - every hour, how much time people spent playing for all games they played in that hour. On iOS and Android, we have *daily aggregates* - every day, how much time people spent playing each game. We describe each platform in more detail below. For concision, we do not repeat the details of @tbl-platforms in here, but direct readers to that table or our supplementary materials for the exact variables in each platform's trace data.

All telemetry timestamps are stored as UTC, but can be converted to the participant's local time using the `local_timezone` variable.

## Attention Control

In study waves 1, 3, and 5 we measured participants' attention control using the Simon Squared task of @burgoyneNatureMeasurementAttention2023, using modified code from @liceralde23squared. Although the original Squared tasks consist of the Simon, Stroop, and Flanker Squared, due to limited participation time we chose to only use the Simon Squared task as it had the greatest factor loading on attention control in the original study [@burgoyneNatureMeasurementAttention2023].

The Simon Squared task is a short and validated measure of attention control that follows the standard Simon task [@simonAuditorySRCompatibility1967] but is completed in about three minutes. Participants see a target arrow pointing either left or right, with response labels "LEFT" and "RIGHT" printed underneath. Participants then must select the response option (e.g. "LEFT") that matches the arrow's direction (e.g. ◀︎). However, the arrow and response options can appear on either side of the screen, and participants must ignore this spatial configuration and attend only to the symbols' meanings.

After reading the instructions, participants practice for 30 seconds with auditory and text feedback for response accuracy. They then see their score from the practice trials, review the instructions again, and are given 90 seconds to gain as many points as possible. Participants gain one point for each correct response, and lose one point for each incorrect response. After the 90 seconds, the number of correct responses minus the number of incorrect responses is the participant's task score. For a complete task description, see @burgoyneNatureMeasurementAttention2023 and Figure 6 therein.

## Time Use

TODO

## Data Quality Checks

We implemented a variety of data quality checks.

1.  In each daily and biweekly survey, one item from the BANGS (daily) and BPNSFS (biweekly) was duplicated to assess response consistency [@MeadeCraig2012identifying]; participants whose responses to the two identical items differed by more than one scale point were flagged for potential careless responding.
2.  In the telemetry, we use several heuristics to identify potential unreliable sessions: sessions beginning or ending in the future (indicative of clock manipulation or other errors), sessions longer than 12 hours long, 3 or more games being played simultaneously \[TODO: other heuristics\]
3.  In the time use, we flag any cases with fewer than 5 distinct activities logged in a day, or that are missing any

## Missingness

As with most longitudinal studies, attrition and missing data present important challenges for data quality and statistical inference. @tbl-missingness presents a comprehensive overview of missingness patterns across all data sources in our study, broken down by region (US and UK) and data type (Survey, Telemetry, and Cognitive Task). 

```{r}
#| label: tbl-missingness
#| tbl-cap: "Missingness patterns across survey, telemetry, and cognitive task data by region. The table shows the number of participants (N), total expected observations, observations actually collected, missing observations, median number of missing observations per participant, and percentage of data completeness for each measure. Retention plots (right columns) visualize the proportion of participants remaining active over time: for surveys and tasks, retention is calculated across study days or waves; for telemetry, across the 84-day study period. Density plots show the distribution of observations per participant."

# Helper function to calculate study days from enrollment date
get_study_days <- function(dates, pid, intake_data) {
  enrollment_date <- intake_data |>
    filter(pid == !!pid) |>
    pull(enrollment_date) |>
    first()

  if (is.na(enrollment_date)) {
    return(rep(NA_real_, length(dates)))
  }

  enrollment_datetime <- as.POSIXct(enrollment_date, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC")
  date_datetimes <- as.POSIXct(dates, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC")
  study_days <- as.numeric(difftime(date_datetimes, enrollment_datetime, units = "days"))
  return(ceiling(study_days) + 1)
}

# Identify participants based on actual survey participation
uk_pids <- panel |>
  mutate(pid = as.character(pid)) |>
  filter(pid %in% (intake |> mutate(pid = as.character(pid)) |> filter(country == "UK") |> pull(pid))) |>
  pull(pid) |>
  unique()

us_pids_panel <- panel |>
  mutate(pid = as.character(pid)) |>
  filter(pid %in% (intake |> mutate(pid = as.character(pid)) |> filter(country == "US") |> pull(pid))) |>
  pull(pid) |>
  unique()

us_pids_diary <- diary |>
  mutate(pid = as.character(pid)) |>
  pull(pid) |>
  unique()

us_pids <- union(us_pids_panel, us_pids_diary)

# Helper function to calculate survey missingness
calc_survey_missingness <- function(data, pids, region, measure_name, expected_n) {
  # Ensure pid is character for consistency
  data <- data |> mutate(pid = as.character(pid))

  by_participant <- data |>
    filter(pid %in% pids) |>
    count(pid, name = "observed") |>
    complete(pid = pids, fill = list(observed = 0)) |>
    mutate(
      expected = expected_n,
      missing = expected - observed
    )

  actual_observed <- data |>
    filter(pid %in% pids) |>
    nrow()

  # Calculate retention over time
  max_days <- if (measure_name == "Daily Diary") 30 else 84

  enrollment_lookup <- intake |>
    filter(pid %in% pids) |>
    select(pid, enrollment_date) |>
    distinct()

  last_response_by_participant <- data |>
    filter(pid %in% pids) |>
    left_join(enrollment_lookup, by = "pid") |>
    mutate(
      enrollment_datetime = as.POSIXct(enrollment_date, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
      response_datetime = as.POSIXct(date, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
      study_day = ceiling(as.numeric(difftime(response_datetime, enrollment_datetime, units = "days"))) + 1
    ) |>
    filter(!is.na(study_day) & study_day >= 1 & study_day <= max_days) |>
    group_by(pid) |>
    summarise(last_day = max(study_day, na.rm = TRUE), .groups = "drop")

  retention_data <- sapply(1:max_days, function(day) {
    n_retained <- sum(last_response_by_participant$last_day >= day)
    n_retained / length(pids)
  })

  tibble(
    region = region,
    data_type = "Survey",
    measure = measure_name,
    n_participants = length(pids),
    total_expected = length(pids) * expected_n,
    total_observed = actual_observed,
    total_missing = length(pids) * expected_n - actual_observed,
    median_missing_per_participant = median(by_participant$missing, na.rm = TRUE),
    obs_per_participant = list(by_participant$observed),
    retention_over_time = list(retention_data),
    complete_missingness = FALSE
  )
}

# Helper function to get platform players
get_platform_players <- function(platform_name, pids = NULL) {
  result <- intake |>
    mutate(pid = as.character(pid)) |>
    filter(
      str_detect(linked_platforms %||% "", platform_name) |
      case_when(
        platform_name == "Xbox" ~ coalesce(prop_xbox, 0) > 0,
        platform_name == "Nintendo" ~ coalesce(prop_nintendo, 0) > 0,
        platform_name == "Steam" ~ coalesce(prop_steam, 0) > 0,
        TRUE ~ FALSE
      )
    ) |>
    pull(pid) |>
    unique()

  if (!is.null(pids)) {
    result <- intersect(result, pids)
  }

  return(as.character(result))
}

# Helper function to calculate telemetry missingness
calc_telemetry_missingness <- function(telemetry_data, platform_name, pids, region) {
  # Ensure pid is character for consistency
  telemetry_data <- telemetry_data |> mutate(pid = as.character(pid))
  telemetry_pids <- unique(telemetry_data$pid)
  players <- get_platform_players(platform_name, pids)
  no_telemetry <- setdiff(players, telemetry_pids)

  by_participant <- telemetry_data |>
    filter(pid %in% players) |>
    count(pid, name = "observed") |>
    complete(pid = players, fill = list(observed = 0))

  obs_values <- by_participant$observed
  has_complete_missingness <- length(intersect(players, telemetry_pids)) == 0

  if (has_complete_missingness) {
    obs_for_plot <- c(0, 0)
  } else if (any(obs_values > 0)) {
    q1 <- quantile(obs_values, 0.25, na.rm = TRUE)
    q3 <- quantile(obs_values, 0.75, na.rm = TRUE)
    iqr <- q3 - q1
    lower_bound <- max(0, q1 - 1.5 * iqr)
    upper_bound <- q3 + 1.5 * iqr
    obs_for_plot <- obs_values[obs_values >= lower_bound & obs_values <= upper_bound]
    if (length(obs_for_plot) < 2) obs_for_plot <- obs_values
  } else {
    obs_for_plot <- obs_values
  }

  if (length(obs_for_plot) < 2) {
    obs_for_plot <- if (length(obs_for_plot) == 1) c(obs_for_plot, obs_for_plot) else c(0, 0)
  }

  # Calculate retention over time (84 days)
  if (has_complete_missingness || nrow(telemetry_data |> filter(pid %in% players)) == 0) {
    retention_data <- rep(0, 84)
  } else {
    enrollment_lookup <- intake |>
      filter(pid %in% players) |>
      select(pid, enrollment_date) |>
      distinct()

    last_activity_by_participant <- telemetry_data |>
      filter(pid %in% players) |>
      left_join(enrollment_lookup, by = "pid") |>
      mutate(
        enrollment_datetime = as.POSIXct(enrollment_date, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
        session_datetime = as.POSIXct(session_start, format = "%Y-%m-%dT%H:%M:%SZ", tz = "UTC"),
        study_day = ceiling(as.numeric(difftime(session_datetime, enrollment_datetime, units = "days"))) + 1
      ) |>
      filter(!is.na(study_day) & study_day >= 1 & study_day <= 84) |>
      group_by(pid) |>
      summarise(last_day = max(study_day, na.rm = TRUE), .groups = "drop")

    retention_data <- sapply(1:84, function(day) {
      n_retained <- sum(last_activity_by_participant$last_day >= day)
      n_retained / length(players)
    })
  }

  tibble(
    region = region,
    data_type = "Telemetry",
    measure = platform_name,
    n_participants = length(players),
    total_expected = length(players),
    total_observed = length(intersect(players, telemetry_pids)),
    total_missing = length(no_telemetry),
    median_missing_per_participant = NA_real_,
    obs_per_participant = list(obs_for_plot),
    retention_over_time = list(retention_data),
    complete_missingness = has_complete_missingness
  )
}

# Helper function to calculate cognitive task missingness
calc_task_missingness <- function(data, pids, region, measure_name, expected_n) {
  # Ensure pid is character for consistency
  data <- data |> mutate(pid = as.character(pid))

  by_participant <- data |>
    filter(pid %in% pids) |>
    count(pid, name = "observed") |>
    complete(pid = pids, fill = list(observed = 0)) |>
    mutate(
      expected = expected_n,
      missing = expected - observed
    )

  actual_observed <- data |>
    filter(pid %in% pids) |>
    nrow()

  expected_waves <- c(1, 3, 5)

  last_wave_by_participant <- data |>
    filter(pid %in% pids) |>
    group_by(pid) |>
    summarise(last_wave = max(wave, na.rm = TRUE), .groups = "drop")

  retention_data <- sapply(expected_waves, function(w) {
    n_retained <- sum(last_wave_by_participant$last_wave >= w)
    n_retained / length(pids)
  })

  tibble(
    region = region,
    data_type = "Task",
    measure = measure_name,
    n_participants = length(pids),
    total_expected = length(pids) * expected_n,
    total_observed = actual_observed,
    median_missing_per_participant = median(by_participant$missing, na.rm = TRUE),
    total_missing = length(pids) * expected_n - actual_observed,
    obs_per_participant = list(by_participant$observed),
    retention_over_time = list(retention_data),
    complete_missingness = FALSE
  )
}

# Calculate all statistics
diary_stats_us <- calc_survey_missingness(diary, us_pids, "US", "Daily Diary", 30)
panel_stats_us <- calc_survey_missingness(panel, us_pids, "US", "Biweekly Panel", 6)
panel_stats_uk <- calc_survey_missingness(panel, uk_pids, "UK", "Biweekly Panel", 6)
xbox_stats_us <- calc_telemetry_missingness(xbox, "Xbox", us_pids, "US")
xbox_stats_uk <- calc_telemetry_missingness(xbox, "Xbox", uk_pids, "UK")
nintendo_stats_us <- calc_telemetry_missingness(nintendo, "Nintendo", us_pids, "US")
nintendo_stats_uk <- calc_telemetry_missingness(nintendo, "Nintendo", uk_pids, "UK")
steam_stats_us <- calc_telemetry_missingness(steam, "Steam", us_pids, "US")
steam_stats_uk <- calc_telemetry_missingness(steam, "Steam", uk_pids, "UK")
simon_stats_us <- calc_task_missingness(simon, us_pids, "US", "Simon Task", 3)
simon_stats_uk <- calc_task_missingness(simon, uk_pids, "UK", "Simon Task", 3)

# Combine all statistics
missingness_table <- bind_rows(
  diary_stats_us, panel_stats_us, xbox_stats_us, nintendo_stats_us, steam_stats_us, simon_stats_us,
  panel_stats_uk, xbox_stats_uk, nintendo_stats_uk, steam_stats_uk, simon_stats_uk
)

# Extract plot data
density_plot_data <- missingness_table$obs_per_participant

retention_plot_data <- lapply(1:nrow(missingness_table), function(i) {
  ret_vec <- missingness_table$retention_over_time[[i]]
  if (missingness_table$measure[i] == "Daily Diary") {
    x_vals <- 1:30
  } else if (missingness_table$data_type[i] == "Task") {
    x_vals <- c(1, 3, 5)
  } else {
    x_vals <- 1:84
  }
  data.frame(x = x_vals, y = ret_vec)
})

# Format table for display
table_data <- missingness_table |>
  mutate(
    pct_complete = (total_observed / total_expected) * 100,
    `N` = format(n_participants, big.mark = ","),
    Expected = format(total_expected, big.mark = ","),
    Observed = format(total_observed, big.mark = ","),
    `Median Missing` = ifelse(
      is.na(median_missing_per_participant),
      "—",
      sprintf("%.1f", median_missing_per_participant)
    ),
    `% Complete` = sprintf("%.1f%%", pct_complete),
    Retention = "",
    Density = ""
  ) |>
  group_by(region, data_type) |>
  mutate(
    show_type = row_number() == 1,
    show_region = row_number() == 1
  ) |>
  ungroup() |>
  group_by(region) |>
  mutate(show_region = ifelse(row_number() == 1, TRUE, FALSE)) |>
  ungroup() |>
  select(
    Region = region,
    `Data Type` = data_type,
    Measure = measure,
    `N`, Expected, Observed,
    `Median Missing`,
    `% Complete`,
    Retention, Density,
    show_type, show_region
  ) |>
  mutate(
    Region = ifelse(show_region, Region, ""),
    `Data Type` = ifelse(show_type, `Data Type`, "")
  ) |>
  select(-show_type, -show_region)

# Identify styling positions
us_start <- 1
uk_start <- which(missingness_table$region == "UK")[1]
data_type_starts <- table_data |>
  mutate(row_num = row_number()) |>
  filter(`Data Type` != "") |>
  pull(row_num)

# Create table - with plots for HTML, without for other formats
if (knitr::is_html_output()) {
  # Find the row index for UK Xbox telemetry  
  uk_xbox_row <- which(missingness_table$region == "UK" & 
                       missingness_table$data_type == "Telemetry" & 
                       missingness_table$measure == "Xbox")
  
  # Prepare percentage complete data for bar plot
  pct_complete_values <- as.list(
    (missingness_table$total_observed / missingness_table$total_expected)
  )
  
  # Create initial table with % complete bar plots
  tbl_html <- tt(table_data) |>
    plot_tt(
      j = 8,
      fun = "bar",
      data = pct_complete_values,
      color = c("steelblue", "lightgrey"),
      xlim = c(0, 1)
    ) |>
    style_tt(i = 0, bold = TRUE) |>
    style_tt(
      i = c(us_start, uk_start),
      j = 1,
      bold = TRUE,
      line = "t",
      line_width = 0.15
    ) |>
    style_tt(
      i = data_type_starts,
      j = 2,
      bold = TRUE,
      line = "t",
      line_width = 0.1
    ) |>
    style_tt(j = c(8, 9, 10), align = "c") |>
    style_tt(fontsize = 0.8)
  
  # Apply retention and density plots to all rows EXCEPT UK Xbox
  if (length(uk_xbox_row) > 0) {
    # Get indices for all rows except UK Xbox
    rows_to_plot <- setdiff(1:nrow(table_data), uk_xbox_row)
    
    # Create subset of plot data for non-UK-Xbox rows
    retention_subset <- retention_plot_data[rows_to_plot]
    density_subset <- density_plot_data[rows_to_plot]
    
    # Apply plots to non-UK-Xbox rows
    tbl_html <- tbl_html |>
      plot_tt(
        i = rows_to_plot,
        j = 9,
        fun = "line",
        data = retention_subset,
        color = "steelblue"
      ) |>
      plot_tt(
        i = rows_to_plot,
        j = 10,
        fun = "density",
        data = density_subset,
        color = "steelblue"
      )
  } else {
    # No UK Xbox row found, plot all rows normally
    tbl_html <- tbl_html |>
      plot_tt(
        j = 9,
        fun = "line",
        data = retention_plot_data,
        color = "steelblue"
      ) |>
      plot_tt(
        j = 10,
        fun = "density",
        data = density_plot_data,
        color = "steelblue"
      )
  }
  
  tbl_html
} else {
  # Other formats: table without plot columns to avoid path issues
  table_data_no_plots <- table_data |>
    select(-Retention, -Density)
  
  tt(table_data_no_plots) |>
    style_tt(i = 0, bold = TRUE) |>
    style_tt(
      i = c(us_start, uk_start),
      j = 1,
      bold = TRUE,
      line = "t",
      line_width = 0.15
    ) |>
    style_tt(
      i = data_type_starts,
      j = 2,
      bold = TRUE,
      line = "t",
      line_width = 0.1
    ) |>
    style_tt(j = 8, align = "c") |>
    style_tt(fontsize = 0.8)
}
```

# Dataset

Here we visualize key aspects of the data to provide an overview of its contents and properties.

@fig-stacked-playtime compares the average self-reported distribution of play across platforms to the distribution in our digital trace data capture. It is vital to note that the self-report data not be treated as ground truth: we have good evidence that people's self-reports of media use are inaccurate [@KahnEtAl2014why; @ParryEtAl2021systematic], with some previous work finding systemic overestimation of video game play [@JohannesEtAl2021Video]. Nonetheless, it is likely that some portion of player's true gaming is systematically uncaptured, due to factors such as missing platforms (e.g., Playstation), missing titles (e.g., Nintendo third party), or player privacy settings (e.g., playing in invisible mode; setting certain games to private on Steam).

::: {.callout-warning}
I'm halfway through making changes to the stacked bar chart preprocessing, ignore the actual values.
:::

```{r}
#| label: fig-stacked-playtime

# TODO: blur or fuzzy lines?
# TODO: week-to-week variance?
# violin plot / side by side - comparison of 5 specific individuals - by quantiles

# --- counts ---
n_qualified <- intake |>
  filter(qualified) |>
  summarise(n = n_distinct(pid)) |>
  pull()

# --- Self-report per platform (as before; drop the temporary Xbox-only filter) ---
sr_long <- intake |>
  filter(qualified) |>
  mutate(
    Steam = self_reported_weekly_play * coalesce(prop_steam / 100, 0),
    Xbox = self_reported_weekly_play * coalesce(prop_xbox / 100, 0),
    PlayStation = self_reported_weekly_play *
      coalesce(prop_playstation / 100, 0),
    Nintendo = self_reported_weekly_play * coalesce(prop_nintendo / 100, 0),
    iOS = self_reported_weekly_play * coalesce(prop_ios / 100, 0),
    Android = self_reported_weekly_play * coalesce(prop_android / 100, 0)
  ) |>
  pivot_longer(
    c(Steam, Xbox, PlayStation, Nintendo, iOS, Android),
    names_to = "platform",
    values_to = "minutes"
  ) |>
  group_by(platform) |>
  summarise(minutes = mean(minutes, na.rm = TRUE), .groups = "drop") |>
  mutate(source = "Self-report")

# --- Telemetry per platform PER QUALIFIED PERSON (new denominator) ---
telemetry_weekly_A <- daily_all |>
  mutate(week = floor_date(day_local, "week")) |>
  group_by(platform, week) |>
  summarise(total_minutes = sum(minutes, na.rm = TRUE), .groups = "drop") |>
  group_by(platform) |>
  summarise(
    minutes = mean(total_minutes, na.rm = TRUE) / n_qualified,
    .groups = "drop"
  ) |>
  mutate(source = "Telemetry")

# --- Combine + order (unchanged) ---
weekly_by_platform <- bind_rows(sr_long, telemetry_weekly_A) |>
  mutate(source = factor(source, levels = c("Self-report", "Telemetry")))

stack_order <- weekly_by_platform |>
  filter(source == "Self-report") |>
  arrange(minutes) |>
  pull(platform)

weekly_by_platform <- weekly_by_platform |>
  mutate(platform = factor(platform, levels = stack_order))

# --- Ghost overlay (unchanged) ---
ghost <- weekly_by_platform |>
  select(platform, source, minutes) |>
  pivot_wider(names_from = source, values_from = minutes, values_fill = 0) |>
  mutate(gap = pmax(`Self-report` - Telemetry, 0)) |>
  arrange(desc(gap)) |>
  mutate(
    tele_top = sum(Telemetry, na.rm = TRUE),
    ymin = tele_top + lag(cumsum(gap), default = 0),
    ymax = ymin + gap,
    xmid = match("Telemetry", levels(weekly_by_platform$source)),
    xmin = xmid - 0.45,
    xmax = xmid + 0.45,
    ymid = (ymin + ymax) / 2
  ) |>
  filter(gap > 0)

# --- Panel A plot (your original plot code works as-is) ---
gA <- ggplot(
  weekly_by_platform,
  aes(x = source, y = minutes, fill = platform)
) +
  geom_col(width = 0.9) +
  scale_fill_viridis_d(
    option = "magma",
    begin = .1,
    end = .8,
    limits = stack_order
  ) +
  labs(
    x = "Data Source",
    y = "Weekly minutes per qualified person",
    fill = "Platform"
  ) +
  theme(legend.position = "bottom") +
  geom_text(
    aes(label = ifelse(minutes < 10, "", round(minutes, 0))),
    position = position_stack(vjust = 0.5),
    color = "white",
    family = "serif"
  ) +
  geom_rect(
    data = ghost,
    aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
    inherit.aes = FALSE,
    linetype = "dotted",
    linewidth = 0.6,
    color = "black",
    fill = NA
  ) +
  geom_text(
    data = ghost,
    aes(x = xmid, y = ymid, label = paste0("+", round(gap, 0))),
    inherit.aes = FALSE,
    family = "serif"
  )

# --- how many qualified users per platform ---
n_users <- intake |>
  filter(qualified) |>
  transmute(
    pid = as.character(pid),
    Steam = str_detect(linked_platforms %||% "", "Steam"),
    Nintendo = str_detect(linked_platforms %||% "", "Nintendo"),
    Xbox = str_detect(linked_platforms %||% "", "Xbox")
  ) |>
  summarise(
    Steam = sum(Steam, na.rm = TRUE),
    Nintendo = sum(Nintendo, na.rm = TRUE),
    Xbox = sum(Xbox, na.rm = TRUE)
  ) |>
  pivot_longer(everything(), names_to = "platform", values_to = "n_users")

# --- SR per platform among users of that platform ---
sr_users <- intake |>
  filter(qualified) |>
  mutate(
    Steam = self_reported_weekly_play * coalesce(prop_steam / 100, 0),
    Nintendo = self_reported_weekly_play * coalesce(prop_nintendo / 100, 0),
    Xbox = self_reported_weekly_play * coalesce(prop_xbox / 100, 0),
    uses_Steam = str_detect(linked_platforms %||% "", "Steam"),
    uses_Nintendo = str_detect(linked_platforms %||% "", "Nintendo"),
    uses_Xbox = str_detect(linked_platforms %||% "", "Xbox")
  ) |>
  select(pid, Steam, Nintendo, Xbox, uses_Steam, uses_Nintendo, uses_Xbox) |>
  pivot_longer(
    c(Steam, Nintendo, Xbox),
    names_to = "platform",
    values_to = "sr_minutes"
  ) |>
  mutate(
    uses = case_when(
      platform == "Steam" ~ uses_Steam,
      platform == "Nintendo" ~ uses_Nintendo,
      platform == "Xbox" ~ uses_Xbox,
      TRUE ~ FALSE
    )
  ) |>
  filter(uses) |>
  group_by(platform) |>
  summarise(sr = mean(sr_minutes, na.rm = TRUE), .groups = "drop")

# --- Telemetry per platform PER USER OF THAT PLATFORM ---
tele_users <- daily_all |>
  filter(platform %in% c("Steam", "Nintendo", "Xbox")) |>
  mutate(week = floor_date(day_local, "week")) |>
  group_by(platform, week) |>
  summarise(total = sum(minutes, na.rm = TRUE), .groups = "drop") |>
  group_by(platform) |>
  summarise(avg_weekly_total = mean(total, na.rm = TRUE), .groups = "drop") |>
  left_join(n_users, by = "platform") |>
  mutate(tele = ifelse(n_users > 0, avg_weekly_total / n_users, NA_real_)) |>
  select(platform, tele)

# --- tidy + facet plot ---
bcd <- sr_users |>
  left_join(tele_users, by = "platform") |>
  pivot_longer(c(sr, tele), names_to = "source", values_to = "minutes") |>
  mutate(
    source = recode(source, sr = "Self-report", tele = "Telemetry"),
    source = factor(source, levels = c("Self-report", "Telemetry")),
    platform = factor(platform, levels = c("Steam", "Nintendo", "Xbox"))
  )

gBCD <- ggplot(bcd, aes(source, minutes, fill = source)) +
  geom_col(width = 0.8) +
  scale_fill_viridis_d(
    option = "magma",
    begin = .15,
    end = .7,
    guide = "none"
  ) +
  labs(x = NULL, y = "Weekly minutes per user of platform") +
  geom_text(aes(label = round(minutes, 0)), vjust = -0.35, family = "serif") +
  facet_wrap(~platform, nrow = 1)

# --- combine if desired ---
gA / gBCD
```

In @fig-heatmap, we visualize the distribution of play across days and times. As expected, we find that the likelihood of play peaks on weekends from 8-11pm, and is lowest in the early morning.

```{r}
#| label: fig-heatmap

# expected number of occurrences of each slot in telemetry span
slot_exposure <- telemetry_spans |>
  rowwise() |>
  mutate(n_weeks = interval(telemetry_start, telemetry_end) %/% weeks(1) + 1) |>
  ungroup() |>
  crossing(dow = 1:7, hour = 0:23) |>
  mutate(n_occurrences = n_weeks) # each slot appears once per week

# total minutes actually played per slot
slot_play <- hourly_all |>
  mutate(
    dow = lubridate::wday(hour_start_local, week_start = 1),
    hour = hour(hour_start_local)
  ) |>
  group_by(pid, platform, dow, hour) |>
  summarise(
    minutes = pmin(sum(minutes, na.rm = TRUE), 60 * n()),
    .groups = "drop"
  )

# combine: per-player–platform average minutes per occurrence
slot_per_player <- slot_exposure |>
  left_join(slot_play, by = c("pid", "platform", "dow", "hour")) |>
  mutate(
    minutes = replace_na(minutes, 0),
    avg_minutes = minutes / n_occurrences
  )

# final: probability across players (avg minutes / 60)
slot_summary <- slot_per_player |>
  group_by(dow, hour) |>
  summarise(
    prob_playing = mean(avg_minutes / 60),
    n_active = n(),
    .groups = "drop"
  ) |>
  mutate(
    dow = factor(
      dow,
      levels = 1:7,
      labels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"),
      ordered = TRUE
    )
  )


ggplot(slot_summary, aes(x = hour, y = dow, fill = prob_playing)) +
  geom_tile() +
  scale_x_continuous(breaks = 0:23) +
  scale_fill_viridis_c(
    option = "magma",
    labels = scales::percent_format(accuracy = .1),
    # limits = c(0, .028), breaks = seq(0, .028, .007),
    name = "Probability of active play"
  ) +
  labs(
    x = "Hour (local)",
    y = "Day",
    title = "Diurnal play across Xbox, Nintendo, Steam"
  ) +
  theme(panel.grid.major.y = element_blank())

```

@fig-case-study illustrates the temporal dynamics of gaming behavior and mental wellbeing through three representative case studies.

```{r}
#| label: fig-case-study
#| fig-cap: "Daily gaming patterns and mental wellbeing for three representative participants. Stacked bars represent total daily playtime across platforms (Nintendo in red, Steam in dark blue, Xbox in green). Orange line shows biweekly mental wellbeing scores (WEMWBS, range 14-70) measured at six study waves. Participants were selected as closest representatives of the 25th, 50th, and 75th percentiles of total playtime distribution across the full 84-day study period."
#| fig-asp: 1.2

# Find each participant's study start date (first panel wave)
study_start_dates <- panel |>
  group_by(pid) |>
  summarise(study_start = min(date, na.rm = TRUE), .groups = "drop") |>
  mutate(pid = as.character(pid))

# Join study start dates and calculate study day
daily_with_study_day <- daily_all |>
  left_join(study_start_dates, by = "pid") |>
  filter(!is.na(study_start)) |>
  mutate(
    study_day = as.integer(floor(as.numeric(difftime(day_local, study_start, units = "days")))) + 1
  ) |>
  filter(study_day >= 1 & study_day <= 84)

# Calculate total playtime per participant (sum across all platforms and days 1-84)
total_playtime <- daily_with_study_day |>
  group_by(pid) |>
  summarise(
    total_minutes = sum(minutes, na.rm = TRUE),
    n_platforms = n_distinct(platform[minutes > 0]),
    .groups = "drop"
  )

# Calculate percentiles
percentiles <- quantile(total_playtime$total_minutes, probs = c(0.25, 0.50, 0.75), na.rm = TRUE)

# Find participant closest to each percentile based on total playtime
find_closest_participant <- function(target_value, data) {
  data |>
    mutate(distance = abs(total_minutes - target_value)) |>
    slice_min(distance, n = 1, with_ties = FALSE) |>
    pull(pid)
}

selected_pids <- c(
  "p25" = find_closest_participant(percentiles[1], total_playtime),
  "p50" = find_closest_participant(percentiles[2], total_playtime),
  "p75" = find_closest_participant(percentiles[3], total_playtime)
)

# Process panel data to get wellbeing scores
panel_wellbeing <- panel |>
  mutate(pid = as.character(pid)) |>
  left_join(study_start_dates, by = "pid") |>
  filter(!is.na(study_start)) |>
  mutate(
    study_day = as.integer(floor(as.numeric(difftime(date, study_start, units = "days")))) + 1
  ) |>
  filter(pid %in% selected_pids) |>
  # Calculate wellbeing composite scores
  rowwise() |>
  mutate(
    # WEMWBS - mental wellbeing (14 items, scale 1-5 each, summed for total score 14-70)
    wemwbs = sum(c_across(starts_with("wemwbs_")), na.rm = TRUE)
  ) |>
  ungroup() |>
  select(pid, study_day, wave, wemwbs) |>
  mutate(
    percentile = case_when(
      pid == selected_pids["p25"] ~ "25th Percentile",
      pid == selected_pids["p50"] ~ "50th Percentile (Median)",
      pid == selected_pids["p75"] ~ "75th Percentile",
      TRUE ~ NA_character_
    ),
    percentile = factor(percentile, levels = c("25th Percentile", "50th Percentile (Median)", "75th Percentile"))
  )

# Prepare data for selected participants
plot_data <- daily_with_study_day |>
  filter(pid %in% selected_pids) |>
  mutate(
    percentile = case_when(
      pid == selected_pids["p25"] ~ "25th Percentile",
      pid == selected_pids["p50"] ~ "50th Percentile (Median)",
      pid == selected_pids["p75"] ~ "75th Percentile",
      TRUE ~ NA_character_
    ),
    percentile = factor(percentile, levels = c("25th Percentile", "50th Percentile (Median)", "75th Percentile"))
  )

# Create a complete grid of study days for each participant
complete_grid <- expand_grid(
  pid = selected_pids,
  study_day = 1:84,
  platform = unique(daily_all$platform)
) |>
  mutate(
    percentile = case_when(
      pid == selected_pids["p25"] ~ "25th Percentile",
      pid == selected_pids["p50"] ~ "50th Percentile (Median)",
      pid == selected_pids["p75"] ~ "75th Percentile",
      TRUE ~ NA_character_
    ),
    percentile = factor(percentile, levels = c("25th Percentile", "50th Percentile (Median)", "75th Percentile"))
  )

# Fill in missing days with 0 minutes
plot_data_complete <- complete_grid |>
  left_join(plot_data |> select(pid, study_day, platform, minutes), 
            by = c("pid", "study_day", "platform")) |>
  mutate(minutes = replace_na(minutes, 0))

# Calculate global max playtime across all percentiles for fixed y-axis
global_max_playtime <- plot_data_complete |>
  group_by(percentile, study_day) |>
  summarise(daily_total = sum(minutes, na.rm = TRUE), .groups = "drop") |>
  pull(daily_total) |>
  max(na.rm = TRUE)

# Calculate coefficient for dual axis (wellbeing 70 maps to global max playtime)
# This will be the same for all panels
coeff <- global_max_playtime / 56
y_max_for_wb70 <- (70 - 14) * coeff

# Create separate plots for each percentile to handle dual y-axes properly
plots_list <- list()
pct_levels <- levels(plot_data_complete$percentile)

for (i in seq_along(pct_levels)) {
  pct_level <- pct_levels[i]
  is_bottom <- (i == length(pct_levels))
  is_middle <- (i == 2)  # Middle facet for y-axis labels
  
  # Filter data for this percentile
  plot_data_pct <- plot_data_complete |> filter(percentile == pct_level)
  wellbeing_data_pct <- panel_wellbeing |> filter(percentile == pct_level)
  
  p_temp <- ggplot() +
    # Playtime bars (primary y-axis)
    geom_col(
      data = plot_data_pct,
      aes(x = study_day, y = minutes, fill = platform),
      width = 1
    ) +
    # Wellbeing line (secondary y-axis) - transform to playtime scale
    geom_line(
      data = wellbeing_data_pct,
      aes(x = study_day, y = (wemwbs - 14) * coeff, group = 1),
      color = "#FF6B35",
      linewidth = 1.2,
      linetype = "solid"
    ) +
    geom_point(
      data = wellbeing_data_pct,
      aes(x = study_day, y = (wemwbs - 14) * coeff, group = 1),
      color = "#FF6B35",
      size = 3,
      shape = 21,
      fill = "white",
      stroke = 1.5
    ) +
    scale_fill_manual(
      values = c(
        "Nintendo" = "#E60012",  # Nintendo red
        "Steam" = "#1B2838",     # Steam dark blue
        "Xbox" = "#107C10"       # Xbox green
      ),
      name = "Platform"
    ) +
    scale_x_continuous(
      name = if (is_bottom) "Study Day" else NULL,
      breaks = seq(0, 84, by = 14),
      expand = c(0, 0)
    ) +
    scale_y_continuous(
      name = if (is_middle) "Total Daily Playtime (minutes)" else NULL,
      limits = c(0, y_max_for_wb70 * 1.05),
      expand = c(0, 0),
      sec.axis = sec_axis(
        transform = ~ . / coeff + 14,
        name = if (is_middle) "Wellbeing (WEMWBS)" else NULL,
        breaks = seq(14, 70, by = 7)
      )
    ) +
    ggtitle(pct_level) +
    theme(
      axis.title.y.right = if (is_middle) element_text(color = "#FF6B35", size = 10) else element_blank(),
      axis.text.y.right = element_text(color = "#FF6B35", size = 9),
      axis.title.y.left = if (is_middle) element_text(size = 10) else element_blank(),
      plot.title = element_text(size = 11, hjust = 0.5),
      axis.title.x = if (is_bottom) element_text() else element_blank()
    )
  
  plots_list[[pct_level]] <- p_temp
}

# Combine plots using patchwork
wrap_plots(plots_list, ncol = 1, guides = "collect") +
  plot_annotation(
    title = "Daily Gaming Patterns and Wellbeing Across Platforms",
    subtitle = "Sample gaming timeseries with biweekly wellbeing (WEMWBS)",
    theme = theme(
      plot.title = element_text(size = 14, face = "bold"),
      plot.subtitle = element_text(size = 11)
    )
  ) &
  theme(legend.position = "bottom")
```

## Attention control

```{r}
#| label: simon-calculate-summary

simon_n <- simon |>
  reframe(n = n(), .by = wave)
simon_sdl <- simon |>
  reframe(
    mean_sdl(score_final, mult = 1),
    .by = c(wave, device_type)
  )
```

Overall, `{r} knitr::combine_words(simon_n$n)` participants completed the Simon task at panel waves 1, 3, and 5. The average performances were approximately 10 points lower than in the in-person study of @burgoyneNatureMeasurementAttention2023 (Figure 16). Participants on mobile devices generally attained lower scores than those not on mobile devices (@fig-simon, rows), but performance was generally stable across the three waves (@fig-simon, columns).

```{r}
#| label: fig-simon
#| fig-cap: Histograms of participants' Simon Squared task scores across study waves. Points and bars indicate means ±1 standard deviation.
#| fig-asp: 0.3

simon |>
  ggplot() +
  aes(score_final) +
  scale_y_continuous(
    "Count",
    expand = expansion(c(0, 0.1))
  ) +
  scale_x_continuous("Final score") +
  geom_histogram(fill = "grey70") +
  geom_pointrange(
    data = simon_sdl,
    aes(x = y, xmin = ymin, xmax = ymax, y = 0),
    linewidth = 3,
    fatten = 14,
    shape = "|"
  ) +
  facet_grid(
    device_type ~ wave,
    labeller = labeller(.cols = as_labeller(~ str_glue("Wave {.}"))),
    scales = "free_y"
  )
```

# Discussion

We believe this dataset has potential to address a wide variety of common research questions in the field.

Some of these questions will be addressed in forthcoming registered reports: specifically, we have plans to test (1) key hypotheses from the Basic Needs in Games model [@BallouDeterding2024Basic] about how gaming relates to basic psychological needs over time, (2) the relationship between late-night gaming and sleep, and (3) the relationship between playtime in different genres and wellbeing.

Nonetheless, the richness of this data means that researchers can explore numerous other questions (or, indeed, conduct and compare alternative analysis approaches to the above questions). To stimulate ideas, we present a few questions we think the data are well-suited to answering.

*How do seasons and weather impact playtime?* Because we capture time-stamped play sessions alongside participants’ geographic locations, researchers can merge in high‐resolution weather and daylight data to examine how environmental factors causally influence gaming behavior. Causal inference techniques such as inverse probability weighting can enable precise estimates of how, when, and how much people play in response to seasonal and meteorological changes. By quantifying these effects, researchers can better distinguish weather‐related demand from other drivers (like work schedules or weekend routines), improving the precision of studies on gaming’s impact on wellbeing, motivation, and cognition.

*How do neurotypical and neurodiverse players differ in their gaming behavior?* Using the neurodivergence data we collected (which includes, for example, `r table(intake$neuroIdenCondition_1)[1]` participants who identify as having autism and `r table(intake$neuroIdenCondition_2)[1]` who identify as having ADHD), researchers can. Neurodiversity in games has regularly been studied in the context of specific games and with qualitative methods

Accuracy of self-reported data - inference from other papers

We encourage researchers from a wide range of disciplines to explore these or other questions using the data we present, which is freely available for reuse under a CC0 license.

## Limitations

While this dataset represents a substantial step forward in holistic coverage of video game play, it remains imperfect: we did not capture data on PlayStation (\~19% of gaming market) or computer games played outside the Steam platform (\~11% of gaming market); on Nintendo, we do not have access to third-party titles (`r third_party_prop` of Nintendo play), and our coverage of smartphone play is limited by the difficulties and inconsistencies of screenshot-based donation and OCR retrieval.

We further are unable to identify idle time (when players have a game open but are not actively playing it) and account sharing (when players let friends or family use their account); some playtime values may therefore be overestimates of the person's true playtime, though we are unable to say by how much.

## Future Work

The trace data presented here is broad in scope but limited in granularity: we capture all gaming activity on a given platform, but not what happens within individual games. Prior work and theory make clear that in-game behaviors (e.g., what role a player adopts, whether they compete or cooperate, or how they perform in competitive modes) are critical determinants of player experience and thereby wellbeing (see e.g., @ElsonEtAl2014More for a review of how in-game contexts shape effects). This highlights a fundamental trade-off in digital trace research between breadth—how comprehensively play can be captured across platforms—and depth—the granularity of in-game behaviors and experiences. At present, our dataset emphasizes breadth, but we see strong potential in future study designs that combine platform-level telemetry with targeted in-game behavioral data to provide a more complete picture.

We also see strong potential in combining digital trace data with experimental designs that enable stronger causal inference—for example, randomizing players to single-player games only, or restricting play to certain times of day, to examine effects on social wellbeing or sleep. Previous researchers have noted a dearth of digital trace data-backed field experiments, while highlighting their potential [@StierEtAl2020Integrating]: Trace data not only captures naturalistic gaming behavior but also allows researchers to assess *substitution* (what games or platforms participants switch to under intervention) and *adherence* (how closely they follow assigned play patterns).

## Data Availability

All data, materials, and code related to the dataset and this manuscript are available under {{< meta data-license >}} at {{< meta data-url >}}.

# References

::: {#refs}
:::

```{=typst}
#show: appendix.with()
```

# Appendix

## Deviations from Preregistration

We made several deviations from our preregistration to ensure we could recruit enough high-quality participants to meet our sample size goals. In our view, none are so severe enough to threaten the validity of the study. Deviations are summarised in @tbl-trait.

```{r}
#| label: tbl-trait
#| tbl-cap: "Summary of deviations from preregistration"

tibble(
  Preregistered = c(
    "All participants sourced from PureProfile",
    "Screening sample would be nationally representative by ethnicity and gender",
    "Sample consists of participants aged 18--30 in the US and 18--75 in the UK",
    "To qualify, ≥75% of a participant's total gaming must take place on platforms included in the study (Xbox, Steam, Nintendo Switch)",
    "Qualification contingent upon valid telemetry within last 7 days",
    "Daily and biweekly surveys sent at 7pm local time",
    "Session-level Android data captured via the ActivityWatch app"
  ),
  Actual = c(
    "Participants sourced from both PureProfile and Prolific",
    "Approximately 50% of screening was done using quotas for national representativeness by ethnicity and gender; all subsequent sampling used convenience sampling with no quotas",
    "Sample consists of participants aged 18-40 in both regions",
    "To qualify, ≥50% of a participant's total gaming must take place on platforms included in the study (Xbox, Steam, Nintendo Switch)",
    "Qualification contingent upon valid telemetry within last 14 days",
    "Daily and biweekly surveys sent at 2pm local time",
    "Daily-level Android data captured using screenshots of the Digital Wellbeing interface"
  ),
  `Justification for Deviation` = c(
    "Exhausted PureProfile participant pool before reaching required sample size",
    "Exhausted participant pools of smaller demographic categories on both Prolific and PureProfile before reaching required sample size",
    "\\(1\\) Unable to recruit enough participants in the US aged 18--30; (2) near-zero qualification rates from UK adults over 50; (3) desire for results from both regions to be more easily comparable",
    "Low rates of study qualification at 75% threshold, in large part due to substantial uncaptured Playstation play",
    "Feedback from participants indicating that play during a 7-day period was subject to too many fluctuations (e.g., a busy workweek)",
    "Feedback from participants indicating that evening plans often interfered with survey completion and thus adversely affected response rate",
    "Restrictions in PureProfile's privacy policy preventing installation of 3rd party apps; technical challenges in supporting users with the installation and data export"
  )
) |>
  tt() |>
  style_tt(fontsize = 0.8)
```
