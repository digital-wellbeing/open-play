---
title: |
  Open Play: A longitudinal dataset of multi-platform video game digital trace data and psychological measures
author:
  - name: Nick Ballou
    corresponding: true
    orcid: 0000-0003-4126-0696
    email: nick@nickballou.com
    affiliations:
      - ref: 1
    roles:
      - conceptualization
      - data curation
      - investigation
      - methodology
      - formal analysis
      - visualization
      - writing
      - editing

  - name: Tamás Andrei Földes
    corresponding: false
    orcid: 0000-0002-0623-9149
    email: tamas.foldes@oii.ox.ac.uk
    affiliations:
      - ref: 1
    roles:
      - conceptualization
      - data curation
      - methodology
      - formal analysis
      - validation
      - writing
      - editing

  - name: Matti Vuorre
    corresponding: false
    orcid: 0000-0001-5052-066X
    email: mjvuorre@uvt.nl
    affiliations:
      - ref: 2
    roles:
      - conceptualization
      - methodology
      - funding acquisition
      - formal analysis
      - supervision
      - writing
      - editing

  - name: Thomas Hakman
    corresponding: false
    orcid: 0009-0009-8292-2482
    email: thomas.hakman@oii.ox.ac.uk
    affiliations:
      - ref: 1
    roles:
      - conceptualization
      - investigation
      - data curation
      - formal analysis
      - writing
      - editing

  - name: Kristoffer Magnusson
    corresponding: false
    orcid: 0000-0003-0713-0556
    affiliations:
      - ref: 3
      - ref: 1
    roles:
      - methodology
      - data curation
      - software
      - editing

  - name: Andrew K Przybylski
    corresponding: true
    email: andy.przybylski@oii.ox.ac.uk
    orcid: 0000-0001-5547-2185
    affiliations:
      - ref: 1
    roles:
      - conceptualization
      - funding acquisition
      - project administration
      - supervision
      - editing
affiliations:
  - id: 1
    name: University of Oxford
    department: Oxford Internet Institute
  - id: 2
    name: Tilburg University
    department: Department of Social Psychology
  - id: 3
    name: Karolinska Institute
bibliography: references.bib
csl: https://www.zotero.org/styles/apa
keywords: [dataset, video games, well-being, mental health, trace data, attention]
editor_options:
  chunk_output_type: console
  markdown:
    wrap: none
knitr:
  opts_chunk:
    echo: false
    cache: true
    warning: false
    message: false
    fig-width: 6
    fig-asp: 0.618
    out-width: 80%
    fig-align: center
code-links: repo
nocite: |
  @RoennebergEtAl2003Life
  @SotoJohn2017short
  @KahnEtAl2015trojan
  @PontesEtAl2019measurement
  @ChenEtAl2015Basic
  @MartelaRyan2024Assessing
  @BallouEtAl2024Basic
  @Cantril1965pattern
  @CarneyEtAl2012Consensus
  @AlmeidaEtAl2002Daily
  @TennantEtAl2007warwickedinburgh
  @PilkonisEtAl2011item
  @BuysseEtAl1989Pittsburgh
  @Johns1991New
format:
  html:
    theme:
      light: united
      dark: darkly
    toc: true
    toc-depth: 2
    fontsize: 13pt
    other-links:
      - text: Stage 1 Registered Report
        href: https://osf.io/pb5nu
        icon: file-pdf
  preprint-typst:
    wordcount: true
    theme: jou
    citeproc: true
    mainfont: "Liberation Serif"
    sansfont: "Liberation Sans"
    include-before-body:
      - text: "#show <refs>: set par(hanging-indent: 1.4em)"
  docx: default
---

```{r}
#| label: libraries
#| cache: false

library(tidyverse)
library(lubridate)
library(jsonlite)
library(qualtRics)
library(sjlabelled)
library(hms)
library(openxlsx)
library(scales)
library(ggsankey)
library(ggrepel)
library(ggstream)
library(patchwork)
library(tinytable)
library(litedown)
library(sparkline)
library(Hmisc)
library(ggtext)
library(svglite)
library(data.table)
library(dtplyr)
library(readxl)
library(DT)
library(purrr)

# avoid conflicts
library(conflicted)
conflicts_prefer(
  dplyr::filter,
  dplyr::summarise,
  dplyr::summarize,
  dplyr::lag,
  dplyr::select,
  dplyr::recode,
  dplyr::first,
  dplyr::recode,
  lubridate::hour
)

# Load custom functions
source("R/helpers.R")

set.seed(8675309)
options(scipen = 999)

# Configure tinytable
options(tinytable_tt_theme_placement = "after")
options(tinytable_print_output = NULL)
options(tinytable_tt_digits = 2)

# For HTML, use portable mode to embed images inline (avoids external file dependencies)
# For other formats, we'll exclude plot columns entirely to avoid path issues
if (knitr::is_html_output()) {
  options(tinytable_html_portable = TRUE)
}

theme_set(theme_minimal())
theme_update(
  strip.background = element_rect(fill = "black"),
  strip.text = element_text(color = "white", size = 10),
  axis.text.y = element_text(color = "black", size = 10),
  axis.text.x = element_text(color = "black", size = 10),
  panel.grid.minor = element_blank(),
  panel.border = element_rect(
    colour = "black",
    fill = NA,
    linewidth = 1
  ),
)

```

```{r}
#| label: data-read

intake <- read_csv("data/clean/survey_intake.csv.gz", guess_max = 34000)
daily <- read_csv("data/clean/survey_daily.csv.gz")
biweekly <- read_csv("data/clean/survey_biweekly.csv.gz", guess_max = 5000)

nintendo <- read_csv("data/clean/nintendo.csv.gz")
steam <- read_csv("data/clean/steam.csv.gz")
steam_visibility <- read_csv(
  "data/raw/telemetry_steam_account_linking_raw.csv.gz"
)

xbox <- read_csv("data/clean/xbox.csv.gz")
simon <- read_csv("data/clean/simon.csv.gz")
ios <- read_csv("data/clean/ios.csv.gz")
android <- read_csv("data/clean/android.csv.gz")
time_use <- read_csv("data/clean/timeuse.csv.gz")

xbox_raw <- read_csv("data/raw/telemetry_xbox_raw.csv.gz")
nintendo_raw <- read_csv("data/raw/telemetry_nintendo_raw.csv.gz")
daily_raw <- read_csv("data/raw/survey_daily_raw.csv.gz", guess_max = 10000)
biweekly_raw <- read_csv(
  "data/raw/survey_biweekly_raw.csv.gz",
  guess_max = 5000
)
```

```{r}
#| label: data-prepare

intake <- intake |>
  mutate(
    enrollment_datetime = as_datetime(date, tz = "UTC"),
    study_end_datetime = enrollment_datetime + days(84)
  )

steam_visibility <- steam_visibility |>
  filter(platform == "steam") |>
  mutate(
    timestamp = as_datetime(timestamp, tz = "UTC"),
    is_public = event == "steamDataPublic"
  ) |>
  select(pid, timestamp, event, is_public)

# import raw data for calculating how many rows were excluded
xbox_valid_sessions <- xbox_raw |>
  merge_adjacent_sessions(tol_sec = 60) |>
  # filter non-foreground games
  filter(
    duration >= 1,
    pid %in% intake$pid,
    title_placement == "Full", # in foreground
    !genres %in% c("Shopping", "Video", "Entertainment", "Utilities & tools"),
    tolower(title_id) != "e1924c10-9c91-4652-81b9-8ca0670e77a7" # home screen
  )

nintendo_valid_sessions <- nintendo_raw |>
  distinct() |>
  filter(
    pid %in% intake$pid,
    duration >= 1
  ) |>
  merge_adjacent_sessions(tol_sec = 60)

```

```{r}
#| label: aggregate-session-hourly-daily

# merge with the local time zone offsets from intake
# Note: We keep country and local_timezone for DST-aware conversion
tz_map <- intake |>
  mutate(
    pid,
    country,
    local_timezone,
    .keep = "none"
  ) |>
  distinct(pid, .keep_all = TRUE)

# aggregate data at each level of granularity (session, hourly, daily)
# --- 1) SESSION-LEVEL (Nintendo + Xbox) ------------------------------------
session_telemetry <- bind_rows(
  xbox |> mutate(platform = "Xbox"),
  nintendo |> mutate(platform = "Nintendo")
) |>
  left_join(tz_map, by = "pid") |>
  filter(!is.na(local_timezone)) |>
  mutate(
    # Calculate DST-aware offset for each timestamp
    offset_start = get_dst_offset(session_start, country, local_timezone),
    offset_end = get_dst_offset(session_end, country, local_timezone),
    # Add offset to get local time values (keeping UTC label for compatibility)
    start_local = session_start + offset_start,
    end_local = session_end + offset_end,
    duration_min = as.numeric(difftime(
      session_end,
      session_start,
      units = "mins"
    ))
  ) |>
  filter(
    !is.na(session_start),
    !is.na(session_end),
    session_end > session_start,
    duration_min >= 1
  )

# --- 2) HOURLY (Nintendo + Xbox expanded) ----------------------------------
# expand sessions into local-hour bins, compute overlap minutes, and add UTC hour
hourly_from_sessions <- session_telemetry |>
  filter(!is.na(start_local), !is.na(end_local)) |>
  mutate(
    h0_local = floor_date(start_local, "hour"),
    h1_local = floor_date(end_local - seconds(1), "hour"),
    n_hours = as.integer(difftime(h1_local, h0_local, units = "hours")) + 1
  ) |>
  filter(!is.na(n_hours), n_hours > 0) |>
  tidyr::uncount(n_hours, .remove = FALSE, .id = "k") |>
  mutate(
    hour_start_local = h0_local + hours(k - 1),
    minutes = pmax(
      0,
      as.numeric(difftime(
        pmin(end_local, hour_start_local + hours(1)),
        pmax(start_local, hour_start_local),
        units = "mins"
      ))
    ),
    # Convert back to UTC (this preserves the instant, just changes label)
    hour_start_utc = with_tz(hour_start_local, tzone = "UTC")
  ) |>
  select(pid, platform, title_id, hour_start_local, hour_start_utc, minutes) |>
  group_by(pid, platform, title_id, hour_start_local, hour_start_utc) |>
  summarise(minutes = sum(minutes, na.rm = TRUE), .groups = "drop")

hourly_from_steam <- steam |>
  mutate(
    datetime_hour_start = ymd_hms(
      paste0(date, " ", sprintf("%02d", hour), ":00:00"),
      tz = "UTC"
    )
  ) |>
  select(pid, title_id, datetime_hour_start, minutes) |>
  left_join(tz_map, by = "pid") |>
  filter(!is.na(local_timezone)) |>
  mutate(
    platform = "Steam",
    hour_start_utc = datetime_hour_start,
    offset = get_dst_offset(datetime_hour_start, country, local_timezone),
    # Note: These are local time values with UTC labels due to R's POSIXct limitation
    hour_start_local = datetime_hour_start + offset
  ) |>
  select(pid, platform, title_id, hour_start_local, hour_start_utc, minutes) |>
  group_by(pid, platform, title_id, hour_start_local, hour_start_utc) |>
  summarise(minutes = sum(minutes, na.rm = TRUE), .groups = "drop")

hourly_telemetry <- bind_rows(hourly_from_sessions, hourly_from_steam)

# --- 3) DAILY (Nintendo + Xbox + Steam; collapse hourly to days) -----------
daily_telemetry <- hourly_telemetry |>
  mutate(
    day_local = as.Date(hour_start_local),
  ) |>
  group_by(pid, platform, day_local) |>
  summarise(minutes = sum(minutes, na.rm = TRUE), .groups = "drop") |>
  bind_rows(ios) |>
  bind_rows(android)

# --- 4) weekly
weekly_all <- daily_telemetry |>
  mutate(
    week = floor_date(day_local, "week")
  ) |>
  group_by(pid, platform, week) |>
  summarise(minutes = sum(minutes, na.rm = TRUE), .groups = "drop")

telemetry_spans <- daily_telemetry |>
  group_by(pid, platform) |>
  summarise(
    telemetry_start = min(day_local, na.rm = TRUE),
    telemetry_end = max(day_local, na.rm = TRUE) + hours(1), # end of last hour bin
    week = floor_date(telemetry_end, "week"),
    n_weeks = as.integer(difftime(
      telemetry_end,
      telemetry_start,
      units = "weeks"
    )) +
      1,
    .groups = "drop"
  )

```

```{r}
#| label: summary-stats

n_baseline <- nrow(intake[intake$qualified, ])
n_participants_unf <- sum(unique(c(daily$pid, biweekly$pid)) %in% intake$pid)
n_participants <- print_num(n_participants_unf)
participant_age <- (function(x) {
  sprintf(
    "%s years old (10th percentile: %s, 90th percentile: %s)",
    print_num(median(x), .1),
    print_num(quantile(x, 0.10), .1),
    print_num(quantile(x, 0.90), .1)
  )
})(na.omit(intake$age[intake$qualified]))


n_daily <- print_num(nrow(daily))
n_biweekly <- print_num(nrow(biweekly))
n_sessions <- print_num(nrow(nintendo) + nrow(xbox) + nrow(steam))
n_hours <- print_num(
  (sum(nintendo$duration, na.rm = TRUE) +
    sum(xbox$duration, na.rm = TRUE) +
    sum(steam$minutes, na.rm = TRUE) +
    sum(ios$minutes, na.rm = TRUE) +
    sum(android$minutes, na.rm = TRUE)) /
    60
)

n_platforms <- table(
  str_count(
    intake$linked_platforms[!is.na(intake$linked_platforms)],
    ","
  ) +
    1
)

third_party_prop <- paste0(
  print_num((1 - median(intake$nintendo_first_party_prop, na.rm = T)) * 100, 1),
  "%"
)
n_hours <- number(
  sum(daily_telemetry$minutes / 60),
  accuracy = 0.1,
  big.mark = ",",
  scale_cut = cut_long_scale()
)
n_users <- number(n_distinct(daily_telemetry$pid), accuracy = 1, big.mark = ",")
n_games <- number(
  n_distinct(hourly_telemetry$title_id),
  accuracy = 1,
  big.mark = ","
)
n_months <- number(max(telemetry_spans$n_weeks) / 4.2, accuracy = 1)
n_surveys <- number(
  nrow(daily) + nrow(biweekly),
  accuracy = 1,
  big.mark = ",",
  scale_cut = cut_long_scale()
)
n_simon <- number(
  nrow(simon),
  accuracy = .1,
  big.mark = ",",
  scale_cut = cut_long_scale()
)

prop_duplicate_daily <- number(
  100 *
    (1 - (nrow(daily) / nrow(daily_raw[!daily_raw$bpnsfs_failed_att_check, ]))),
  accuracy = .1
)

prop_duplicate_biweekly <- number(
  100 *
    (1 -
      (nrow(biweekly) /
        nrow(biweekly_raw[!biweekly_raw$bangs_failed_att_check, ]))),
  accuracy = .1
)

prop_failed_daily_attn <- number(
  sum(daily_raw$bpnsfs_failed_att_check) / nrow(daily_raw) * 100,
  accuracy = .1
)

prop_failed_biweekly_attn <- number(
  sum(biweekly_raw$bangs_failed_att_check) / nrow(biweekly_raw) * 100,
  accuracy = .1
)

prop_invalid_xbox <- number(
  (1 - (nrow(xbox_valid_sessions) / nrow(xbox_raw))) * 100,
  accuracy = 0.1
)
prop_invalid_nintendo <- number(
  (1 - (nrow(nintendo_valid_sessions) / nrow(nintendo_raw))) * 100,
  accuracy = 0.1
)

prop_suspicious_xbox <- number(
  (1 - (nrow(xbox) / nrow(xbox_valid_sessions))) * 100,
  accuracy = 0.1
)
prop_suspicious_nintendo <- number(
  (1 - (nrow(nintendo) / nrow(nintendo_valid_sessions))) * 100,
  accuracy = 0.1
)

prop_overlapping_steam <- "TODO"

n_timeuse_entries_per_day <- number(
  time_use |>
    group_by(pid, date) |>
    summarise(n_activites = n()) |>
    summarise(median = median(n_activites, na.rm = TRUE)) |>
    summarise(median = median(median, na.rm = TRUE)) |>
    pull(median),
  accuracy = 1
)

platforms_count <- str_count(
  replace_na(intake$linked_platforms[intake$qualified], ""),
  ","
) +
  1
n_one_platform <- sum(platforms_count == 1, na.rm = TRUE)
n_two_platforms <- sum(platforms_count == 2, na.rm = TRUE)
n_three_platforms <- sum(platforms_count >= 3, na.rm = TRUE)

# Steam quality metrics
steam_qc <- read_csv("data/qc/steam_quality_metrics.csv", show_col_types = FALSE)
prop_steam_hours_exceeding <- number(
  steam_qc$prop_hours_exceeding * 100,
  accuracy = 0.1
)
prop_steam_segments_scaled <- number(
  steam_qc$prop_segments_scaled * 100,
  accuracy = 0.1
)
mean_minutes_reduced <- number(steam_qc$mean_minutes_reduced, accuracy = 0.1)
median_minutes_reduced <- number(steam_qc$median_minutes_reduced, accuracy = 0.1)

```

---
abstract: |
  A major limitation to understanding digital technology use, and its potential psychological consequences, is the lack of sufficiently detailed, multidimensional, and accurate data. We present a dataset of `{r} n_participants` individuals' video game play telemetry data from Nintendo Switch, Steam, and Xbox, paired with psychological measures across multiple dimensions of mental health, motivations, well-being, and cognitive ability. The data were collected under a preregistered design that included 12 weeks of survey data (thirty daily surveys, six biweekly surveys, three biweekly cognitive tests), and digital trace data for `{r} n_months` months. Cleaned data include `{r} n_hours` hours of video game play across `{r} n_games` titles, `{r} n_surveys` responses to 14 survey instruments, and `{r} n_simon` attention ability measures to facilitate examining longitudinal associations between play behaviors and psychological functioning. Data and codebook are available under a {{< meta data-license >}} license at {{< meta data-url >}}.
---

# Introduction

Digital trace data---behavioral logs automatically collected by digital devices and online platforms---are necessary to better understand technology use and its psychological and health effects [@BurgessEtAl2024Potential; @Freelon2014Interpretation; @GriffioenEtAl2020improveda]. Self-reports of technology use do not accurately reflect objective technology use [@ParryEtAl2021systematic; @sewallRoleDepressionDiscrepancy2021], and are unsuitable for examining many phenomena of interest, such as seasonal patterns over long temporal horizons; high-frequency behavioral analysis at the level of hours, minutes, or even seconds; historical content analysis; and others due to limited accuracy and temporal resolution. Moreover, when technology use's relations to psychological survey instruments are of interest, digital trace data removes the possibility of common methods bias.

To combat these issues, digital trace data is increasingly used in studies on the psychological effects of smartphone and social media [e.g. @sewallDoesObjectivelyMeasured2022; @SiebersEtAl2024Adolescents; @YapEtAl2024Digital]. But despite the rapid growth of research using trace data from social media and smartphones, comparable efforts in video games remain rare. This absence is notable given that games constitute one of the most popular and psychologically rich forms of digital media, engaging billions of players worldwide across diverse genres, platforms, and social contexts [@Kaye2019Gaming; @EntertainmentSoftwareAssociation20242024, @Ofcom2023Online]. As a result, our understanding of video game play is often limited to narrow slices of behavior---single titles, single platforms, or small samples--leaving open questions about how gaming affects and interacts with people’s everyday lives.

Existing use of trace data in video games typically takes the form of either game analytics research, often conducted in collaboration with industry at the level of an individual game, typically focused on industry-relevant behavioral rather than harms and benefits; or narrow-scope investigations into play and health within a single game or gaming platform (e.g., Xbox). Each of these approaches has limitations that constrain their utility for understanding gaming behavior in the wild.

Game analytics research has made substantial contributions to understanding player behavior, engagement, and game design processes [@SifaEtAl2021LargeScale; @ElsonEtAl2014More]. However, these studies are often conducted within commercial partners using proprietary data that are not publicly shared [@KahnEtAl2014why; @LiuEtAl2024Connecting], which limits transparency, reproducibility, and opportunities for independent reanalysis. Moreover, because their objectives often center on optimizing player experience, design, or monetization rather than examining psychological or health outcomes [e.g., @RattingerEtAl2016Integrating; @CanossaEtAl2019influencers], findings may not speak directly to questions of wellbeing. In other cases, game analytics is conducted on fully anonymized data wherein researchers have access to detailed behavioral logs but little information about who the players are or survey data that could provide indicators of mental health and motivation [@ZendleEtAl2023Crosscultural; @VuorreEtAl2021largescale].

Trace data research into games and wellbeing comprises a much smaller part of the literature with its own set of key limitations. Where trace data has been used in video games studies, it has largely been limited to a few games [@VuorreEtAl2022Time; @JohannesEtAl2021Video; @LarrieuEtAl2023How; @PerryEtAl2018onlineonly; @LiuEtAl2024Connecting] or single platform [@BallouEtAl2024Registered; @BallouEtAl2025Perceived]. However, players commonly play many games on multiple platforms. This fragmentation obscures a key part of real-world behavior: players’ shifting engagement patterns across consoles, PCs, and mobile devices.

A multi-platform approach enables (1) more diverse samples by allowing researchers to recruit from more than one gaming platform whose players differ in both gaming engagement and wider demographics [@VahloEtAl2017Digital; @EntertainmentSoftwareAssociation20242024], and (2) more comprehensive and ecologically valid investigations of how and when people play, whether different platforms substitute or complement one another, and how these choices relate to daily routines or broader temporal rhythms (e.g., weekends, seasons, or daylight variation).

Taken together, these considerations underscore the urgent need for comprehensive (i.e., multi-platform) and transparent data to advance the scientific study of video game play. The dataset presented here directly addresses this need by combining longitudinal telemetry from multiple gaming platforms with rich psychological and cognitive measures collected under preregistered, ethically approved conditions. We leverage both industry-facilitated data access from major gaming platforms and open-source, participant-driven tools [@KingPersily2020New; @Xiao2023Debate] to achieve unprecedented coverage of naturalistic play. By making these data openly available, we aim to provide a foundation for cumulative, methodologically rigorous, and theory-driven research on digital play and its role in everyday life.

Here, we present a longitudinal dataset of digital trace data across multiple gaming platforms consisting of `r n_participants` participants, `r n_daily` daily and `r n_biweekly` biweekly surveys, `r n_simon` behavioral attention assessments, and `r n_hours` gameplay hours distributed across `r n_sessions` sessions. Digital trace data was sourced from five platforms---Xbox, Nintendo, Steam, iOS, and Android---through distinct pipelines detailed below. The data collection methods were preregistered as part of a Stage 1 registered report (<https://osf.io/pb5nu>).

The dataset is openly available under a {{< meta data-license >}} license at <https://doi.org/10.5281/zenodo.17536656> for minimally restricted reuse. @tbl-overview shows a high-level overview of the dataset.

::: {.place arguments='top, scope: "parent", float: true'}
```{r}
#| label: tbl-overview
#| tbl-cap: "Overview of the dataset and sources."

survey_pids <- unique(c(biweekly$pid, daily$pid))

steam_survey <- steam |> filter(pid %in% survey_pids)
nintendo_survey <- nintendo |> filter(pid %in% survey_pids)
xbox_survey <- xbox |> filter(pid %in% survey_pids)
ios_survey <- ios |> filter(pid %in% survey_pids)
android_survey <- android |> filter(pid %in% survey_pids)

overview <- bind_rows(
  summ_basic(intake, pid, "Intake survey", avg_days = 1),
  summ_basic(
    daily,
    pid,
    "Daily surveys",
    avg_days = nrow(daily) / n_distinct(daily$pid)
  ),
  summ_basic(
    biweekly,
    pid,
    "Biweekly surveys",
    avg_days = nrow(biweekly) / n_distinct(biweekly$pid)
  ),
  summ_basic(
    time_use |>
      distinct(pid, date) |>
      mutate(events = 1),
    pid,
    "Time use diaries",
    avg_days = time_use |>
      group_by(pid) |>
      summarise(n_days = n_distinct(date)) |>
      pull(n_days) |>
      median(na.rm = TRUE)
  ),
  summ_basic(
    simon,
    pid,
    "Attention tasks",
    avg_days = nrow(simon) / n_distinct(simon$pid)
  ),
  summ_basic(
    steam_survey,
    pid,
    "Steam",
    avg_days = avg_days_platform("Steam", survey_pids)
  ),
  summ_basic(
    nintendo_survey,
    pid,
    "Nintendo",
    avg_days = avg_days_platform("Nintendo", survey_pids)
  ),
  summ_basic(
    xbox_survey,
    pid,
    "Xbox",
    avg_days = avg_days_platform("Xbox", survey_pids)
  ),
  summ_basic(ios_survey, pid, "iOS", avg_days = avg_days_platform("iOS")),
  summ_basic(
    android_survey,
    pid,
    "Android",
    avg_days = avg_days_platform("Android")
  )
) |>
  rename(`N survey\nsubj.` = Participants)

# ---- telemetry totals (all users) ----
telemetry_pairs <- list(
  list(label = "Steam", full = steam, overlap = steam_survey),
  list(label = "Nintendo", full = nintendo, overlap = nintendo_survey),
  list(label = "Xbox", full = xbox, overlap = xbox_survey),
  list(label = "iOS", full = ios, overlap = ios_survey),
  list(label = "Android", full = android, overlap = android_survey)
)

telemetry_stats <- map_dfr(telemetry_pairs, function(x) {
  by_pid_full <- x$full |> count(pid, name = "events")
  tibble(
    `Data type` = x$label,
    `N users w/ trace data` = n_distinct(x$full$pid),
    `Median events / user (all telemetry)` = median(
      by_pid_full$events,
      na.rm = TRUE
    )
  )
})

# Join + replace telemetry medians; add telemetry-only participants
overview <- overview |>
  left_join(telemetry_stats, by = "Data type") |>
  mutate(
    `Median events / user` = coalesce(
      `Median events / user (all telemetry)`,
      `Median events / user`
    ),
    `N users w/ trace data` = coalesce(`N users w/ trace data`, NA_real_)
  ) |>
  select(-`Median events / user (all telemetry)`)

overview <- overview |>
  mutate(
    `N survey\nsubj.` = number(
      `N survey\nsubj.`,
      accuracy = 1,
      big.mark = ","
    ),
    `Total users w/ trace data` = if_else(
      is.na(`N users w/ trace data`),
      "",
      number(`N users w/ trace data`, accuracy = 1, big.mark = ",")
    ),
    `Observable Period` = case_match(
      `Data type`,
      "Intake survey" ~ "",
      "Daily surveys" ~ "30 days",
      "Biweekly surveys" ~ "10 weeks",
      "Attention tasks" ~ "8 weeks",
      "Time use diaries" ~ "30 days",
      "Steam" ~ "3-12 months",
      "Nintendo" ~ paste0(
        floor(time_length(
          interval(
            min(nintendo$session_start, na.rm = TRUE),
            max(nintendo_raw$time_of_pull, na.rm = TRUE)
          ),
          "months"
        )) +
          1,
        " months"
      ),
      "Xbox" ~ paste0(
        floor(time_length(
          interval(
            min(xbox$session_start, na.rm = TRUE),
            max(xbox$time_of_pull, na.rm = TRUE)
          ),
          "months"
        )) +
          1,
        " months"
      ),
      "iOS" ~ "12 weeks",
      "Android" ~ "12 weeks"
    ),
    Events = number(
      Events,
      accuracy = 1,
      big.mark = ",",
      scale_cut = cut_long_scale()
    ),
    Hours = number(
      Hours,
      accuracy = 0.1,
      big.mark = ",",
      scale_cut = cut_long_scale()
    ),
    `Median events / user` = number(
      `Median events / user`,
      accuracy = 0.1,
      big.mark = ",",
      scale_cut = cut_long_scale()
    ),
    `Median active days` = number(
      `Median active days`,
      accuracy = 1,
      big.mark = ",",
      scale_cut = cut_long_scale()
    )
  ) |>
  select(
    `Data type`,
    `Observable Period`,
    `N survey\nsubj.`,
    `N users w/ trace data`,
    Events,
    Hours,
    `Median events / user`,
    `Median active days`
  )

tt(overview) |>
  style_tt(fontsize = 0.7) |>
  format_tt(replace = "") |>
  style_tt(i = 0, bold = TRUE, align = "c")

```

:::

# Method

## Design

::: {.place arguments='top, scope: "parent", float: true'}
```{r}
#| label: fig-sankey
#| fig-cap: Participant flow during study intake.
#| fig-asp: 0.37
#| out-width: 95%

lvl <- c(
  "Screened",
  "Ineligible",
  "No trace data",
  "No recent trace data",
  "Did not begin surveys",
  "Linked ≥1 account",
  "Any trace data",
  "Recent trace data",
  "Began surveys"
)

node_counts <- intake |>
  mutate(
    stage1 = "Screened",
    stage2 = ifelse(is.na(linked_platforms), "Ineligible", "Linked ≥1 account"),
    stage3 = ifelse(
      pid %in% c(xbox$pid, nintendo$pid, steam$pid),
      "Any trace data",
      "No trace data"
    ),
    stage4 = ifelse(qualified, "Recent trace data", "No recent trace data"),
    stage5 = ifelse(
      pid %in% c(daily$pid, biweekly$pid),
      "Began surveys",
      "Did not begin surveys"
    ),
    .keep = "none"
  ) |>
  pivot_longer(everything(), names_to = "stage", values_to = "node") |>
  filter(!is.na(node)) |>
  count(node, name = "n")

sankey_data <- intake |>
  mutate(
    Screening = "Screened",
    Linking = ifelse(
      is.na(linked_platforms),
      "Ineligible",
      "Linked ≥1 account"
    ),
    `Account\nValidation 1` = ifelse(
      pid %in% c(xbox$pid, nintendo$pid, steam$pid),
      "Any trace data",
      "No trace data"
    ),
    `Account\nValidation 2` = ifelse(
      qualified,
      "Recent trace data",
      "No recent trace data"
    ),
    Surveys = ifelse(
      pid %in% c(daily$pid, biweekly$pid),
      "Began surveys",
      "Did not begin surveys"
    ),
    .keep = "none"
  ) |>
  make_long(
    Screening,
    Linking,
    `Account\nValidation 1`,
    `Account\nValidation 2`,
    Surveys
  ) |>
  left_join(node_counts, by = "node") |>
  mutate(
    node = factor(node, levels = rev(lvl)),
    next_node = factor(next_node, levels = rev(lvl)),
    node_label = sprintf("%s\n%s", as.character(node), n), # force character + real newline
    node_label = stringi::stri_replace_all_fixed(node_label, "\\n", "\n") # just in case
  )

ggplot(
  sankey_data,
  aes(
    x = x,
    next_x = next_x,
    node = node,
    next_node = next_node,
    fill = factor(node),
    label = node_label
  )
) +
  geom_sankey(flow.alpha = .6, node.color = "gray30") +
  # pass parse = TRUE into the label layer
  geom_sankey_label(
    parse = FALSE,
    size = 2.3,
    color = "white",
    fill = "gray40",
    vjust = 1.4
  ) +
  scale_x_discrete(position = "top") +
  coord_cartesian(clip = "off") +
  theme(
    legend.position = "none",
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.x = element_blank(),
    panel.grid = element_blank(),
    panel.border = element_blank(),
    axis.text.x = element_text(size = 8.5, color = "black")
  ) +
  scale_y_continuous(expand = expansion(c(0.2, 0.1))) +
  scale_fill_grey(drop = FALSE, start = .85, end = 0)
```

:::

The study consisted of four stages (@fig-sankey).

### Stage 1: Screening

In the first stage, we screened participants in order to find people aged 18--40 who (1) self-report playing video games, (2) self-report that at least 50% of their total video game play takes place on the platforms included in the study, and (3) were willing to link their gaming accounts to provide digital trace data. We screened participants from two panel sources: PureProfile and Prolific.

Participants were recruited under an initial set of ethnicity‐based quotas designed to mirror the general population’s demographic composition. After we reached approximately 50% of our target sample under quota constraints and found that further quota‐eligible recruits were scarce, we suspended the quotas for the remainder of data collection; all subsequent participants were enrolled on a first‐come, first‐served basis. Final sample characteristic reflect both quota‐driven and open‐enrollment phases (see below).

### Stage 2: Account Linking

Participants who were deemed eligible during screening proceeded directly to an account linking survey wherein they provided details of the gaming platforms they actively use. For UK participants, this includes Nintendo Switch, Steam, Android and iOS. For US participants, this includes the same four alongside Xbox. Details of how participants linked each type of account are shown in @tbl-platforms.

::: {.place arguments='top, scope: "parent", float: true'}

```{r}
#| label: tbl-platforms
#| tbl-cap: "Platform Details"

tibble(
  Platform = c(
    "Nintendo",
    "Xbox (US only)",
    "Steam",
    "iOS",
    "Android"
  ),
  `Data Source` = c(
    "Data-sharing agreements with Nintendo of America (US) and Nintendo of Europe (UK)",
    "Data-sharing agreement with Microsoft",
    "Custom web app (Gameplay.Science)",
    "iOS Screen Time Screenshots",
    "Digital Wellbeing Screenshots"
  ),
  `Account Linking Process` = c(
    "Participants share an identifier contained within a QR code on Nintendo web interface. Nintendo of America/Europe uses this identifier to retrieve gameplay data and share it with the research team.",
    "Participants consent to data sharing by opting in to the study on Xbox Insiders with their Xbox account. Microsoft retrieved and shared pseudonymized gameplay data for all consented accounts.",
    "Using a web app we developed (https://gameplay.science), participants consented to have their gameplay data monitored for the duration of the study. Authentication uses the official Steam API (OpenID).",
    "Screenshots from the iOS Screen Time app, showing details of up to 3 prior weeks of gaming. Data was extracted using OCR.",
    "Screenshots from the Digital Screen Time interface (if available), showing details of up to 3 prior weeks of gaming. Data was extracted using OCR."
  ),
  `Type of Data Collected` = c(
    "Session records (what game was played, at what time, for how long) for first-party games only (games published in whole or in part by Nintendo).",
    "Session records (what game was played, at what time, for how long). Game titles were replaced with a random persistent identifier, but genre(s) and age ratings are shared.",
    "Hourly aggregates per game (every hour, the total time spent playing during the previous hour)",
    "Daily aggregates (e.g., 2 total hours of gaming)",
    "Daily aggregates (e.g., 2 total hours of gaming)"
  )
) |>
  tt(
    escape = TRUE,
    notes = list(
      "a" = list(
        i = 1,
        j = 3,
        text = "See https://accounts.nintendo.com/qrcode."
      ),
      "b" = list(
        i = 1,
        j = 4,
        text = "Nintendo-published games accounted for 63% of Switch playtime in our sample."
      ),
      "c" = list(
        i = 2,
        j = 3,
        text = "See https://support.xbox.com/en-US/help/account-profile/manage-account/guide-to-insider-program."
      )
    )
  ) |>
  style_tt(fontsize = 0.7) |>
  style_tt(i = 0, bold = TRUE, align = "c")
```

:::

### Stage 3: Account Validation

After players completed the account linking process, we checked each account for evidence of valid gaming—specifically, records of active gameplay sessions on one or more of Steam, Xbox, and Nintendo within the 2 weeks before survey completion. Participants who did not have recent, valid telemetry on any console platform were excluded from the rest of the study.

### Stage 4: Surveys

```{r}
#| label: fig-design
#| fig-cap: !expr paste0("Survey administration schedule across the 12-week study period. Participants completed biweekly surveys (orange) every two weeks, with US participants additionally completing daily surveys (blue) for the first 30 days. Cognitive tests (green) were administered during biweekly surveys at weeks 1, 5, and 9. Gray circles indicate days with no scheduled surveys. Retention percentages show the proportion of baseline participants (N=", n_participants_unf, ") who were still active at each measurement week (defined as having completed either a daily diary or biweekly survey at any time after that week).")

knitr::include_graphics("figures/design.svg")
```

Eligible participants were invited to complete 6 waves of biweekly surveys, one every two weeks (@fig-design). US participants were additionally invited to complete daily surveys for 30 days, concurrently with the first biweekly surveys. During waves 1, 3, and 5, a cognitive task was also administered within the biweekly survey.

Daily survey links were sent every day at 2pm local time for the participant and remained available until 3am. Biweekly survey links were sent every second week from the first day of the study at 12pm and remained available for 96 hours.

## Participants

::: {.place arguments='auto, scope: "parent", float: true'}
```{r}
#| label: tbl-demographics
#| tbl-cap: "Sample demographics for qualified participants with general-population benchmarks (where available)."

# programmatically getting the census figures was a nightmare, and this isn't the purpose of the manuscript
# US ethnicity: https://usafacts.org/data/topics/people-society/population-and-demographics/our-changing-population/
# US gender**: https://williamsinstitute.law.ucla.edu/wp-content/uploads/Trans-Pop-Update-Aug-2025.pdf
# US education**: https://www2.census.gov/programs-surveys/demo/tables/educational-attainment/2021/cps-detailed-tables/table-1-1.xlsx and https://data.worldbank.org/indicator/SE.TER.CUAT.MS.ZS?locations=US
# UK gender: https://www.ons.gov.uk/peoplepopulationandcommunity/culturalidentity/genderidentity/articles/genderidentityageandsexenglandandwalescensus2021/2023-01-25
# UK ethnicity: https://www.ethnicity-facts-figures.service.gov.uk/uk-population-by-ethnicity/national-and-regional-populations/population-of-england-and-wales/latest/
# UK education:
# **values are from the 18-40 population where possible

GENPOP <- tibble::tribble(
  ~Variable                                    ,
  ~Level                                       ,
  ~`US Gen Pop %`                              ,
  ~`UK Gen Pop %`                              ,
  # --- Gender ---
  "Gender"                                     ,
  "Man"                                        ,
  49.0                                         ,
  49.6                                         ,
  "Gender"                                     ,
  "Woman"                                      ,
  48.9                                         ,
  49.5                                         ,
  "Gender"                                     ,
  "Other gender identity"                      ,
   2.1                                         ,
    .9                                         ,
  # --- Ethnicity ---
  "Ethnicity"                                  ,
  "White"                                      ,
  75.5                                         ,
  81.7                                         ,
  "Ethnicity"                                  ,
  "Black"                                      ,
  13.6                                         ,
   4.0                                         ,
  "Ethnicity"                                  ,
  "Asian"                                      ,
   6.3                                         ,
   9.3                                         ,
  "Ethnicity"                                  ,
  "Two or More Races"                          ,
   3                                           ,
   2.9                                         ,
  "Ethnicity"                                  ,
  "American Indian and Alaska Native"          ,
   1.3                                         ,
   0                                           ,
  "Ethnicity"                                  ,
  "Native Hawaiian and Other Pacific Islander" ,
    .3                                         ,
   0                                           ,
  "Ethnicity"                                  ,
  "Other"                                      ,
   0                                           ,
   2.1                                         ,
  # --- Education (3 buckets) ---
  "Education"                                  ,
  "Completed secondary or less"                ,
  61.0                                         ,
  60.1                                         ,
  "Education"                                  ,
  "Bachelor's degree"                          ,
  24.0                                         ,
  25.5                                         ,
  "Education"                                  ,
  "Postgraduate"                               ,
  15.0                                         ,
  14.4
)

age_string <- intake |>
  filter(qualified) |>
  summarise(
    n = n(),
    mean = round(mean(age, na.rm = TRUE), 1),
    sd = round(sd(age, na.rm = TRUE), 1),
    min = min(age, na.rm = TRUE),
    p25 = quantile(age, 0.25, na.rm = TRUE),
    median = median(age, na.rm = TRUE),
    p75 = quantile(age, 0.75, na.rm = TRUE),
    max = max(age, na.rm = TRUE)
  ) |>
  mutate(
    across(everything(), as.character),
    age_string = paste0(
      median,
      " (mean = ",
      mean,
      ", SD = ",
      sd,
      ", range ",
      min,
      "-40)"
    )
  ) |>
  pull(age_string)

demo <- intake |>
  filter(qualified) |>
  mutate(
    ethnicity = case_match(
      ethnicity,
      c("Other ethnic group", "Others", "Some Other Race alone") ~ "Other",
      c("White", "White alone") ~ "White",
      c(
        "Black or African American alone",
        "Black, African, Caribbean or Black British"
      ) ~
        "Black",
      "Native Hawaiian and Other Pacific Islander alone" ~
        "Native Hawaiian and Other Pacific Islander",
      "American Indian and Alaska Native alone" ~
        "American Indian and Alaska Native",
      c("Asian", "Asian alone", "Asian or Asian British") ~ "Asian",
      c("Mixed or multiple ethnic groups", "Mixed", "Two or More Races") ~
        "Two or More Races",
      .default = NA
    ),

    edu_bucket = case_when(
      # Completed secondary or less
      edu_level %in%
        c(
          "Apprenticeships",
          "Associate degree",
          "Completed Primary School",
          "Completed Secondary School",
          "Five or more GCSE passes (grade A* to C or grade 4 and above) or equivalent qualifications",
          "High school graduate or equivalent",
          "Less than a high school diploma or equivalent",
          "No formal qualifications",
          "One to four GCSE passes (grade A* to C or grade 4 and above) and any other GCSEs at other grades, or equivalent qualifications",
          "Other qualifications",
          "Some college but no degree",
          "Some University but no degree",
          "Two or more A Levels or equivalent qualifications",
          "Vocational or Similar"
        ) ~
        "Completed secondary or less",

      # Bachelor's degree
      edu_level %in%
        c(
          "Bachelor's degree",
          "University Bachelors Degree",
          "Higher National Certificate, Higher National Diploma, Bachelor's degree, or post-graduate qualifications"
        ) ~
        "Bachelor's degree",

      # Postgraduate
      edu_level %in%
        c(
          "Master's degree, professional degree, or doctoral degree",
          "Graduate or professional degree (MA, MS, MBA, PhD, etc)"
        ) ~
        "Postgraduate",

      # Not usable / missing
      edu_level %in% c("Prefer not to say") ~ NA_character_,

      TRUE ~ NA_character_
    ),

    gender = ifelse(
      !gender %in% c("Man", "Woman"),
      "Other gender identity",
      gender
    )
  ) |>
  select(country, age, gender, ethnicity, edu_bucket) |>
  pivot_longer(
    c(gender, ethnicity, edu_bucket),
    names_to = "Variable",
    values_to = "Level"
  ) |>
  filter(!is.na(Level)) |>
  mutate(
    Variable = recode(
      Variable,
      gender = "Gender",
      ethnicity = "Ethnicity",
      edu_bucket = "Education"
    ),
  ) |>
  count(country, Variable, Level) |>
  group_by(country, Variable) |>
  mutate(pct = round(100 * n / sum(n), 1)) |>
  ungroup() |>
  select(Variable, Level, country, pct) |>
  mutate(country = paste0(country, " Sample %")) |>
  pivot_wider(names_from = country, values_from = pct) |>
  mutate(`UK Sample %` = coalesce(`UK Sample %`, 0)) |>
  arrange(desc(Variable), desc(`UK Sample %`)) |>
  left_join(GENPOP, by = c("Variable", "Level")) |>
  select(
    Variable,
    Level,
    `US Sample %`,
    `US Gen Pop %`,
    `UK Sample %`,
    `UK Gen Pop %`
  ) |>
  group_by(Variable) |>
  mutate(Variable = ifelse(row_number() == 1, Variable, ""))

data_type_starts <- c(1, 4, 11)

tt(
  demo,
  caption = "Demographic characteristics of qualified participants alongside general population benchmarks (where available). See supplementary materials for sources of general population estimates."
) |>
  style_tt(fontsize = 0.7) |>
  style_tt(
    i = data_type_starts,
    j = 1,
    bold = TRUE,
    line = "t",
    line_width = 0.12
  ) |>
  style_tt(i = 0, bold = TRUE, align = "c")

```

:::

Our final sample consists of `r nrow(intake[intake$qualified,])` qualified participants, selected from a pool of `r nrow(intake)` screened participants. Of the `r nrow(intake[intake$qualified,])` with recent telemetry, `r n_participants_unf` also completed at least one survey. On average, participants were `r participant_age`. Due to errors in screening, four participants over the age of 40 were included. 

Among our participants, `r n_one_platform` linked digital trace data on one gaming platform, `r n_two_platforms` linked two gaming platforms, and `r n_three_platforms` linked three or more platforms. 

@tbl-demographics shows demographic characteristics of the final sample alongside general population benchmarks. Our participants are more likely to be male, non-binary, and bi- or multiracial than the general population. Although the demographics of the population of people who play video games are less well understood, our sample's demographics are broadly consistent with previously reported data [e.g., @EntertainmentSoftwareAssociation20242024].

Besides those appearing in @tbl-demographics, we collected various other demographic variables from eligible participants at intake, including employment status, height and weight, self-identified and diagnosed neurodivergence (e.g., ASD, ADHD, dyslexia), political party affiliation, marital status, caretaking responsibilities, and postal geography (general area only; first three digits of the five-digit US ZIP Code; UK outward code). Further details of these are available in the online codebook.

## Ethics and Compensation

This study received ethical approval from the Social Sciences and Humanities Inter-Divisional Research Ethics Committee at the University of Oxford (OII_CIA_23_107). All participants provided informed consent at the start of the study, including consent to their data being shared openly for reanalysis.

The data released here is pseudonymized for participant privacy. However, we recognize that the possibility of reidentification cannot be ruled out due to the detailed demographics present in the dataset alongside the digital trace data. With careful consideration of the risks [e.g., @ShawEtAl2025DECIDE], we elect to share the full data, owing to several factors: (1) participants were thoroughly briefed on the procedure and directly consented to their data being pseudonymously shared; (2) for sensitive items (e.g., political affiliation), participants could select "prefer not to share"; (3) evidence suggests even a pared-down version of our dataset with less participant information may be reidentifiable based on trace data alone [e.g., @SekaraEtAl2021Temporal]; (4) data-sharing agreements with Nintendo and Microsoft specifically prohibit the companies themselves from connecting our survey data back to their telemetry; (5) the harms associated with reidentification are moderate, and (6) the research community has a strong interest in open data for reproducibility and cumulative science.

To balance openness with ethical responsibility, the dataset is released under a modified CC0 license that retains full reuse rights but includes an additional No-Reidentification Clause, requiring users to follow standard research ethics and prohibiting any attempt to reidentify participants.

Prolific participants were paid at a rate of £12/hour for all study components, which equates to: £0.20 for a 1-minute screening, £2 for the 10-minute intake survey (plus £5 for linking at least one account with recent data), £0.80 for each 4-minute daily survey, and £2 for each 10-minute biweekly survey. Participants received £10 bonus payments for completing at least 24 out of 30 daily surveys and/or 5 out of 6 biweekly surveys.


# Dataset

## Digital Trace Data

As described above, we collected video game play data from five platforms: Xbox, Nintendo Switch, Steam, iOS, and Android (full details in @tbl-platforms). To recap, on Xbox and Nintendo, we have *session-level* data, characterized by the following fields: a game ID (Xbox) or title (Nintendo), a start and end time, and genre(s). On Steam we have *hourly game-level aggregates* - every hour, how much time people spent playing each game they played that hour. On iOS and Android, we have *daily aggregates* - every day, how much time people spent playing games as a whole. We describe each platform in more detail below. For concision, we do not repeat the details of @tbl-platforms here, but direct readers to that table or our supplementary materials for the exact variables in each platform's trace data.

To prepare the data, we first merged adjacent sessions of the same game, removed sessions of under 1 minute (reflective of artifacts such as turning on a console that had been previously paused in-game before immediately switching to a new game) and those that took place in non-games (e.g., streaming services or storefronts). These preprocessing steps resulted in the joining or removal of `r prop_invalid_nintendo`% of Nintendo rows, and `r prop_invalid_xbox`% of Xbox rows---the vast majority of which were under 5 minutes. Orphaned and non-game sessions are not present on other platforms. We then further filtered suspicious sessions or days (see Data Quality below).

In @fig-heatmap, we visualize the distribution of play across days and times. As expected, we find that the likelihood of play peaks on weekends from 8-11pm local time for each participant, and is lowest in the early morning.

::: {.place arguments='top, scope: "parent", float: true'}
```{r}
#| label: fig-heatmap
#| fig-cap: "Diurnal play across Xbox, Nintendo, Steam."
#| fig-asp: 0.35
#| out-width: 80%
#| dev: svglite

# expected number of occurrences of each slot in telemetry span
slot_exposure <- telemetry_spans |>
  rowwise() |>
  mutate(n_weeks = interval(telemetry_start, telemetry_end) %/% weeks(1) + 1) |>
  ungroup() |>
  crossing(dow = 1:7, hour = 0:23) |>
  mutate(n_occurrences = n_weeks) # each slot appears once per week

# total minutes actually played per slot
slot_play <- hourly_telemetry |>
  mutate(
    dow = lubridate::wday(hour_start_local, week_start = 1),
    hour = hour(hour_start_local)
  ) |>
  group_by(pid, platform, dow, hour) |>
  summarise(
    minutes = pmin(sum(minutes, na.rm = TRUE), 60 * n()),
    .groups = "drop"
  )

# combine: per-player–platform average minutes per occurrence
slot_per_player <- slot_exposure |>
  left_join(slot_play, by = c("pid", "platform", "dow", "hour")) |>
  mutate(
    minutes = replace_na(minutes, 0),
    avg_minutes = minutes / n_occurrences
  )

# final: probability across players (avg minutes / 60)
slot_summary <- slot_per_player |>
  group_by(dow, hour) |>
  summarise(
    prob_playing = mean(avg_minutes / 60),
    n_active = n(),
    .groups = "drop"
  ) |>
  mutate(
    dow = factor(
      dow,
      levels = 1:7,
      labels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"),
      ordered = TRUE
    )
  )


ggplot(slot_summary, aes(x = hour, y = dow, fill = prob_playing)) +
  geom_tile() +
  scale_x_continuous(
    breaks = 0:23,
    expand = expansion(0)
  ) +
  scale_fill_viridis_c(
    option = "magma",
    labels = scales::percent_format(accuracy = .1),
    limits = c(0, .10),
    breaks = seq(0, .10, .02),
    name = "Probability of\nactive play"
  ) +
  labs(
    x = "Hour (local)",
    y = "Day",
  ) +
  theme(
    panel.grid.major.y = element_blank(),
    axis.text.x = element_text(size = 8.5)
  )

```

:::

### Self-reported Gaming

We also collected self-reported gaming data in each biweekly survey. Participants estimated the time they spent playing games on platforms they had linked during the study (e.g., excluding other platforms such as Playstation) in each of the following periods: last 24 hours, last 7 days, and last 14 days. In addition, participants reported details of at least 1 and up to 3 of their most recent gaming sessions (game, date, and start/end time).

@fig-paired-dotplot compares the average self-reported distribution of play across platforms to the distribution in our digital trace data capture. It is vital to note that the self-report data not be treated as ground truth: we have good evidence that people's self-reports of media use are inaccurate [@KahnEtAl2014why; @ParryEtAl2021systematic], with some previous work finding systemic overestimation of video game play [@JohannesEtAl2021Video]. Nonetheless, it is likely that some portion of players' true gaming is systematically uncaptured, due to factors such as  missing titles (e.g., Nintendo third party), or player privacy settings (e.g., playing in invisible mode; setting certain games to private on Steam). The figure therefore provides a useful overview of the relative coverage of different platforms in our telemetry data.

::: {.place arguments='top, scope: "parent", float: true'}
```{r}
#| label: fig-paired-dotplot
#| fig-cap: "Average weekly playtime (minutes) by platform, self-reported vs telemetry (among users with telemetry). Lines connect estimates for the same platform. Telemetry calculated per user-platform, based only on the period between their first and last recorded session."
#| fig-asp: 0.4
#| out.width: 95%

# Paired dot plot: self-report vs telemetry (normalized by exposure)

intake_window <- intake |>
  transmute(
    pid,
    win_start = date %m-% months(3),
    win_end = date %m+% months(3)
  )

telemetry_platforms <- c("Steam", "Nintendo", "Xbox", "iOS", "Android")
platforms_all <- c(telemetry_platforms, "Other (uncaptured)")

# ---- TELEMETRY (avg weekly minutes among users with telemetry; normalized) ----
tele_exposure <- telemetry_spans |>
  filter(platform %in% telemetry_platforms) |>
  left_join(intake_window, by = "pid") |>
  mutate(
    telemetry_start = pmax(telemetry_start, win_start, na.rm = TRUE),
    telemetry_end = pmin(telemetry_end, win_end, na.rm = TRUE)
  ) |>
  mutate(
    exposure_weeks = as.numeric(difftime(
      telemetry_end,
      telemetry_start,
      units = "days"
    )) /
      7
  ) |>
  filter(is.finite(exposure_weeks), exposure_weeks > 4) |>
  group_by(pid, platform) |>
  summarise(exposure_weeks = sum(exposure_weeks), .groups = "drop")


tele_users <- daily_telemetry |>
  filter(platform %in% telemetry_platforms) |>
  left_join(intake_window, by = "pid") |>
  filter(day_local >= win_start, day_local <= win_end) |>
  group_by(pid, platform) |>
  summarise(total_minutes = sum(minutes, na.rm = TRUE), .groups = "drop") |>
  left_join(tele_exposure, by = c("pid", "platform")) |>
  filter(!is.na(exposure_weeks), exposure_weeks > 0) |>
  mutate(weekly_minutes = total_minutes / exposure_weeks)

tele_totals <- tele_users |>
  group_by(platform) |>
  summarise(
    minutes = mean(weekly_minutes, na.rm = TRUE),
    n_users = n_distinct(pid),
    .groups = "drop"
  ) |>
  mutate(source = "Telemetry")

# --- self-report -----

valid_pairs <- tele_users |>
  distinct(pid, platform) |>
  mutate(has_tel = TRUE)

sr_totals <- intake |>
  filter(self_reported_weekly_play < 50 * 60) |>
  filter(qualified) |>
  mutate(
    uses_Steam = str_detect(used_platforms %||% "", "Steam"),
    uses_Nintendo = str_detect(used_platforms %||% "", "Nintendo"),
    uses_Xbox = str_detect(used_platforms %||% "", "Xbox"),
    uses_iOS = str_detect(used_platforms %||% "", "iOS"),
    uses_Android = str_detect(used_platforms %||% "", "Android"),
    other_prop = coalesce(prop_playstation, 0) + coalesce(prop_other, 0),
    uses_Other = other_prop > 0 & !is.na(other_prop),
    Steam = self_reported_weekly_play * coalesce(prop_steam / 100, NA),
    Nintendo = self_reported_weekly_play * coalesce(prop_nintendo / 100, NA),
    Xbox = self_reported_weekly_play * coalesce(prop_xbox / 100, NA),
    iOS = self_reported_weekly_play * coalesce(prop_ios / 100, NA),
    Android = self_reported_weekly_play * coalesce(prop_android / 100, NA),
    `Other (uncaptured)` = self_reported_weekly_play *
      if_else(uses_Other, other_prop / 100, NA_real_)
  ) |>
  pivot_longer(
    c(`Other (uncaptured)`, Steam, Nintendo, Xbox, iOS, Android),
    names_to = "platform",
    values_to = "sr_minutes"
  ) |>
  mutate(
    uses = case_when(
      platform == "Steam" ~ uses_Steam,
      platform == "Nintendo" ~ uses_Nintendo,
      platform == "Xbox" ~ uses_Xbox,
      platform == "iOS" ~ uses_iOS,
      platform == "Android" ~ uses_Android,
      platform == "Other (uncaptured)" ~ uses_Other,
      TRUE ~ FALSE
    )
  ) |>
  filter(uses, platform %in% platforms_all) |>
  left_join(valid_pairs, by = c("pid", "platform")) |>
  filter(platform == "Other (uncaptured)" | has_tel) |>
  group_by(platform) |>
  summarise(
    minutes = mean(sr_minutes, na.rm = TRUE),
    n_users = n_distinct(pid[!is.na(sr_minutes)]),
    .groups = "drop"
  ) |>
  mutate(source = "Self-report")

# ---- COMBINE + PLOT ----
paired_summary <- bind_rows(sr_totals, tele_totals) |>
  mutate(
    platform = factor(platform, levels = platforms_all),
    source = factor(source, levels = c("Self-report", "Telemetry")),
    hours = minutes / 60,
    .keep = "used"
  )

# segments only where both SR and Telemetry exist
segment_df <- paired_summary |>
  select(platform, source, hours) |>
  pivot_wider(names_from = source, values_from = hours) |>
  # filter(!is.na(`Self-report`) & !is.na(Telemetry)) |>
  mutate(xmin = `Self-report`, xmax = Telemetry) |>
  mutate(
    platform = factor(
      platform,
      levels = c(
        "Other (uncaptured)",
        "iOS",
        "Android",
        "Nintendo",
        "Xbox",
        "Steam"
      )
    )
  )

ggplot(paired_summary, aes(x = hours, y = platform)) +
  geom_segment(
    data = segment_df,
    aes(x = xmin, xend = xmax, y = platform, yend = platform),
    color = "grey70",
    linewidth = 0.9,
    alpha = 0.8,
    inherit.aes = FALSE
  ) +
  geom_point(
    aes(shape = source, color = source, fill = source),
    size = 2.5,
    stroke = 0.7
  ) +
  scale_shape_manual(values = c("Self-report" = 24, "Telemetry" = 22)) +
  scale_color_manual(
    values = c("Self-report" = "#D95F0E", "Telemetry" = "#2B8CBE")
  ) +
  scale_fill_manual(
    values = c("Self-report" = "#D95F0E", "Telemetry" = "#2B8CBE")
  ) +
  scale_y_discrete(expand = expansion(mult = c(0.2, 0.2))) +
  scale_x_continuous(
    limits = c(0, 20),
    breaks = seq(0, 20, 4),
    expand = expansion(mult = c(0.02, 0.08))
  ) +
  guides(
    shape = guide_legend(title = NULL),
    color = guide_legend(title = NULL),
    fill = guide_legend(title = NULL),
  ) +
  labs(
    x = "Mean weekly hours per active user of platform",
    y = NULL,
  ) +
  theme(
    legend.position = "bottom",
  )

```

:::

## Survey measures

We collected a variety of self-report measures at different time scales. We briefly describe which constructs we collected here; for further details of the specific measures and example items, see @tbl-measures.

To prepare daily and biweekly survey data, we removed duplicate entries from within the same survey wave and those that had errors in the coding of waves (e.g., administration of survey wave 31 due to technical issues), in total comprising `r prop_duplicate_daily`% of daily surveys, `r prop_duplicate_biweekly`% of biweekly surveys. We further removed rows including failed attention checks (see Data Quality below), comprising `r prop_failed_daily_attn`% of daily surveys and `r prop_failed_biweekly_attn`% of biweekly surveys. After cleaning, the final survey dataset includes `r number(nrow(daily), big.mark = ",")` daily surveys and `r number(nrow(biweekly), big.mark = ",")` biweekly surveys.

*Trait / traitlike (baseline).* We assessed chronotype, Big 5 personality, player trait typology, and gaming disorder symptoms at baseline. Gaming disorder symptoms were measured twice, at biweekly waves 1 and 6.

*Daily.* Daily surveys captured: basic psychological need satisfaction and frustration in life in general; basic psychological need satisfaction and frustration in the context of video games; life satisfaction; affective valence; sleep quality; stressors; types of social gaming engaged; and self-reported displacement.

*Biweekly.* Every two weeks we assessed: general mental wellbeing; depression symptoms; life satisfaction; basic psychological need satisfaction and frustration – video games; and subjective displacement.

*Monthly (alternating biweekly).* On alternating biweekly surveys (i.e., monthly), we assessed: sleep quality; daytime sleepiness; and perceived harms and benefits of gaming.

@fig-case-study illustrates the temporal dynamics of gaming behavior and mental wellbeing through three representative case studies.

::: {.place arguments='top, scope: "parent", float: true'}
```{r}
#| label: fig-case-study
#| fig-cap: !expr 'paste0("Sample of daily gaming patterns and mental wellbeing for three representative participants. Stacked bars represent total daily playtime across platforms. Orange line shows biweekly mental wellbeing scores (short WEMWBS) measured at six study waves. Participants were selected from those closest to the 25th, 50th, and 75th percentiles of total playtime, prioritizing those with the most varied multi-platform gaming behavior. Participant IDs: ", selected_pids["p25"], " (25th percentile), ", selected_pids["p50"], " (50th percentile), ", selected_pids["p75"], " (75th percentile).")'
#| fig-asp: .8
#| out.width: 80%

# Identify participants who completed between 1 and 6 biweekly surveys
biweekly_participants <- biweekly |>
  group_by(pid) |>
  summarise(
    study_start = min(date, na.rm = TRUE),
    n_surveys = n(),
    .groups = "drop"
  ) |>
  filter(n_surveys >= 3 & n_surveys <= 6) # Only 1-6 biweekly responses

# Join study start dates and calculate study day
# Only include participants who completed between 1-6 biweekly surveys
daily_with_study_day <- daily_telemetry |>
  inner_join(biweekly_participants |> select(pid, study_start), by = "pid") |>
  mutate(
    study_day = as.integer(floor(as.numeric(difftime(
      day_local,
      study_start,
      units = "days"
    )))) +
      1
  ) |>
  filter(study_day >= 1 & study_day <= 84)

# Calculate total playtime per participant (sum across all platforms and days 1-84)
total_playtime <- daily_with_study_day |>
  group_by(pid) |>
  summarise(
    total_minutes = sum(minutes, na.rm = TRUE),
    n_platforms = n_distinct(platform[minutes > 0]),
    .groups = "drop"
  )

# Calculate percentiles
percentiles <- quantile(
  total_playtime$total_minutes,
  probs = c(0.25, 0.50, 0.75),
  na.rm = TRUE
)

# Find participant closest to each percentile with most varied platform usage
# Strategy: Among candidates within 10% of target percentile, select the one with most platforms
find_closest_multiplatform_participant <- function(target_value, data) {
  # Calculate tolerance as 10% of target value (or minimum 60 minutes for low values)
  tolerance <- max(target_value * 0.10, 60)

  # First, filter candidates within tolerance
  candidates <- data |>
    mutate(distance = abs(total_minutes - target_value)) |>
    filter(distance <= tolerance)

  # If we have candidates, select the one with most platforms, breaking ties by closest distance
  if (nrow(candidates) > 0) {
    candidates |>
      arrange(desc(n_platforms), distance) |>
      slice(1) |>
      pull(pid)
  } else {
    # Fallback: if no candidates in tolerance band, just pick closest
    data |>
      mutate(distance = abs(total_minutes - target_value)) |>
      slice_min(distance, n = 1, with_ties = FALSE) |>
      pull(pid)
  }
}

selected_pids <- c(
  "p25" = find_closest_multiplatform_participant(
    percentiles[1],
    total_playtime
  ),
  "p50" = find_closest_multiplatform_participant(
    percentiles[2],
    total_playtime
  ),
  # "p75" = find_closest_multiplatform_participant(percentiles[3], total_playtime)
  "p75" = "p7162729307" # Manually selected for more varied platform usage
)

# Process biweekly data to get wellbeing scores
# Only include participants who completed at least one biweekly survey
biweekly_wellbeing <- biweekly |>
  inner_join(biweekly_participants |> select(pid, study_start), by = "pid") |>
  mutate(
    study_day = as.integer(floor(as.numeric(difftime(
      date,
      study_start,
      units = "days"
    )))) +
      1
  ) |>
  filter(pid %in% selected_pids) |>
  # Calculate wellbeing composite scores
  rowwise() |>
  mutate(
    # WEMWBS - mental wellbeing (short version: 7 items, scale 1-5 each, summed for total score 7-35)
    wemwbs = sum(c_across(starts_with("wemwbs_")), na.rm = TRUE)
  ) |>
  ungroup() |>
  select(pid, study_day, wave, wemwbs) |>
  filter(!is.na(wemwbs) & wemwbs >= 7) |> # Remove invalid/missing wellbeing scores
  mutate(
    percentile = case_when(
      pid ==
        selected_pids["p25"] ~ "Example participant 1 (25th gaming percentile)",
      pid ==
        selected_pids["p50"] ~ "Example participant 2 (50th gaming percentile)",
      pid ==
        selected_pids["p75"] ~ "Example participant 3 (75th gaming percentile)",
      TRUE ~ NA_character_
    ),
    percentile = factor(
      percentile,
      levels = c(
        "Example participant 1 (25th gaming percentile)",
        "Example participant 2 (50th gaming percentile)",
        "Example participant 3 (75th gaming percentile)"
      )
    )
  )

# Prepare data for selected participants
plot_data <- daily_with_study_day |>
  filter(pid %in% selected_pids) |>
  mutate(
    percentile = case_when(
      pid ==
        selected_pids["p25"] ~ "Example participant 1 (25th gaming percentile)",
      pid ==
        selected_pids["p50"] ~ "Example participant 2 (50th gaming percentile)",
      pid ==
        selected_pids["p75"] ~ "Example participant 3 (75th gaming percentile)",
      TRUE ~ NA_character_
    ),
    percentile = factor(
      percentile,
      levels = c(
        "Example participant 1 (25th gaming percentile)",
        "Example participant 2 (50th gaming percentile)",
        "Example participant 3 (75th gaming percentile)"
      )
    )
  )

# Create a complete grid of study days for each participant
complete_grid <- expand_grid(
  pid = selected_pids,
  study_day = 1:84,
  platform = unique(daily_telemetry$platform)
) |>
  mutate(
    percentile = case_when(
      pid ==
        selected_pids["p25"] ~ "Example participant 1 (25th gaming percentile)",
      pid ==
        selected_pids["p50"] ~ "Example participant 2 (50th gaming percentile)",
      pid ==
        selected_pids["p75"] ~ "Example participant 3 (75th gaming percentile)",
      TRUE ~ NA_character_
    ),
    percentile = factor(
      percentile,
      levels = c(
        "Example participant 1 (25th gaming percentile)",
        "Example participant 2 (50th gaming percentile)",
        "Example participant 3 (75th gaming percentile)"
      )
    )
  )

# Fill in missing days with 0 minutes
plot_data_complete <- complete_grid |>
  left_join(
    plot_data |> select(pid, study_day, platform, minutes),
    by = c("pid", "study_day", "platform")
  ) |>
  mutate(minutes = replace_na(minutes, 0))

# Calculate biweekly-specific max playtime for free gaming time y-axis
# Wellbeing axis will be fixed at 7-35 across all biweeklies
biweekly_max_playtime <- plot_data_complete |>
  group_by(percentile, study_day) |>
  summarise(daily_total = sum(minutes, na.rm = TRUE), .groups = "drop") |>
  group_by(percentile) |>
  summarise(max_playtime = max(daily_total, na.rm = TRUE), .groups = "drop")

# Calculate global max playtime across all participants to ensure consistent wellbeing scale
global_max_playtime <- max(biweekly_max_playtime$max_playtime, na.rm = TRUE)

# Fixed wellbeing range
wb_range <- 28 # Fixed wellbeing range (35 - 7)

# Calculate fixed coefficient based on global max, ensuring wellbeing scale is consistent
# The coefficient maps wellbeing values to the playtime y-axis
coeff_global <- global_max_playtime / wb_range

# Calculate the y-axis maximum to ensure wellbeing 7-35 is fully visible
y_for_wb35 <- (35 - 7) * coeff_global
y_min_for_wellbeing <- y_for_wb35 * 1.15 # Add 15% headroom
y_axis_max_global <- max(global_max_playtime, y_min_for_wellbeing)

# Create separate plots for each percentile to handle dual y-axes properly
plots_list <- list()
pct_levels <- levels(plot_data_complete$percentile)

for (i in seq_along(pct_levels)) {
  pct_level <- pct_levels[i]
  is_bottom <- (i == length(pct_levels))
  is_middle <- (i == 2) # Middle facet for y-axis labels

  # Filter data for this percentile
  plot_data_pct <- plot_data_complete |> filter(percentile == pct_level)
  wellbeing_data_pct <- biweekly_wellbeing |> filter(percentile == pct_level)

  p_temp <- ggplot() +
    # Playtime bars (primary y-axis)
    geom_col(
      data = plot_data_pct,
      aes(x = study_day, y = minutes, fill = platform),
      width = 1
    ) +
    # Wellbeing line (secondary y-axis) - transform to playtime scale
    geom_line(
      data = wellbeing_data_pct,
      aes(x = study_day, y = (wemwbs - 7) * coeff_global, group = 1),
      color = "#FF6B35",
      linewidth = 1.2,
      linetype = "solid",
      alpha = 0.7,
      na.rm = TRUE
    ) +
    # Filled square points
    geom_point(
      data = wellbeing_data_pct,
      aes(x = study_day, y = (wemwbs - 7) * coeff_global, group = 1),
      color = "#FF6B35",
      size = 2.5,
      shape = 15, # solid square
    ) +
    scale_fill_manual(
      values = c(
        "Nintendo" = "#E60012", # Nintendo red
        "Steam" = "#215e8a", # Steam dark blue
        "Xbox" = "#107C10", # Xbox green
        "iOS" = "#b43bf5", # iOS purple
        "Android" = "#f5f53b" # Android yellow
      ),
      name = "Platform"
    ) +
    scale_x_continuous(
      name = if (is_bottom) "Study Day" else NULL,
      breaks = seq(0, 84, by = 14),
      expand = c(0, 0)
    ) +
    scale_y_continuous(
      name = if (is_middle) "Total Daily Playtime (minutes)" else NULL,
      limits = c(0, y_axis_max_global),
      expand = c(0, 0),
      sec.axis = sec_axis(
        transform = ~ . / coeff_global + 7,
        name = if (is_middle) "Wellbeing (SWEMWBS)" else NULL,
        breaks = seq(7, 35, by = 7)
      )
    ) +
    ggtitle(pct_level) +
    theme(
      plot.background = element_blank(),
      panel.background = element_rect(fill = "white", color = NA),
      panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.6),
      axis.text = element_text(size = 9),
      axis.title.y.right = element_text(color = "#FF6B35"),
      axis.text.y.right = element_text(color = "#FF6B35"),
      plot.title = element_text(
        color = "black",
        size = 10,
        hjust = 0.5
      )
    )

  plots_list[[pct_level]] <- p_temp
}

# Combine plots using patchwork
wrap_plots(plots_list, ncol = 1, guides = "collect") +
  plot_annotation() &
  theme(
    legend.position = "bottom",
    legend.key.height = unit(5, "mm"),
    legend.key.width = unit(5, "mm"),
    legend.text = element_text(size = 9),
    legend.title = element_text(size = 9)
  )
```

:::

## Attention Control

In study waves 1, 3, and 5 we measured participants' attention control using the Simon Squared task of @burgoyneNatureMeasurementAttention2023, using modified code from @liceralde23squared. Although the original Squared tasks consist of the Simon, Stroop, and Flanker Squared, due to limited participation time we chose to only use the Simon Squared task as it had the greatest factor loading on attention control in the original study [@burgoyneNatureMeasurementAttention2023].

The Simon Squared task is a short and validated measure of attention control that follows the standard Simon task [@simonAuditorySRCompatibility1967] but is completed in about three minutes. Participants see a target arrow pointing either left or right, with response labels "LEFT" and "RIGHT" printed underneath. Participants then must select the response option (e.g. "LEFT") that matches the arrow's direction (e.g. ◀︎). However, the arrow and response options can appear on either side of the screen, and participants must ignore this spatial configuration and attend only to the symbols' meanings.

After reading the instructions, participants practice for 30 seconds with auditory and text feedback for response accuracy. They then see their score from the practice trials, review the instructions again, and are given 90 seconds to gain as many points as possible. Participants gain one point for each correct response, and lose one point for each incorrect response. After the 90 seconds, the number of correct responses minus the number of incorrect responses is the participant's task score. For a complete task description, see @burgoyneNatureMeasurementAttention2023 and Figure 6 therein.


```{r}
#| label: simon-calculate-summary

simon_n <- simon |>
  reframe(n = n(), .by = wave)
simon_sdl <- simon |>
  reframe(
    mean_sdl(score_final, mult = 1),
    .by = c(wave, device_type)
  )
```

Overall, `r knitr::combine_words(simon_n$n)` participants completed the Simon task at biweekly waves 1, 3, and 5. The average performances were approximately 10 points lower than in the in-person study of @burgoyneNatureMeasurementAttention2023 (Figure 16). Participants on mobile devices generally attained lower scores than those not on mobile devices (@fig-simon, rows), but performance was generally stable across the three waves (@fig-simon, columns).

```{r}
#| label: fig-simon
#| fig-cap: Histograms of participants' Simon Squared task scores across study waves. Points and bars indicate means ±1 standard deviation.
#| fig-asp: 0.5
#| out.width: 90%

simon |>
  ggplot() +
  aes(score_final) +
  scale_y_continuous(
    "Count",
    expand = expansion(c(0, 0.1))
  ) +
  scale_x_continuous("Final score") +
  geom_histogram(fill = "grey70") +
  geom_pointrange(
    data = simon_sdl,
    aes(x = y, xmin = ymin, xmax = ymax, y = 0),
    linewidth = 3,
    fatten = 14,
    shape = "|"
  ) +
  facet_grid(
    device_type ~ wave,
    labeller = labeller(.cols = as_labeller(~ str_glue("Wave {.}"))),
    scales = "free_y"
  )
```


## Time Use

::: {.place arguments='top, scope: "parent", float: true'}
```{r}
#| label: fig-time-use
#| fig-cap: "Illustrative overview of one study day (p9730913604, Apr 4 2025), consisting of a participant's time use diary (top row), gaming as recorded through telemetry (middle row), and daily survey completion (bottom row)."
#| fig-asp: 0.35
#| out.width: 95%

person_id <- "p9730913604"
survey_day <- as.Date("2025-04-04")
window_start <- as.POSIXct(paste(survey_day - 1, "04:00:00"))
window_end <- as.POSIXct(paste(survey_day, "04:00:00"))
tz_str <- intake |>
  filter(pid == person_id) |>
  summarise(tz = first(na.omit(local_timezone))) |>
  pull(tz)

# --- DIARY (already local) ---
diary_df <- time_use |>
  filter(pid == person_id) |>
  transmute(
    activity,
    start = as.POSIXct(start_time),
    end = as.POSIXct(end_time)
  ) |>
  clip_to_window() |>
  mutate(
    activity = fct_relevel(
      activity,
      "Video Gaming",
      "Sleep",
      "Other Digital Media Use",
      "Personal Care",
      "Offline Leisure & Social Life",
    )
  )

# --- SESSIONS ---
sessions_df <- hourly_telemetry |>
  filter(pid == person_id) |>
  transmute(
    platform,
    start = as.POSIXct(hour_start_local),
    end = as.POSIXct(hour_start_local) +
      lubridate::dminutes(pmax(pmin(minutes, 60), 0))
  ) |>
  clip_to_window()

# --- SURVEY (previous day's submission inside the window; already local) ---
survey_df <- daily |>
  filter(pid == person_id) |>
  mutate(ts = as.POSIXct(date)) |>
  filter(
    as.Date(date) == (survey_day - 1),
    ts >= window_start,
    ts < window_end
  ) |>
  select(ts)

activity_colors <- c(
  "Education" = "#2a9df4", # Education blue
  "Exercise and Sports" = "#63b600", # Exercise and Sports green
  "Household and Family Care" = "#fac800", # Household and Family Care yellow
  "Offline Leisure & Social Life" = "#00897B", # Offline Leisure & Social Life teal
  "Other Digital Media Use" = "#9C27B0", # Other Digital Media Use purple
  "Personal Care" = "#666666", # Personal Care grey
  "Sleep" = "#111111", # Sleep black
  "Travel and Transportation" = "#ffb380", # Travel and Transportation orange
  "Video Gaming" = "#CA662B", # Video Gaming brown
  "Work and Employment" = "#28536b", # Work and Employment blue
  "Other" = "#d572d6" # Other pink
)

platform_colors <- c(
  "Nintendo" = "#E60012", # Nintendo red
  "Steam" = "#215e8a", # Steam dark blue
  "Xbox" = "#107C10", # Xbox green
  "iOS" = "#b43bf5", # iOS purple
  "Android" = "#f5f53b" # Android yellow
)

ggplot() +
  geom_rect(
    data = diary_df,
    aes(xmin = start, xmax = end, ymin = 2.6, ymax = 3.4, fill = activity),
    color = NA,
    alpha = 0.95
  ) +
  geom_segment(
    data = sessions_df,
    aes(x = start, xend = end, y = 2, yend = 2, color = platform),
    linewidth = 10.5,
    lineend = "butt",
  ) +
  geom_segment(
    data = survey_df,
    aes(x = ts, xend = ts, y = 0.6, yend = 3.4),
    linetype = "dashed",
    linewidth = 1.1,
    alpha = 0.55
  ) +
  geom_point(
    data = survey_df,
    aes(x = ts, y = 1),
    shape = 21,
    size = 4,
    stroke = 0.6,
    fill = "#FF6B35"
  ) +
  scale_x_datetime(
    limits = c(window_start, window_end),
    breaks = seq(window_start, window_end, by = "3 hours"),
    date_labels = "%H",
    expand = expansion(mult = c(0, 0))
  ) +
  scale_y_continuous(
    breaks = c(1, 2, 3),
    labels = c("Survey", "Digital Trace Data", "Diary"),
    limits = c(0.6, 3.4),
    expand = expansion(mult = 0)
  ) +
  scale_fill_manual(
    values = activity_colors,
    guide = guide_legend(title = "Diary activity")
  ) +
  scale_color_manual(values = platform_colors, name = "Platform") +
  guides(
    fill = guide_legend(
      title = "Diary activity",
      override.aes = list(alpha = 1),
      keywidth = grid::unit(6, "mm"),
      keyheight = grid::unit(4, "mm")
    ),
    color = guide_legend(
      title = "Platform",
      override.aes = list(linewidth = 3.5, linetype = 1), # thinner, less "cartoony"
      keywidth = grid::unit(6, "mm"),
      keyheight = grid::unit(4, "mm")
    )
  ) +
  labs(
    x = "Local time",
    y = NULL,
  ) +
  theme(
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    axis.title.y = element_blank(),
    legend.key.width = grid::unit(6, "mm"),
    legend.key.height = grid::unit(4, "mm"),
    legend.spacing.y = unit(-1, "mm"),
    legend.box.margin = margin(0, 0, 0, 0),
    legend.box = "vertical"
  )

```

:::

In each daily survey (US only), participants completed a light time use diary [@hakmanLOTUDLightOnline2025] asking participants to record their daily activities from 4:00am the previous day to 4:00am the morning of survey completion. Participants provided this information by placing activities based on their start and end time using a custom web app ([https://github.com/Thomhak/timediary-game](https://github.com/Thomhak/timediary-game)). Participants could select from 11 categories: Education, Exercise and Sports, Household and Family Care, Offline Leisure & Social Life, Other Digital Media Use, Personal Care, Sleep, Travel and Transportation, Video Gaming, Work and Employment, and Other. The median number of distinct activities entered per day was `r n_timeuse_entries_per_day`.

@fig-time-use presents an illustrative example of one participant's time use diary for a single day, alongside their gaming sessions recorded through telemetry and survey completion time.

## Data Quality

```{r}
#| label: explore-concurrency

crossplatform_concurrency <- hourly_telemetry |>
  group_by(pid, hour_start_utc) |>
  summarise(
    n_minutes = sum(minutes),
    n_platforms = n_distinct(platform),
    platforms = paste(sort(unique(platform)), collapse = ", "),
    .groups = "drop"
  ) |>
  filter(n_platforms >= 2 & n_minutes > 60)

prop_concurrent_sessions <- number(
  nrow(crossplatform_concurrency) /
    nrow(hourly_telemetry) *
    100,
  accuracy = 0.01
)

```

While not exhaustive, the cleaned data we share implements a variety of data quality checks. Specifically, in each daily and biweekly survey, one item from the BANGS (daily) and BPNSFS (biweekly) was duplicated to assess response consistency [@MeadeCraig2012identifying]; participants whose responses to the two identical items differed by more than one scale point were flagged for potential careless responding (daily surveys: `r prop_failed_daily_attn`% flagged, biweekly: `r prop_failed_biweekly_attn`%).

In the telemetry, we use several heuristics to identify potential unreliable sessions: (a) sessions beginning or ending in the future (indicative of clock manipulation or other errors), (b) sessions longer than 8 hours (indicative of substantial idle time or background usage), (c) 3 or more games played simultaneously (indicative of inactivity on multiple games or data corruption). In total, we removed `r prop_suspicious_xbox`% of Xbox sessions, and `r prop_suspicious_nintendo`% of Nintendo sessions based on these criteria. A further `r prop_concurrent_sessions`% of hourly telemetry records indicated concurrent gaming on multiple platforms within the same hour, suggesting potential background usage or idling on at least one platform; these records are retained but flagged for caution.

Steam data posed a unique data quality challenge due to concurrent gameplay: players sometimes ran multiple games at once (`r prop_overlapping_steam`% of sessions), causing hourly totals to exceed 60 minutes. Because Steam’s API reports cumulative playtime per game, our reconstructed session times could overlap across titles. To correct this, we applied proportional scaling at the hourly level—whenever total playtime exceeded 60 minutes, each game’s contribution was reduced proportionally to preserve the relative distribution while maintaining temporal plausibility.

Steam data presented a unique data quality challenge due to concurrent gameplay: players sometimes ran multiple games simultaneously, resulting in hourly intervals where total recorded playtime exceeded 60 minutes. This occurred because Steam's API reports cumulative playtime for each game independently, and our reconstruction of approximate session times from polling intervals could overlap when multiple games were active. In the raw data, `r prop_steam_hours_exceeding`% of player-hour combinations exceeded 60 minutes. To address this, we implemented proportional scaling at the hourly level. For any hour where the sum of playtime across all games exceeded 60 minutes, we scaled each game's contribution proportionally such that the total equaled 60 minutes, preserving the relative distribution of play across titles while ensuring temporal plausibility. Overall, `r prop_steam_segments_scaled`% of session segments required scaling, with a median reduction of `r median_minutes_reduced` minutes per scaled segment.

## Missingness

As with most longitudinal studies, attrition and missing data present important challenges for data quality and statistical inference. @tbl-missingness presents a comprehensive overview of missingness patterns across all data sources in our study, broken down by data type (Survey, Cognitive Task, and Telemetry).

The unit of observation differs across data types. For surveys and cognitive tasks, the unit is a completed survey or task. For telemetry, the unit varies by platform: Xbox and Nintendo use binary account linking, with data coverage considered maximal once linked (data provided directly by platform holders); Steam is measured hourly, with observations representing whether the participant's profile was publicly visible in each hour; iOS and Android are measured daily, with observations representing whether a valid screenshot was submitted covering that day's gaming.

```{r}
#| label: tbl-missingness
#| tbl-cap: "Missingness patterns across survey, telemetry, and cognitive task data by region. The table shows the number of participants (N), maximum possible observations, median number of missing observations per participant (with maximum in parentheses), and percentage of data completeness for each measure."

## ---- constants -------------------------------------------------------------
export_end <- as.POSIXct("2025-09-30 23:59:59", tz = "UTC")
H_STEAM <- 84L * 24L
D_MOBILE <- 84L
E_DAILY <- 30L
E_BIWEEKLY <- 6L
E_SIMON <- 3L

## ---- helpers ---------------------------------------------------------------
n_fmt <- \(x) format(x, big.mark = ",")

## ---- Steam visibility window (unchanged logic) -----------------------------
steam_windows <- steam_visibility |>
  group_by(pid) |>
  summarise(
    link_start = if (any(event == "accountLinked")) {
      min(timestamp[event == "accountLinked"])
    } else {
      as.POSIXct(NA)
    },
    link_end = if (any(event == "accountUnlinked")) {
      max(timestamp[event == "accountUnlinked"])
    } else {
      as.POSIXct(NA)
    },
    public_start = if (any(event == "steamDataPublic")) {
      min(timestamp[event == "steamDataPublic"])
    } else {
      as.POSIXct(NA)
    },
    private_end = if (any(event == "steamDataPrivate")) {
      max(timestamp[event == "steamDataPrivate"])
    } else {
      as.POSIXct(NA)
    },
    .groups = "drop"
  ) |>
  left_join(
    intake |>
      transmute(
        pid,
        enroll = coalesce(
          as.POSIXct(enrollment_datetime, tz = "UTC"),
          as.POSIXct(
            enrollment_datetime,
            format = "%Y-%m-%dT%H:%M:%SZ",
            tz = "UTC"
          )
        ),
        study_end = as.POSIXct(study_end_datetime, tz = "UTC")
      ),
    by = "pid"
  ) |>
  mutate(
    window_start = coalesce(link_start, public_start, enroll),
    hard_end = coalesce(link_end, private_end, study_end, export_end),
    assumed_end = pmin(hard_end, window_start + months(6), na.rm = TRUE),
    hours_link = if_else(
      !is.na(link_start) & !is.na(link_end) & link_end > link_start,
      as.numeric(difftime(link_end, link_start, "hours")),
      NA_real_
    ),
    hours_pp = if_else(
      !is.na(public_start) & !is.na(private_end) & private_end > public_start,
      as.numeric(difftime(private_end, public_start, "hours")),
      NA_real_
    ),
    hours_fallback = if_else(
      !is.na(window_start) & !is.na(assumed_end) & assumed_end > window_start,
      as.numeric(difftime(assumed_end, window_start, "hours")),
      NA_real_
    ),
    hours_visible = coalesce(hours_link, hours_pp, hours_fallback)
  ) |>
  select(pid, hours_visible)

## ---- cohorts (no regions) --------------------------------------------------
pids_daily <- daily |>
  count(pid, name = "n_daily") |>
  filter(n_daily >= 1) |>
  pull(pid)
pids_bw <- biweekly |>
  count(pid, name = "n_bw") |>
  filter(n_bw >= 1) |>
  pull(pid)
pids_timeuse <- time_use |>
  distinct(pid) |>
  pull(pid)

## telemetry sets
tele_sets <- list(
  Xbox = xbox |> distinct(pid),
  Nintendo = nintendo |> distinct(pid),
  iOS = ios |> distinct(pid),
  Android = android |> distinct(pid),
  Steam = steam |> distinct(pid)
)

## eligible for telemetry = (in any survey cohort) ∩ (present in that telemetry table)
elig_union <- tibble(pid = union(pids_daily, pids_bw))
eligible_tel <- imap(tele_sets, \(tel_df, nm) {
  elig_union |> inner_join(tel_df, by = "pid") |> mutate(measure = nm)
}) |>
  list_rbind()

## ---- build per-measure rows ------------------------------------------------
rows <- list(
  # ---- Surveys ----
  {
    # Daily diary
    obs <- daily |> filter(pid %in% pids_daily) |> count(pid, name = "obs")
    base <- tibble(pid = pids_daily) |>
      left_join(obs, by = "pid") |>
      mutate(obs = coalesce(obs, 0L), miss = E_DAILY - obs)

    tibble(
      data_type = "Survey",
      measure = "Daily surveys",
      n_total = daily |> distinct(pid) |> nrow(),
      n_participants = length(pids_daily),
      maximum_possible = length(pids_daily) * E_DAILY,
      total_observed = sum(base$obs),
      total_missing = sum(base$miss),
      median_missing_per_participant = suppressWarnings(median(
        base$miss,
        na.rm = TRUE
      )),
      max_missing_per_participant = suppressWarnings(max(
        base$miss,
        na.rm = TRUE
      ))
    )
  },
  {
    # Biweekly
    obs <- biweekly |> filter(pid %in% pids_bw) |> count(pid, name = "obs")
    base <- tibble(pid = pids_bw) |>
      left_join(obs, by = "pid") |>
      mutate(obs = coalesce(obs, 0L), miss = E_BIWEEKLY - obs)

    tibble(
      data_type = "Survey",
      measure = "Biweekly surveys",
      n_total = biweekly |> distinct(pid) |> nrow(),
      n_participants = length(pids_bw),
      maximum_possible = length(pids_bw) * E_BIWEEKLY,
      total_observed = sum(base$obs),
      total_missing = sum(base$miss),
      median_missing_per_participant = suppressWarnings(median(
        base$miss,
        na.rm = TRUE
      )),
      max_missing_per_participant = suppressWarnings(max(
        base$miss,
        na.rm = TRUE
      ))
    )
  },
  {
    # Time use diaries (among those who submitted at least one, max 30 days)
    time_use_by_day <- time_use |>
      distinct(pid, date) |>
      filter(pid %in% pids_timeuse)

    obs <- time_use_by_day |> count(pid, name = "obs")
    base <- tibble(pid = pids_timeuse) |>
      left_join(obs, by = "pid") |>
      mutate(obs = coalesce(obs, 0L), miss = E_DAILY - obs)

    tibble(
      data_type = "Task",
      measure = "Time use diaries",
      n_total = length(pids_timeuse),
      n_participants = length(pids_timeuse),
      maximum_possible = length(pids_timeuse) * E_DAILY,
      total_observed = sum(base$obs),
      total_missing = sum(base$miss),
      median_missing_per_participant = suppressWarnings(median(
        base$miss,
        na.rm = TRUE
      )),
      max_missing_per_participant = suppressWarnings(max(
        base$miss,
        na.rm = TRUE
      ))
    )
  },

  # ---- Telemetry: binary platforms ----
  {
    # Xbox (≥1 session)
    p_elig <- eligible_tel |> filter(measure == "Xbox") |> pull(pid)
    tibble(
      data_type = "Telemetry",
      measure = "Xbox",
      n_total = xbox |> distinct(pid) |> nrow(),
      n_participants = length(p_elig),
      maximum_possible = length(p_elig), # binary: ≥1 session
      total_observed = length(p_elig), # eligible implies ≥1 session present
      total_missing = 0L,
      median_missing_per_participant = NA_real_,
      max_missing_per_participant = NA_real_
    )
  },
  {
    # Nintendo (≥1 session)
    p_elig <- eligible_tel |> filter(measure == "Nintendo") |> pull(pid)
    tibble(
      data_type = "Telemetry",
      measure = "Nintendo",
      n_total = nintendo |> distinct(pid) |> nrow(),
      n_participants = length(p_elig),
      maximum_possible = length(p_elig),
      total_observed = length(p_elig),
      total_missing = 0L,
      median_missing_per_participant = NA_real_,
      max_missing_per_participant = NA_real_
    )
  },

  # ---- Telemetry: day-level platforms ----
  {
    # iOS (days out of 84)
    p_elig <- eligible_tel |> filter(measure == "iOS") |> pull(pid)
    obs <- ios |> filter(pid %in% p_elig) |> count(pid, name = "obs")
    base <- tibble(pid = p_elig) |>
      left_join(obs, by = "pid") |>
      mutate(obs = coalesce(obs, 0L), miss = pmax(0L, D_MOBILE - obs))

    tibble(
      data_type = "Telemetry",
      measure = "iOS",
      n_total = ios |> distinct(pid) |> nrow(),
      n_participants = length(p_elig),
      maximum_possible = length(p_elig) * D_MOBILE,
      total_observed = sum(base$obs),
      total_missing = sum(base$miss),
      median_missing_per_participant = suppressWarnings(median(
        base$miss,
        na.rm = TRUE
      )),
      max_missing_per_participant = suppressWarnings(max(
        base$miss,
        na.rm = TRUE
      ))
    )
  },
  {
    # Android (days out of 84)
    p_elig <- eligible_tel |> filter(measure == "Android") |> pull(pid)
    obs <- android |> filter(pid %in% p_elig) |> count(pid, name = "obs")
    base <- tibble(pid = p_elig) |>
      left_join(obs, by = "pid") |>
      mutate(obs = coalesce(obs, 0L), miss = pmax(0L, D_MOBILE - obs))

    tibble(
      data_type = "Telemetry",
      measure = "Android",
      n_total = android |> distinct(pid) |> nrow(),
      n_participants = length(p_elig),
      maximum_possible = length(p_elig) * D_MOBILE,
      total_observed = sum(base$obs),
      total_missing = sum(base$miss),
      median_missing_per_participant = suppressWarnings(median(
        base$miss,
        na.rm = TRUE
      )),
      max_missing_per_participant = suppressWarnings(max(
        base$miss,
        na.rm = TRUE
      ))
    )
  },

  # ---- Telemetry: hour-level (Steam) ----
  {
    p_elig <- eligible_tel |> filter(measure == "Steam") |> pull(pid)
    hrs <- steam_windows |>
      right_join(tibble(pid = p_elig), by = "pid") |>
      mutate(hours_visible = coalesce(hours_visible, 0)) |>
      pull(hours_visible)
    miss <- pmax(0, H_STEAM - hrs)

    tibble(
      data_type = "Telemetry",
      measure = "Steam",
      n_total = steam |> distinct(pid) |> nrow(),
      n_participants = length(p_elig),
      maximum_possible = length(p_elig) * H_STEAM,
      total_observed = as.integer(sum(H_STEAM - miss)), # total “visible hours”
      total_missing = as.integer(sum(miss)),
      median_missing_per_participant = suppressWarnings(median(
        miss,
        na.rm = TRUE
      )),
      max_missing_per_participant = suppressWarnings(max(miss, na.rm = TRUE))
    )
  },

  # ---- Task ----
  {
    # Simon (3 administrations among biweekly cohort)
    obs <- simon |> filter(pid %in% pids_bw) |> count(pid, name = "obs")
    base <- tibble(pid = pids_bw) |>
      left_join(obs, by = "pid") |>
      mutate(obs = coalesce(obs, 0L), miss = E_SIMON - obs)

    tibble(
      data_type = "Task",
      measure = "Attention tasks",
      n_total = simon |> distinct(pid) |> nrow(),
      n_participants = simon |> distinct(pid) |> nrow(),
      maximum_possible = length(pids_bw) * E_SIMON,
      total_observed = sum(base$obs),
      total_missing = sum(base$miss),
      median_missing_per_participant = suppressWarnings(median(
        base$miss,
        na.rm = TRUE
      )),
      max_missing_per_participant = suppressWarnings(max(
        base$miss,
        na.rm = TRUE
      ))
    )
  }
) |>
  list_rbind() |>
  arrange(
    factor(data_type, levels = c("Survey", "Task", "Telemetry")),
    factor(
      measure,
      levels = c(
        "Daily surveys",
        "Biweekly surveys",
        "Time use diaries",
        "Attention tasks",
        "Xbox",
        "Nintendo",
        "Steam",
        "iOS",
        "Android"
      )
    )
  )

## ---- present table (hide median; band by Data Type) ------------------------
table_data <- rows |>
  mutate(
    pct_missing_num = if_else(
      maximum_possible > 0,
      100 * (total_missing / maximum_possible),
      0
    ),
    `N subj.` = n_fmt(n_total),
    `N survey\nsubj.` = n_fmt(n_participants),
    `Max. possible obs.` = n_fmt(maximum_possible),
    `N obs.` = n_fmt(total_observed),
    `Median (Max) Missing` = if_else(
      is.na(median_missing_per_participant),
      "—",
      sprintf(
        "%.0f (%.0f)",
        floor(median_missing_per_participant),
        floor(max_missing_per_participant)
      )
    ),
    `% Missing` = sprintf("%.1f%%", pct_missing_num)
  ) |>
  group_by(data_type) |>
  mutate(show_type = row_number() == 1L) |>
  ungroup() |>
  select(
    `Data Type` = data_type,
    Measure = measure,
    `N subj.`,
    `N survey\nsubj.`,
    `N obs.`,
    `Max. possible obs.`,
    # `Median (Max) Missing`,  # compute but hide
    `% Missing`,
    show_type
  ) |>
  mutate(`Data Type` = if_else(show_type, `Data Type`, "")) |>
  select(-show_type)

## for band headers
data_type_starts <- table_data |>
  mutate(row_num = row_number()) |>
  filter(`Data Type` != "") |>
  pull(row_num)

max_possible_text <- "Max. possible obs.: Daily Diary = N × 30; Biweekly = N × 6: Xbox/Nintendo = N participants with ≥1 session; Steam = N × 84 days × 24 hours (visibility hours); iOS/Android = N × 84 days; Simon Task = N × 3 administrations."

n_total_text <- "N subjects can exceed N survey subjects because some participants (1) provided digital trace data that was not recent enough to qualify, or (2) provided recent digital trace data but did not return for subsequent surveys."

tt(
  table_data,
  notes = list(
    "a" = list(i = 0, j = 3, text = n_total_text),
    "b" = list(i = 0, j = 6, text = max_possible_text)
  ),
  width = 1,
  escape = TRUE
) |>
  style_tt(i = 0, bold = TRUE, align = "c") |>
  style_tt(
    i = data_type_starts,
    j = 1,
    bold = TRUE,
    line = "t",
    line_width = 0.12
  ) |>
  style_tt(fontsize = 0.7)
```

# Discussion

Spanning `r n_hours` hours, `r n_users` user-platform combinations, and `r n_games` games, the dataset presented here represents a major advance in the comprehensiveness and scale at which video game play has been captured. Previous studies on games and health have typically used (1) self-report data with substantial limitations on accuracy, time range, and detail; (2) large-scale data that is either anonymized and not linkable to individuals or survey responses [@ZendleEtAl2023Crosscultural; @VuorreEtAl2021largescale] and/or focused on commercially-relevant game analytics topics [@RattingerEtAl2016Integrating; @CanossaEtAl2019influencers], and thus lacking good measures of ground truth for questions about player health; or (3) small-scale trace data from single platforms or games, providing only a small slice of overall gaming behaviour and thus limiting generalizability [@VuorreEtAl2022Time; @JohannesEtAl2021Video; @LarrieuEtAl2023How; @PerryEtAl2018onlineonly; @BallouEtAl2024Registered; @BallouEtAl2025Perceived].

## Digital Trace Data Infrastructure

This dataset delivers on both calls for greater collaboration between the tech industry and academics [@KingPersily2020New; @LazerEtAl2020Computational], as well as simultaneous calls for better trace data access without direct industry involvement [@Xiao2023Debate; @ValkenburgEtAl2024It; @BoeschotenEtAl2023Port]. The data-sharing agreements with Nintendo of America, Nintendo of Europe, and Microsoft are the first of their kind, enabling researchers to access telemetry data from major gaming platforms while ensuring user privacy and data security. By making this dataset openly available under a modified CC0 license, we aim to catalyze further research in this area, allowing other researchers to explore the rich data we have collected.

At the same time, the Steam and smartphone data collection show the value of open trace data to study platforms that are less willing or able to participate actively. Several previous studies have collected smartphone data using screenshots, with some success in automating the extraction of these data [@ParrySewall2021How; @FeizEtAl2022Understanding; @LiuEtAl2024UserDonated]. Open trace data infrastructure remains an arms race, often relying on industry-provided APIs that can change or be deprecated at any time [@DavidsonEtAl2023Platformcontrolled; @BallouEtAl2023Four]. Nonetheless, we believe it is vital for researchers to advance both pathways for the best chance to understand the effects of technology. 

## Empirical Opportunities

We believe this dataset has potential to address a wide variety of pertinent research questions for the field. Some of these questions will be addressed in forthcoming registered reports [@BallouEtAl2025Psychological]: specifically, we have plans to test (1) key hypotheses from the Basic Needs in Games model [@BallouDeterding2024Basic] about how gaming relates to basic psychological needs over time, (2) the relationship between late-night gaming and sleep, and (3) the relationship between playtime in different genres and wellbeing.

The data's richness means that researchers can explore numerous other questions (or, indeed, conduct and compare alternative analysis approaches to the above questions). We particularly encourage work that tests explicit causal models---when assumptions about data generating processes are transparent, observational data can provide meaningful falsifying tests of causal effects. A recent paper presents 13 such potential causal effects of games on wellbeing [@BallouEtAl2025How], and several of these could tested using the data here. For example, the relationship between *action game mechanics, executive function, and stress* [@HilgardEtAl2019overestimation; @BediouEtAl2018metaanalysis] could be investigated by combining the game genre data, stress data from daily surveys, and digital trace data with genre coding. The relationship between *social experiences in games and wellbeing* [@MandrykEtAl2020how] could be investigating by coding games for single- or multiplayer features, using the self-reported social context of play, and subsequent relatedness satisfaction scores.  

The dataset need not be limited to questions of wellbeing. Other questions of potential interest include:

*How do neurotypical and neurodiverse players differ in their gaming behavior?* Using the neurodivergence data we collected (which includes, for example, `r table(intake$neuro_iden_asd)[1]` participants who identify as having autism and `r table(intake$neuro_iden_adhd)[1]` who identify as having ADHD), researchers can explore differences in the types of games, gaming experiences, and trajectory of play over time between neurotypical and neurodiverse players. Neurodiversity in games has regularly been studied in the context of specific games and with qualitative methods [@KilmerEtAl2023Therapeuticallya; @ZolyomiSchmalz2017Mining], with some researchers noting that research focuses on therapeutic interventions delivered through games to "cure" neurodivergent players of undesirable traits [@SpielGerling2021Purpose]. Research on neurodivergent players' naturalistic gaming behavior across platforms has to date received less attention.

*How do seasons and weather impact play?* Because we capture time-stamped play sessions alongside participants’ geographic locations, researchers can merge in high‐resolution weather and daylight data to examine how environmental factors influence gaming behavior. Causal inference techniques such as inverse probability weighting can enable estimates of how, when, and how much people play in response to seasonal and meteorological changes. Quantifying these effects can better distinguish environmental factors driving video game play from other drivers (like work schedules or weekend routines) and improve our ability to predict regional changes to gaming behavior on a day-to-day basis [@Palomba2019Digital], or the behavioral impacts of climate change in the long term.

*Does irregular, extended, or overnight play predict subsequent rises in problematic play, and can telemetry provide early warning signals?* Using the data we collected about displacement---the extent to which gaming supports or interferes with other areas of life---as well as about gaming disorder (at Waves 1 and 6), telemetry can provide insights into patterns of gaming that coincide or lead to subsequent changes in the degree of problematic play. Long-term, more accurately identifying the longitudinal behavioral correlates of maladaptive gaming---beyond simple heuristics such as the average amount of weekly play in the last 6 months---can help us create personalized models and early warning systems for players and parents.

We encourage researchers from a wide range of disciplines to explore these or other questions using the data we present.

## Limitations

While this dataset represents a substantial step forward in holistic coverage of video game play, it remains imperfect: we did not capture data on PlayStation (\~19% of gaming market) or computer games played outside the Steam platform (\~11% of gaming market); on Nintendo, we do not have access to third-party titles (`r third_party_prop` of Nintendo play, based on Nintendo-provided metrics for each player in our sample); and our coverage of smartphone play is limited by the onerous workflow limiting the response rate, and the difficulties of reliable OCR extraction.

We further are unable to identify idle time (when players have a game open but are not actively playing it) and account sharing (when players let friends or family use their account); some playtime values may therefore be overestimates of the person's true playtime, though we are unable to say by how much.

Finally, while our sample is large, diverse, and reasonably well matched with national averages on certain demographic characteristics (e.g., ethnicity and education), is it unlikely to be representative either of the general population or the population of adults who play games. Selection effects are inherent to our study: not all players will be willing to share their gaming history and identifiable information with researchers, or to participate in an intensive battery of surveys. However, we are largely unable to quantify representativeness, as the (adult) gaming population is poorly understood in its own right. Although we have broad estimates of engagement among key demographic segments showing, for example, that men are more likely to play games [@Ofcom2023Online; @EntertainmentSoftwareAssociation20242024], digital trace data describing gaming in terms of amount, seasonality, specific titles and genres, or other factors remains lacking. In other words, we cannot say how representative our sample is of players in the US or UK, because we do not yet know what representative gaming behavior looks like [see e.g., @RehbeinEtAl2016Video]---or whether representativeness is even a meaningful aim [@Kaye2019Gaming].

## Future Work

The trace data presented here is broad in scope but limited in granularity: we capture all gaming activity on a given platform, but not what happens within individual games. Prior work and theory make clear that in-game behaviors (e.g., what role a player adopts, whether they compete or cooperate, or how they perform in competitive modes) are critical determinants of player experience and thereby wellbeing (see e.g., @ElsonEtAl2014More for a review of how in-game contexts shape effects). This highlights a fundamental trade-off in digital trace research between breadth—how comprehensively play can be captured across platforms—and depth—the granularity of in-game behaviors and experiences. At present, our dataset emphasizes breadth, but we see strong potential in future study designs that combine platform-level telemetry with targeted in-game behavioral data to provide a more complete picture.

We also see strong potential in combining digital trace data with experimental designs that enable stronger causal inference—for example, randomizing players to single-player games only, or restricting play to certain times of day, to examine effects on social wellbeing or sleep. Previous researchers have noted a dearth of digital trace data-backed field experiments, while highlighting their potential [@StierEtAl2020Integrating]: Trace data not only captures naturalistic gaming behavior but also allows researchers to assess *substitution* (what games or platforms participants switch to under intervention) and *adherence* (how closely they follow assigned play patterns).

## Data Availability

All data, materials, and code related to the dataset and this manuscript are available under {{< meta data-license >}} at <https://doi.org/10.5281/zenodo.17536656>.

# References

:::: {#refs}
::::

```{=typst}
#show: appendix.with()
```

# Appendix

## Deviations from Preregistration

We made several deviations from our preregistration to ensure we could recruit enough high-quality participants to meet our sample size goals. In our view, none are so severe enough to threaten the validity of the study. Deviations are summarised in @tbl-deviations.

```{r}
#| label: tbl-deviations
#| tbl-cap: "Summary of deviations from preregistration"

tibble(
  Preregistered = c(
    "All participants sourced from PureProfile",
    "Screening sample would be nationally representative by ethnicity and gender",
    "Sample consists of participants aged 18--30 in the US and 18--75 in the UK",
    "To qualify, >=75% of a participant's total gaming must take place on platforms included in the study (Xbox, Steam, Nintendo Switch)",
    "Qualification contingent upon valid telemetry within last 7 days",
    "Daily and biweekly surveys sent at 7pm local time",
    "Session-level Android data captured via the ActivityWatch app"
  ),
  Actual = c(
    "Participants sourced from both PureProfile and Prolific",
    "Approximately 50% of screening was done using quotas for national representativeness by ethnicity and gender; all subsequent sampling used convenience sampling with no quotas",
    "Sample consists of participants aged 18-40 in both regions",
    "To qualify, >=50% of a participant's total gaming must take place on platforms included in the study (Xbox, Steam, Nintendo Switch)",
    "Qualification contingent upon valid telemetry within last 14 days",
    "Daily and biweekly surveys sent at 2pm local time",
    "Daily-level Android data captured using screenshots of the Digital Wellbeing interface"
  ),
  `Justification for Deviation` = c(
    "Exhausted PureProfile participant pool before reaching required sample size",
    "Exhausted participant pools of smaller demographic categories on both Prolific and PureProfile before reaching required sample size",
    "(1) Unable to recruit enough participants in the US aged 18--30; (2) near-zero qualification rates from UK adults over 50; (3) desire for results from both regions to be more easily comparable",
    "Low rates of study qualification at 75% threshold, in large part due to substantial uncaptured Playstation play",
    "Feedback from participants indicating that play during a 7-day period was subject to too many fluctuations (e.g., a busy workweek)",
    "Feedback from participants indicating that evening plans often interfered with survey completion and thus adversely affected response rate",
    "Restrictions in PureProfile's privacy policy preventing installation of 3rd party apps; technical challenges in supporting users with the installation and data export"
  )
) |>
  tt() |>
  style_tt(fontsize = 0.8) |>
  style_tt(i = 0, bold = TRUE, align = "c")
```


```{r}
#| label: tbl-measures
#| tbl-cap: "Summary of survey measures used in the study"

t_trait <- tibble(
  Construct = c(
    "Chronotype",
    "Big 5 Personality",
    "Player Trait Typology",
    "Gaming Disorder Symptoms"
  ),
  Measure = c(
    "Munich Chronotype Questionnaire (Roenneberg et al., 2003)",
    "BFI-2-XS (Soto & John, 2017)",
    "Trojan Player Typology (Kahn et al., 2015)",
    "Gaming Disorder Test (Pontes et al., 2019)"
  ),
  `Example Item` = c(
    "I go to bed at...",
    "I am someone who...is compassionate, has a soft heart.",
    "It's important to me to play with a tightly knit group.",
    "In the past 3 months...I have had difficulties controlling my gaming activity."
  ),
  `Response Format` = c(
    # note original casing retained, will rename for merge only
    "Times and numbers of minutes",
    "5-pt Likert scale from 1 (Disagree strongly) to 5 (Agree strongly)",
    "5-pt Likert scale from 1 (Strongly disagree) to 5 (Strongly agree)",
    "5-pt Likert scale from 1 (Never) to 5 (Very often)"
  )
) |>
  mutate(
    Frequency = c(
      "Once (baseline)",
      "Once (baseline)",
      "Once (baseline)",
      "Twice (biweekly waves 1 & 6)"
    ),
    Group = "Trait / traitlike"
  )

t_daily <- tibble(
  Construct = c(
    "Basic psychological need satisfaction and frustration - life in general",
    "Basic psychological need satisfaction and frustration - video games",
    "Life satisfaction",
    "Affective valence",
    "Sleep Quality",
    "Stressors",
    "Self-reported displacement"
  ),
  Measure = c(
    "Basic Psychological Need Satisfaction and Frustration Scale (Chen et al., 2015), brief version (Martela & Ryan, 2024)",
    "Basic Needs in Games scale (Ballou, Denisova, et al., 2024), brief session-level version",
    "Cantril Self-anchoring Scale (Cantril, 1965), daily version",
    "ad hoc",
    "Sleep quality item (Item 9) from Consensus Sleep Diary (Carney et al., 2012)",
    "Daily Inventory of Stressful Events (Almeida et al., 2002), modified for digital delivery",
    "ad hoc"
  ),
  `Example Item` = c(
    "In the last 24 hours...I was able to do things I really want and value in life.",
    "In my most recent session of X...I felt disappointed with my performance.",
    "I was satisfied with my life today.",
    "How are you feeling right now?",
    "How do you rate the quality of your sleep?",
    "[In the last 24 hours], what kinds of stressful event(s) occurred? [Participant selects among 7 options, including e.g. argument or disagreement]",
    "Think back to your most recent gaming session. If you hadn't played a game, what would you most likely have done instead?"
  ),
  `Response format` = c(
    "7-pt Likert scale from 1 (very strongly disagree) to 7 (very strongly agree)",
    "7-pt Likert scale from 1 (very strongly disagree) to 7 (very strongly agree)",
    "Visual Analogue Scale from 1 (Strongly disagree) to 100 (Strongly agree)",
    "Visual Analogue Scale from 1 (very bad) to 100 (very good)",
    "5-pt Likert scale from 1 (very poor) to 5 (very good)",
    "Yes/No, followed by a 4-pt Likert scale from 1 (Not at all stressful) to 4 (Very stressful)",
    "Open response"
  )
) |>
  mutate(
    Frequency = "Daily",
    Group = "Daily"
  )

t_biweekly <- tibble(
  Construct = c(
    "General Mental Wellbeing",
    "Depression symptoms",
    "Life satisfaction",
    "Basic psychological need satisfaction and frustration - video games",
    "Subjective displacement"
  ),
  Measure = c(
    "Warwick-Edinburgh Mental Wellbeing Scale (Tennant et al., 2007)",
    "PROMIS Short Form 8a Adult Depression Scale (Pilkonis et al., 2011)",
    "Cantril Self-anchoring Scale (Cantril, 1965)",
    "Basic Needs in Games scale (Ballou, Denisova, et al., 2024), gaming in general version",
    "ad hoc"
  ),
  `Example Item` = c(
    "I've been feeling optimistic about the future",
    "In the past 7 days...I felt that I had nothing to look forward to.",
    "On which step of [a ladder from 0 to 10 representing the best possible life] would you say you personally feel you stood over the past two weeks?",
    "When playing video games during the last 2 weeks...I could play in the way I wanted.",
    "Over the last two weeks, to what extent has the time you spend playing video games influenced the following areas of your life? [...] Work/school performance"
  ),
  `Response format` = c(
    "5-pt Likert scale from 1 (none of the time) to 5 (all of the time)",
    "5-pt Likert scale from 1 (Never) to 5 (Always)",
    "10-pt unlabeled scale from 0 to 10",
    "7-pt Likert scale from 1 (very strongly disagree) to 7 (very strongly agree)",
    "7-pt Likert scale from 1 (greatly interfered) to 7 (greatly supported)"
  )
) |>
  mutate(
    Frequency = "Biweekly (every 2 weeks)",
    Group = "Biweekly"
  )

t_monthly <- tibble(
  Construct = c(
    "Sleep quality",
    "Daytime sleepiness",
    "Harms and benefits of gaming"
  ),
  Measure = c(
    "Pittsburgh Sleep Quality Index (Buysse et al., 1989)",
    "Epworth Sleepiness Scale (Johns, 1991)",
    "2 free text questions"
  ),
  `Example Item` = c(
    "During the past month, what time have you usually gotten up in the morning?",
    "How likely are you to doze off or fall asleep in the following situations, in comparison to feeling just tired? [...] Watching TV",
    "Do you feel that gaming is sometimes a problem for you? Please describe."
  ),
  `Response format` = c(
    "Various",
    "4-pt Likert scale from 1 (No chance of dozing) to 4 (High chance of dozing)",
    "Open text"
  )
) |>
  mutate(
    Frequency = "Monthly (alternating biweekly surveys)",
    Group = "Monthly"
  )

t_selfreport <- tibble(
  Construct = c(
    "Social context of play",
    "Self-reported Playtime",
    "Self-reported recent sessions"
  ),
  Measure = c(
    "Types of social play engaged during the last 24 hours (single-player games only, multiplayer with real-world friends, multiplayer with online-only friends, multiplayer with strangers). Participants could select more than one option.",
    "Time spent playing games on platforms they had linked during the study (e.g., excluding other platforms such as Playstation) in each of the following periods: last 24 hours, last 7 days, and last 14 days.",
    "Details of at least 1 and up to 3 of their most recent gaming sessions (game, date, and start/end time)."
  ),
  `Example Item` = c(NA_character_, NA_character_, NA_character_),
  `Response format` = c(
    "Multiple selection from listed options",
    "Time estimates for last 24 hours, last 7 days, last 14 days",
    "Game title, date, and start/end time for 1-3 sessions"
  ),
  Frequency = c(
    "Daily", # last 24h context
    "Biweekly (every 2 weeks)",
    "Biweekly (every 2 weeks)"
  )
)

appendix_tbl <- bind_rows(
  # rename the single differing column header for merge only
  rename(t_trait, `Response format` = `Response Format`),
  t_daily,
  t_biweekly,
  t_monthly,
  t_selfreport
) |>
  arrange(Group, Construct) |>
  select(Construct, Measure, `Example Item`, `Response format`, Frequency) |>
  mutate(
    Frequency = factor(
      Frequency,
      levels = c(
        "Once (baseline)",
        "Twice (biweekly waves 1 & 6)",
        "Monthly (alternating biweekly surveys)",
        "Biweekly (every 2 weeks)",
        "Daily"
      ),
      ordered = TRUE
    )
  ) |>
  arrange(Frequency)

# Footnote index for Gaming Disorder Symptoms row
idx_gd <- which(appendix_tbl$Construct == "Gaming Disorder Symptoms")

# ----- Render tinytable (small font, escaped safely) -----
appendix_tbl |>
  tt(
    caption = "All self-report measures and their assessment frequency",
    notes = list(
      "a" = list(
        i = idx_gd,
        j = 1,
        text = "Measured twice, at biweekly waves 1 and 6."
      )
    )
  ) |>
  # keep markdown/quarto processing for non-Typst runs too
  format_tt(escape = FALSE, markdown = TRUE, quarto = TRUE) |>
  style_tt(fontsize = 0.5) |>
  style_tt(i = 0, bold = TRUE, align = "c")

```

## Codebook

::: {.content-visible when-format="html"}

```{r}
#| label: codebook-interactive
#| echo: false
#| warning: false
#| eval: !expr knitr::is_html_output()

# Read all sheets from the codebook
codebook_path <- "codebook.xlsx"
sheet_names <- excel_sheets(codebook_path)

# Read all sheets and combine with sheet identifier
codebook_all <- map_dfr(sheet_names, function(sheet) {
  if (sheet == "Meta-info") {
    return(NULL) # Skip meta-info sheet
  }
  read_excel(codebook_path, sheet = sheet) |>
    mutate(Sheet = sheet, .before = 1)
})

# Create interactive datatable
datatable(
  codebook_all,
  filter = 'top',
  options = list(
    pageLength = 25,
    scrollX = TRUE,
    autoWidth = TRUE,
    searchHighlight = TRUE,
    dom = 'Bfrtip',
    buttons = c('copy', 'csv', 'excel')
  ),
  extensions = 'Buttons',
  rownames = FALSE,
  caption = "Interactive Codebook - All Variables Across All Data Sources",
  class = 'compact'
) |>
  formatStyle(columns = 1:ncol(codebook_all), fontSize = '12px')
```

:::

::: {.content-hidden when-format="html"}

*Interactive codebook available in the HTML version of this document: [https://digital-wellbeing.github.io/open-play/#codebook](https://digital-wellbeing.github.io/open-play/#codebook).*

:::
