---
title: Clean raw data and save outputs
---

```{r}
#| label: load-libraries

library(data.table)
library(dtplyr)
library(fs)
library(tidyverse)

# Load custom functions
source("R/helpers.R")
```

```{r}
#| label: load-data-raw

intake <- read_csv("data/raw/survey_intake_raw.csv.gz", guess_max = 34000)
daily <- read_csv("data/raw/survey_daily_raw.csv.gz")
biweekly <- read_csv("data/raw/survey_biweekly_raw.csv.gz", guess_max = 5000)

nintendo <- read_csv("data/raw/telemetry_nintendo_raw.csv.gz")
steam <- read_csv("data/raw/telemetry_steam_raw.csv.gz")
xbox <- read_csv("data/raw/telemetry_xbox_raw.csv.gz")
ios <- read_csv("data/raw/telemetry_ios_raw.csv.gz")
android <- read_csv("data/raw/telemetry_android_raw.csv.gz")
timeuse <- read_csv("data/raw/timeuse_raw.csv.gz")

dat_cog <- read_csv("data/raw/cognitive_tasks_raw.csv.gz")
```

```{r}
#| label: preprocess-steam

# non- and idle games lists are in helpers.R

# Arrange data to calculate differences correctly
steam_clean <- steam |>
  # get rid of idle games and non-games, see list in R/helpers.R
  filter(!title_id %in% idle_games & !title_id %in% non_games) |>
  mutate(
    steam_id = as.character(steam_id),
    pid = as.character(pid)
  ) |>
  arrange(pid, title_id, timestamp) |>
  group_by(pid, steam_id) |>
  mutate(
    # Calculate time difference between consecutive polls for the same user/game
    time_diff_hours = as.numeric(difftime(
      timestamp,
      lag(timestamp),
      units = "hours"
    )),
    # Calculate playtime difference
    playtime_diff = playtime_forever - lag(playtime_forever)
  ) |>
  ungroup() |>
  # Filter out invalid calculations:
  # - First entry for each user/game (lag is NA)
  # - Negative playtime difference (data anomaly or reset?)
  # - Large time gaps between polls (e.g., > 2 hours for hourly polling, indicates missed polls or stopped tracking)
  filter(
    !is.na(playtime_diff),
    playtime_diff >= 0,
    !is.na(time_diff_hours),
    time_diff_hours > 0,
    time_diff_hours < 2 # Assuming roughly hourly polls
  ) |>
  # If playtime_diff is large, it might span multiple hours.
  # This calculation assumes playtime is evenly distributed over the polling interval.
  # A more complex approach might be needed for precise hourly allocation if intervals vary widely.
  # For simplicity here, we assign the entire playtime_diff to the hour the poll *ended*.
  mutate(
    minutes = playtime_diff,
    date = as.Date(timestamp),
    hour = hour(timestamp),
    datetime_hour_start = ymd_hms(
      paste0(date, " ", sprintf("%02d", hour), ":00:00"),
      tz = "UTC"
    )
  ) |>
  # filter out rows where 3+ games are being played simultaneously
  group_by(pid, datetime_hour_start) |>
  mutate(concurrent_titles = n_distinct(steam_id[minutes > 0])) |>
  ungroup() |>
  mutate(minutes = ifelse(concurrent_titles >= 3, 0, minutes)) |>
  group_by(pid, steam_id) |>
  # Calculate difference from previous hour and identify new sessions
  mutate(
    hour_diff = as.numeric(difftime(
      datetime_hour_start,
      lag(datetime_hour_start),
      units = "hours"
    )),
    is_new_session = ifelse(is.na(hour_diff) | hour_diff > 1, 1, 0),
    session_group_id = cumsum(is_new_session),
    session_start = datetime_hour_start,
    session_end = datetime_hour_start + hours(1), # Session ends at the end of the last hour block played
    hour_start = floor_date(timestamp, "hour")
  ) |>
  filter(minutes > 0 & minutes < 120) |>
  ungroup()

```

```{r}
#| label: preprocess-xbox

# Clean Xbox data and keep only active gaming sessions
# In particular, Xbox has a handful of overlapping sessions, potentially a result of 
# multi-device use or 
# that we need to resolve:
# - at most one title at any instant
# - handoff tolerance 'tol_sec' (short overlaps => later-starting title wins)
# - â‰¥3 concurrent titles => drop that slice

xbox_clean <- xbox |>
  arrange(pid, session_start, session_end) |> 
  merge_adjacent_sessions(tol_sec = dseconds(60)) |>
  
  # filter non-foreground games
  filter(
    duration >= 1,
    title_placement == "Full", # in foreground
    !genres %in% c("Shopping", "Video", "Entertainment", "Utilities & tools"),
    tolower(title_id) != "e1924c10-9c91-4652-81b9-8ca0670e77a7" # home screen
  ) |>
  
  # filter suspicious sessions
  filter(
    duration <= 480,
    !is.na(session_start),
    !is.na(session_end),
    session_end > session_start,
  ) |> 
  resolve_overlaps() |> 
  arrange(pid, session_start, session_end)

```

```{r}
#| label: preprocess-nintendo

tol <- dseconds(60)

nintendo_clean <- nintendo |>
  
  # filter duplicate rows and merge adjacent sessions
  distinct() |> 
  filter(duration >= 1, pid %in% intake$pid) |> 
  merge_adjacent_sessions(tol_sec = tol) |>
  
  # filter suspicious sessions
  filter(
    duration <= 480,
    !is.na(session_start),
    !is.na(session_end),
    session_end > session_start,
    session_start < time_of_pull
  ) |>
  
  # clean up
  arrange(pid, title_id, session_start, session_end) |>
  select(pid, title_id, session_start, session_end, duration, device_type, operation_mode) |>
  as_tibble()

```

```{r}
#| label: preprocess-simon

# Only include non-pilot-study participants who qualified at intake
dat_cog <- dat_cog |>
  filter(
    pid %in%
      unique(intake$pid[intake$qualified & intake$cohort != "Platform Pilot"])
  )

# Take relevant variables and observations only
# `score_final` is automatically added to all rows so distinct works here
dat_cog <- dat_cog |>
  filter(practice == 0, task == "simon") |>
  add_count(pid, wave, name = "n_trials") |>
  distinct(
    pid,
    wave,
    datetime,
    device_type,
    n_trials,
    score_final,
    meanrt_final
  )

dat_cog <- dat_cog |>
  mutate(wave = factor(wave)) |>
  arrange(wave, pid)
```

```{r}
#| label: preprocess-android

android <- android |> 
  mutate(platform = "Android") |> 
  select(pid, platform, day_local = date, minutes = total_gaming_minutes)

```

```{r}
#| label: preprocess-ios

ios <- ios |> 
  mutate(platform = "iOS") |> 
  select(pid, platform, day_local = date, minutes = total_gaming_minutes)

```

As preregistered, we exclude days where total gaming time (across all platforms) exceeds 16 hours, as well as sessions that are more than 8 hours long or whose end time precedes their start time. The exclusions are applied to all relevant datasets.

```{r}
#| label: apply-exclusions

# --- Build tz map (as in your script) ---------------------------------------
tz_map <- intake |>
  mutate(
    pid = as.character(pid),
    off = offset_secs(local_timezone),
    .keep = "none"
  ) |>
  distinct(pid, .keep_all = TRUE)

# --- Hourly from sessions (Nintendo + Xbox) ---------------------------------
sessions_telemetry <- bind_rows(
  xbox_clean |> mutate(platform = "Xbox"),
  nintendo_clean |> mutate(platform = "Nintendo")
) |>
  mutate(pid = as.character(pid)) |>
  left_join(tz_map, by = "pid") |>
  mutate(
    start_local = session_start + off,
    end_local   = session_end   + off,
    duration_min = as.numeric(difftime(session_end, session_start, units = "mins"))
  ) |>
  filter(
    !is.na(session_start), !is.na(session_end),
    session_end > session_start,
    duration_min >= 1
  )

hourly_from_sessions <- sessions_telemetry |>
  filter(!is.na(off)) |>
  mutate(
    h0_local = floor_date(start_local, "hour"),
    h1_local = floor_date(end_local - seconds(1), "hour"),
    n_hours = as.integer(difftime(h1_local, h0_local, units = "hours")) + 1
  ) |>
  tidyr::uncount(n_hours, .remove = FALSE, .id = "k") |>
  mutate(
    hour_start_local = h0_local + hours(k - 1),
    minutes = pmax(
      0,
      as.numeric(difftime(
        pmin(end_local, hour_start_local + hours(1)),
        pmax(start_local, hour_start_local),
        units = "mins"
      ))
    ),
    hour_start_utc = hour_start_local - off
  ) |>
  select(pid, platform, hour_start_local, hour_start_utc, minutes)

# --- Hourly from Steam (already hourly) -------------------------------------
hourly_from_steam <- steam_clean |>
  select(pid, datetime_hour_start, minutes) |>
  mutate(pid = as.character(pid)) |>
  left_join(tz_map, by = "pid") |>
  mutate(
    platform = "Steam",
    hour_start_utc   = datetime_hour_start,
    hour_start_local = datetime_hour_start + off
  ) |>
  select(pid, platform, hour_start_local, hour_start_utc, minutes)

hourly_telemetry <- bind_rows(hourly_from_sessions, hourly_from_steam)

# --- Daily totals (local day) incl. iOS/Android ------------------------------
daily_telemetry <- hourly_telemetry |>
  mutate(day_local = as.Date(hour_start_local)) |>
  group_by(pid, platform, day_local) |>
  summarise(minutes = sum(minutes, na.rm = TRUE), .groups = "drop")

daily_ios_android <- bind_rows(
  ios     |> transmute(pid = as.character(pid), platform = "iOS",     day_local = as.Date(day_local), minutes),
  android |> transmute(pid = as.character(pid), platform = "Android", day_local = as.Date(day_local), minutes)
)

daily_all <- bind_rows(daily_telemetry, daily_ios_android) |>
  group_by(pid, day_local) |>
  summarise(total_minutes = sum(minutes, na.rm = TRUE), .groups = "drop")

exclusion_days <- daily_all |>
  filter(total_minutes > 16 * 60) |>
  select(pid, day_local)

# --- Apply exclusions back to each dataset ----------------------------------
steam_clean <- steam_clean |>
  left_join(tz_map, by = c("pid" = "pid")) |>
  mutate(day_local = as.Date(datetime_hour_start + off)) |>
  anti_join(exclusion_days, by = c("pid", "day_local")) |>
  select(-off, -day_local)

xbox_clean <- xbox_clean |>
  left_join(tz_map, by = "pid") |>
  mutate(day_local = as.Date(session_start + off)) |>
  anti_join(exclusion_days, by = c("pid", "day_local")) |>
  select(-off, -day_local)

nintendo_clean <- nintendo_clean |>
  left_join(tz_map, by = "pid") |>
  mutate(day_local = as.Date(session_start + off)) |>
  anti_join(exclusion_days, by = c("pid", "day_local")) |>
  select(-off, -day_local)

ios <- ios |>
  mutate(pid = as.character(pid), day_local = as.Date(day_local)) |>
  anti_join(exclusion_days, by = c("pid", "day_local"))

android <- android |>
  mutate(pid = as.character(pid), day_local = as.Date(day_local)) |>
  anti_join(exclusion_days, by = c("pid", "day_local"))

```


```{r}
#| label: preprocess-surveys

daily <- daily |>
  # Pureprofile accidentally let through some unscreened and/or UK participants by mistake; exclude them here
  filter(
    pid %in% intake$pid,
    !(pid %in% intake$pid[intake$country == "UK"])
  ) |> 
  filter(wave <= 30)

intake <- intake |> 
  
  # self-reported playtime over 50 hours per week is treated as NA 
  mutate(self_reported_weekly_play = if_else(self_reported_weekly_play > 50*60, NA, self_reported_weekly_play))

```

## Write clean data files

```{r}
#| label: write-clean-files

# Construct path and directory
path_out <- path("data", "clean")
dir_create(path_out)

# Write files
write_csv(intake, path(path_out, "survey_intake", ext = "csv.gz"))
write_csv(daily, path(path_out, "survey_daily", ext = "csv.gz"))
write_csv(biweekly, path(path_out, "survey_biweekly", ext = "csv.gz"))
write_csv(nintendo_clean, path(path_out, "nintendo", ext = "csv.gz"))
write_csv(steam_clean, path(path_out, "steam", ext = "csv.gz"))
write_csv(xbox_clean, path(path_out, "xbox", ext = "csv.gz"))
write_csv(ios, path(path_out, "ios", ext = "csv.gz"))
write_csv(android, path(path_out, "android", ext = "csv.gz"))
write_csv(dat_cog, path(path_out, "simon", ext = "csv.gz"))
write_csv(timeuse, path(path_out, "timeuse", ext = "csv.gz"))
```
