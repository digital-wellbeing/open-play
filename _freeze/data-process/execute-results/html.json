{
  "hash": "8c825d1fb23ce9b5f59cd7aec356939e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Clean raw data and save outputs\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\nlibrary(dtplyr)\nlibrary(fs)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::between()     masks data.table::between()\n✖ dplyr::filter()      masks stats::filter()\n✖ dplyr::first()       masks data.table::first()\n✖ lubridate::hour()    masks data.table::hour()\n✖ lubridate::isoweek() masks data.table::isoweek()\n✖ dplyr::lag()         masks stats::lag()\n✖ dplyr::last()        masks data.table::last()\n✖ lubridate::mday()    masks data.table::mday()\n✖ lubridate::minute()  masks data.table::minute()\n✖ lubridate::month()   masks data.table::month()\n✖ lubridate::quarter() masks data.table::quarter()\n✖ lubridate::second()  masks data.table::second()\n✖ purrr::transpose()   masks data.table::transpose()\n✖ lubridate::wday()    masks data.table::wday()\n✖ lubridate::week()    masks data.table::week()\n✖ lubridate::yday()    masks data.table::yday()\n✖ lubridate::year()    masks data.table::year()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\n# Load custom functions\nsource(\"R/helpers.R\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintake <- read_csv(\"data/raw/survey_intake_raw.csv.gz\", guess_max = 34000)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 34922 Columns: 88\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (60): pid, country, geo_area, panel_source, cohort, local_timezone, gen...\ndbl  (19): survey_duration, diaries_completed, panels_completed, age, height...\nlgl   (7): qualified, plays_xbox, plays_steam, plays_nintendo, plays_ios, pl...\ndttm  (2): date, nintendo_account_creation_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ndaily <- read_csv(\"data/raw/survey_daily_raw.csv.gz\", guess_max = 10000)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 18829 Columns: 46\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (27): pid, played24hr, social_gaming_4, social_gaming_5, social_gaming_...\ndbl  (17): wave, survey_duration, bangs_1, bangs_2, bangs_3, bangs_4, bangs_...\nlgl   (1): bpnsfs_failed_att_check\ndttm  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nbiweekly <- read_csv(\"data/raw/survey_biweekly_raw.csv.gz\", guess_max = 5000)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 7038 Columns: 158\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (57): pid, bangs_dup_item, positives, problematic_play, gdt_1, gdt_2, g...\ndbl  (74): wave, survey_duration, affective_valence, life_sat, self_reported...\nlgl  (16): bangs_failed_att_check, bfi_2_xs_1, bfi_2_xs_2, bfi_2_xs_3, bfi_2...\ndttm  (1): date\ndate  (3): recent_session1_date, recent_session2_date, recent_session3_date\ntime  (7): mctq_wd_bedtime, mctq_wd_sleep_onset_time, mctq_wd_wake_time, mct...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nnintendo <- read_csv(\"data/raw/telemetry_nintendo_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 756519 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): pid, title_id, operation_mode, device_type, genre, game_ref\ndbl  (2): duration, duration_nintendo\ndttm (3): session_start, session_end, time_of_pull\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nsteam <- read_csv(\"data/raw/telemetry_steam_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 1046613 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): pid, title_id, game_ref\ndbl  (4): hour, steam_id, playtime_forever, playtime_2weeks\ndttm (1): timestamp\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nxbox <- read_csv(\"data/raw/telemetry_xbox_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 4903834 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (11): pid, title_id, title_placement, publisher_id, gameplay_type, genr...\ndbl   (1): duration\ndttm  (3): session_start, session_end, time_of_pull\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nios <- read_csv(\"data/raw/telemetry_ios_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 3633 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): pid\ndbl  (1): total_gaming_minutes\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nandroid <- read_csv(\"data/raw/telemetry_android_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 2576 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): pid\ndbl  (1): total_gaming_minutes\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ntimeuse <- read_csv(\"data/raw/timeuse_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 183280 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): pid, activity, activity_other\ndbl  (1): daily_wave\ndttm (2): start_time, end_time\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ndat_cog <- read_csv(\"data/raw/cognitive_tasks_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 247756 Columns: 38\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (7): pid, task, device_type, stim, location, resp1, resp2\ndbl  (21): wave, rt, trial_index, time_elapsed, block_trial_count, practice,...\nlgl   (9): response, timeout, success, failed_images, failed_audio, failed_v...\ndttm  (1): datetime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n## Preprocess Telemetry \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# non- and idle games lists are in helpers.R\n\n# First, create expanded dataset with scaling calculations for quality metrics\nsteam_expanded_for_metrics <- steam |>\n  filter(!title_id %in% idle_games & !title_id %in% non_games) |>\n  mutate(\n    steam_id = as.character(steam_id),\n    pid = as.character(pid)\n  ) |>\n  arrange(pid, title_id, timestamp) |>\n  group_by(pid, steam_id) |>\n  mutate(\n    time_diff_hours = as.numeric(difftime(\n      timestamp,\n      lag(timestamp),\n      units = \"hours\"\n    )),\n    playtime_diff = playtime_forever - lag(playtime_forever)\n  ) |>\n  ungroup() |>\n  filter(\n    !is.na(playtime_diff),\n    playtime_diff >= 0,\n    !is.na(time_diff_hours),\n    time_diff_hours > 0\n  ) |>\n  mutate(\n    minutes_original = playtime_diff,\n    approximate_session_start = timestamp - minutes(playtime_diff),\n    approximate_session_end = timestamp,\n    datetime_hour_start = floor_date(approximate_session_start, \"hour\")\n  ) |>\n  group_by(pid, steam_id) |>\n  mutate(\n    time_since_last = as.numeric(difftime(\n      approximate_session_start,\n      lag(approximate_session_start),\n      units = \"hours\"\n    )),\n    is_new_session = ifelse(\n      is.na(time_since_last) | time_since_last > 1.5,\n      1,\n      0\n    ),\n    session_group_id = cumsum(is_new_session)\n  ) |>\n  filter(minutes_original > 0, minutes_original <= 480) |>\n  ungroup() |>\n  mutate(\n    h0_utc = floor_date(approximate_session_start, \"hour\"),\n    h1_utc = floor_date(approximate_session_end - seconds(1), \"hour\"),\n    n_hours = as.integer(difftime(h1_utc, h0_utc, units = \"hours\")) + 1\n  ) |>\n  tidyr::uncount(n_hours, .remove = FALSE, .id = \"k\") |>\n  mutate(\n    hour_start_utc = h0_utc + hours(k - 1),\n    hour_end_utc = hour_start_utc + hours(1),\n    minutes_in_hour = pmax(\n      0,\n      as.numeric(difftime(\n        pmin(approximate_session_end, hour_end_utc),\n        pmax(approximate_session_start, hour_start_utc),\n        units = \"mins\"\n      ))\n    )\n  ) |>\n  group_by(pid, hour_start_utc) |>\n  mutate(\n    hour_total = sum(minutes_in_hour),\n    scale_factor = if_else(hour_total > 60, 60 / hour_total, 1.0),\n    minutes_in_hour_corrected = minutes_in_hour * scale_factor\n  )\n\n# Continue processing to create steam_clean\nsteam_clean <- steam_expanded_for_metrics |>\n  ungroup() |>\n  # Aggregate back to session level with corrected minutes\n  group_by(\n    pid,\n    steam_id,\n    session_group_id,\n    approximate_session_start,\n    approximate_session_end\n  ) |>\n  summarise(\n    minutes_original = first(minutes_original),\n    minutes = sum(minutes_in_hour_corrected),\n    datetime_hour_start = first(h0_utc),\n    timestamp = first(timestamp),\n    date = first(date),\n    hour = first(hour),\n    title_id = first(title_id),\n    playtime_forever = first(playtime_forever),\n    playtime_2weeks = first(playtime_2weeks),\n    game_ref = first(game_ref),\n    .groups = \"drop\"\n  ) |>\n  # Calculate corrected session end time based on corrected minutes\n  mutate(\n    approximate_session_end_original = approximate_session_end,\n    approximate_session_end = approximate_session_start + seconds(minutes * 60)\n  ) |>\n  # Remove session_group_id (only used for intermediate calculations)\n  select(-session_group_id)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate Steam data quality metrics using the intermediate expanded data\nsteam_quality_metrics <- steam_expanded_for_metrics |>\n  ungroup() |>\n  mutate(\n    minutes_reduced = minutes_in_hour - minutes_in_hour_corrected,\n    was_scaled = scale_factor < 1.0\n  ) |>\n  summarise(\n    # Metric 1: Player-hour combinations\n    total_player_hours = n_distinct(pid, hour_start_utc),\n    hours_exceeding_60 = n_distinct(\n      pid[hour_total > 60],\n      hour_start_utc[hour_total > 60]\n    ),\n\n    # Metric 2: Hour segments\n    total_segments = n(),\n    segments_scaled = sum(was_scaled),\n\n    # Metric 3: Minutes reduced (only for scaled segments)\n    mean_minutes_reduced = mean(minutes_reduced[was_scaled], na.rm = TRUE),\n    median_minutes_reduced = median(minutes_reduced[was_scaled], na.rm = TRUE),\n\n    # Calculate proportions\n    prop_hours_exceeding = hours_exceeding_60 / total_player_hours,\n    prop_segments_scaled = segments_scaled / total_segments\n  )\n\n# Save metrics for use in index.qmd\nwrite_csv(steam_quality_metrics, \"data/qc/steam_quality_metrics.csv\")\n\n# Clean up temporary variable\nrm(steam_expanded_for_metrics)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Clean Xbox data and keep only active gaming sessions\n# In particular, Xbox has a handful of overlapping sessions, potentially a result of\n# multi-device use or\n# that we need to resolve:\n# - at most one title at any instant\n# - handoff tolerance 'tol_sec' (short overlaps => later-starting title wins)\n# - ≥3 concurrent titles => drop that slice\n\nxbox_clean <- xbox |>\n  arrange(pid, session_start, session_end) |>\n  merge_adjacent_sessions(tol_sec = dseconds(60)) |>\n\n  # filter non-foreground games\n  filter(\n    duration >= 1,\n    title_placement == \"Full\", # in foreground\n    !genres %in% c(\"Shopping\", \"Video\", \"Entertainment\", \"Utilities & tools\"),\n    tolower(title_id) != \"e1924c10-9c91-4652-81b9-8ca0670e77a7\" # home screen\n  ) |>\n\n  # filter suspicious sessions\n  filter(\n    duration <= 480,\n    !is.na(session_start),\n    !is.na(session_end),\n    session_end > session_start,\n  ) |>\n  resolve_overlaps() |>\n  arrange(pid, session_start, session_end)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntol <- dseconds(60)\n\nnintendo_clean <- nintendo |>\n\n  # filter duplicate rows and merge adjacent sessions\n  distinct() |>\n  filter(duration >= 1, pid %in% intake$pid) |>\n  merge_adjacent_sessions(tol_sec = tol) |>\n\n  # filter suspicious sessions\n  filter(\n    duration <= 480,\n    !is.na(session_start),\n    !is.na(session_end),\n    session_end > session_start,\n    session_start < time_of_pull\n  ) |>\n\n  # clean up\n  arrange(pid, title_id, session_start, session_end) |>\n  select(\n    pid,\n    title_id,\n    session_start,\n    session_end,\n    duration,\n    device_type,\n    operation_mode\n  ) |>\n  as_tibble()\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nandroid <- android |>\n  mutate(platform = \"Android\") |>\n  select(pid, platform, day_local = date, minutes = total_gaming_minutes)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nios <- ios |>\n  mutate(platform = \"iOS\") |>\n  select(pid, platform, day_local = date, minutes = total_gaming_minutes)\n```\n:::\n\n\nAs preregistered, we exclude days where total gaming time (across all platforms) exceeds 16 hours, as well as sessions that are more than 8 hours long or whose end time precedes their start time. The exclusions are applied to all relevant datasets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- Build tz map with country and timezone for DST-aware conversion --------\ntz_map <- intake |>\n  mutate(\n    pid = as.character(pid),\n    country,\n    local_timezone,\n    .keep = \"none\"\n  ) |>\n  distinct(pid, .keep_all = TRUE)\n\n# --- Hourly from sessions (Nintendo + Xbox) ---------------------------------\n# Join with timezone map and calculate DST-aware offsets\n# Note: start_local/end_local have UTC labels but local time VALUES (for hour extraction)\nsessions_telemetry <- bind_rows(\n  xbox_clean |> mutate(platform = \"Xbox\"),\n  nintendo_clean |> mutate(platform = \"Nintendo\")\n) |>\n  mutate(pid = as.character(pid)) |>\n  left_join(tz_map, by = \"pid\") |>\n  filter(!is.na(local_timezone)) |>\n  mutate(\n    # Calculate DST-aware offset for each timestamp\n    offset_start = get_dst_offset(session_start, country, local_timezone),\n    offset_end = get_dst_offset(session_end, country, local_timezone),\n    # Add offset to get local time values (keeping UTC label for compatibility)\n    start_local = session_start + offset_start,\n    end_local = session_end + offset_end,\n    duration_min = as.numeric(difftime(\n      session_end,\n      session_start,\n      units = \"mins\"\n    ))\n  ) |>\n  filter(\n    !is.na(session_start),\n    !is.na(session_end),\n    session_end > session_start,\n    duration_min >= 1\n  )\n\nhourly_from_sessions <- sessions_telemetry |>\n  filter(!is.na(start_local), !is.na(end_local)) |>\n  mutate(\n    h0_local = floor_date(start_local, \"hour\"),\n    h1_local = floor_date(end_local - seconds(1), \"hour\"),\n    n_hours = as.integer(difftime(h1_local, h0_local, units = \"hours\")) + 1\n  ) |>\n  filter(!is.na(n_hours), n_hours > 0) |>\n  tidyr::uncount(n_hours, .remove = FALSE, .id = \"k\") |>\n  mutate(\n    hour_start_local = h0_local + hours(k - 1),\n    minutes = pmax(\n      0,\n      as.numeric(difftime(\n        pmin(end_local, hour_start_local + hours(1)),\n        pmax(start_local, hour_start_local),\n        units = \"mins\"\n      ))\n    ),\n    # Convert back to UTC (this preserves the instant, just changes label)\n    hour_start_utc = with_tz(hour_start_local, tzone = \"UTC\")\n  ) |>\n  select(pid, platform, hour_start_local, hour_start_utc, minutes)\n\n# --- Hourly from Steam (already hourly) -------------------------------------\nhourly_from_steam <- steam_clean |>\n  select(pid, datetime_hour_start, minutes) |>\n  mutate(pid = as.character(pid)) |>\n  left_join(tz_map, by = \"pid\") |>\n  filter(!is.na(local_timezone)) |>\n  mutate(\n    platform = \"Steam\",\n    hour_start_utc = datetime_hour_start,\n    # Calculate DST-aware offset and add to get local time values\n    offset = get_dst_offset(datetime_hour_start, country, local_timezone),\n    hour_start_local = datetime_hour_start + offset\n  ) |>\n  select(pid, platform, hour_start_local, hour_start_utc, minutes)\n\nhourly_telemetry <- bind_rows(hourly_from_sessions, hourly_from_steam)\n\n# --- Daily totals (local day) incl. iOS/Android ------------------------------\ndaily_telemetry <- hourly_telemetry |>\n  mutate(day_local = as.Date(hour_start_local)) |>\n  group_by(pid, platform, day_local) |>\n  summarise(minutes = sum(minutes, na.rm = TRUE), .groups = \"drop\")\n\ndaily_ios_android <- bind_rows(\n  ios |>\n    transmute(\n      pid = as.character(pid),\n      platform = \"iOS\",\n      day_local = as.Date(day_local),\n      minutes\n    ),\n  android |>\n    transmute(\n      pid = as.character(pid),\n      platform = \"Android\",\n      day_local = as.Date(day_local),\n      minutes\n    )\n)\n\ndaily_all <- bind_rows(daily_telemetry, daily_ios_android) |>\n  group_by(pid, day_local) |>\n  summarise(total_minutes = sum(minutes, na.rm = TRUE), .groups = \"drop\")\n\nexclusion_days <- daily_all |>\n  filter(total_minutes > 16 * 60) |>\n  select(pid, day_local)\n\n# --- Apply exclusions back to each dataset ----------------------------------\nsteam_clean <- steam_clean |>\n  left_join(tz_map, by = c(\"pid\" = \"pid\")) |>\n  filter(!is.na(local_timezone)) |>\n  mutate(\n    offset = get_dst_offset(datetime_hour_start, country, local_timezone),\n    day_local = as.Date(datetime_hour_start + offset)\n  ) |>\n  anti_join(exclusion_days, by = c(\"pid\", \"day_local\")) |>\n  select(-country, -local_timezone, -offset, -day_local)\n\nxbox_clean <- xbox_clean |>\n  left_join(tz_map, by = \"pid\") |>\n  filter(!is.na(local_timezone)) |>\n  mutate(\n    offset = get_dst_offset(session_start, country, local_timezone),\n    day_local = as.Date(session_start + offset)\n  ) |>\n  anti_join(exclusion_days, by = c(\"pid\", \"day_local\")) |>\n  select(-country, -local_timezone, -offset, -day_local)\n\nnintendo_clean <- nintendo_clean |>\n  left_join(tz_map, by = \"pid\") |>\n  filter(!is.na(local_timezone)) |>\n  mutate(\n    offset = get_dst_offset(session_start, country, local_timezone),\n    day_local = as.Date(session_start + offset)\n  ) |>\n  anti_join(exclusion_days, by = c(\"pid\", \"day_local\")) |>\n  select(-country, -local_timezone, -offset, -day_local)\n\nios <- ios |>\n  mutate(pid = as.character(pid), day_local = as.Date(day_local)) |>\n  anti_join(exclusion_days, by = c(\"pid\", \"day_local\"))\n\nandroid <- android |>\n  mutate(pid = as.character(pid), day_local = as.Date(day_local)) |>\n  anti_join(exclusion_days, by = c(\"pid\", \"day_local\"))\n```\n:::\n\n\n## Preprocess Survey Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintake_clean <- intake |>\n\n  # self-reported playtime over 50 hours per week is treated as NA\n  mutate(\n    self_reported_weekly_play = if_else(\n      self_reported_weekly_play > 50 * 60,\n      NA,\n      self_reported_weekly_play\n    )\n  ) |>\n\n  # linked platforms are exported only as console (\"core\") platforms - update to include whether people linked iOS as well\n  rowwise() |>\n  mutate(\n    linked_platforms = {\n      v <- unique(trimws(strsplit(coalesce(linked_platforms, \"\"), \",\")[[1]]))\n      v <- v[v != \"\"]\n      if (pid %in% ios$pid) {\n        v <- union(v, \"iOS\")\n      }\n      if (pid %in% android$pid) {\n        v <- union(v, \"Android\")\n      }\n      na_if(paste(v, collapse = \", \"), \"\")\n    }\n  ) |>\n  ungroup()\n```\n:::\n\n\nIn both the daily and biweekly surveys, we have cases of (1) duplicate survey entries within a single day, such that only the first should be considered, (2) participants with errors in the recorded \"wave\" coding which should be recoded based on their start date, and (3) \"false starts\" that should be ignored in favor of a later start date and contiguous surveys from Day 1 to 30. We handle each of those cases here. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndaily_clean <- daily |>\n  filter(!bpnsfs_failed_att_check) |>\n  mutate(baseline_day = as.Date(date) - if_else(hour(date) < 4, 1L, 0L)) %>%\n  {\n    base <- .\n\n    best <- base |>\n      distinct(pid, ideal_baseline_day = baseline_day) |> # candidates = observed days only\n      inner_join(\n        base |> select(pid, baseline_day, date),\n        by = \"pid\",\n        relationship = \"many-to-many\"\n      ) |>\n      mutate(dw = 1L + as.integer(baseline_day - ideal_baseline_day)) |>\n      filter(dplyr::between(dw, 1L, 30L)) |>\n      group_by(pid, ideal_baseline_day, dw) |>\n      slice_min(date, n = 1, with_ties = FALSE) |> # one per (pid, candidate, wave)\n      ungroup() |>\n      count(pid, ideal_baseline_day, name = \"n_valid\") |>\n      group_by(pid) |>\n      arrange(desc(n_valid), ideal_baseline_day) |>\n      slice(1) |>\n      ungroup()\n\n    base |>\n      left_join(best, by = \"pid\") |>\n      mutate(\n        original_wave = wave,\n        wave = 1L + as.integer(baseline_day - ideal_baseline_day)\n      ) |>\n      filter(dplyr::between(wave, 1L, 30L)) |>\n      group_by(pid, wave) |>\n      slice_min(date, n = 1, with_ties = FALSE) |>\n      ungroup()\n  }\n\nideal <- daily_clean |>\n  distinct(pid, ideal_baseline_day)\n\ndropped <- daily |>\n  mutate(baseline_day = as.Date(date) - if_else(hour(date) < 4, 1L, 0L)) |>\n  anti_join(daily_clean |> select(pid, date), by = c(\"pid\", \"date\")) |>\n  left_join(ideal, by = \"pid\") |>\n  mutate(\n    derived_wave = if_else(\n      !is.na(ideal_baseline_day),\n      1L + as.integer(baseline_day - ideal_baseline_day),\n      NA_integer_\n    ),\n    drop_reason = case_when(\n      is.na(ideal_baseline_day) ~ \"no ideal baseline (check pid)\",\n      !dplyr::between(\n        derived_wave,\n        1L,\n        30L\n      ) ~ \"outside 1..30 after reassignment\",\n      TRUE ~ \"duplicate within (pid, wave) — later submission dropped\"\n    )\n  ) |>\n  arrange(pid, date) |>\n  select(pid, date, baseline_day, ideal_baseline_day, derived_wave, drop_reason)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbiweekly_clean <- biweekly |>\n  filter(!bangs_failed_att_check) |>\n  mutate(baseline_day = as.Date(date) - if_else(hour(date) < 4, 1L, 0L)) %>%\n  {\n    base <- .\n\n    # candidates = observed days only; score by count of valid panel waves (1..30) within ±7d of 14d grid\n    best <- base |>\n      distinct(pid, ideal_baseline_day = baseline_day) |>\n      inner_join(\n        base |> select(pid, baseline_day, date),\n        by = \"pid\",\n        relationship = \"many-to-many\"\n      ) |>\n      mutate(\n        delta_days = as.integer(baseline_day - ideal_baseline_day),\n        dpw = 1L + as.integer(round(delta_days / 14)) # derived panel wave (nearest 14d)\n      ) |>\n      filter(\n        dplyr::between(dpw, 1L, 30L),\n        abs(delta_days - 14L * (dpw - 1L)) <= 7L # within ±7 days of 14d grid\n      ) |>\n      group_by(pid, ideal_baseline_day, dpw) |>\n      slice_min(date, n = 1, with_ties = FALSE) |>\n      ungroup() |>\n      count(pid, ideal_baseline_day, name = \"n_valid\") |>\n      group_by(pid) |>\n      arrange(desc(n_valid), ideal_baseline_day) |>\n      slice(1) |>\n      ungroup()\n\n    base |>\n      left_join(best, by = \"pid\") |>\n      mutate(\n        original_wave = wave,\n        delta_days_final = as.integer(baseline_day - ideal_baseline_day),\n        derived_wave = 1L + as.integer(round(delta_days_final / 14)),\n        wave = derived_wave\n      ) |>\n      filter(\n        dplyr::between(wave, 1L, 30L),\n        abs(delta_days_final - 14L * (wave - 1L)) <= 7L # enforce validity post-assign\n      ) |>\n      group_by(pid, wave) |>\n      slice_min(date, n = 1, with_ties = FALSE) |>\n      ungroup() |>\n      select(\n        ideal_baseline_day,\n        baseline_day,\n        date,\n        original_wave,\n        derived_wave,\n        wave,\n        everything()\n      )\n  }\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assign quality flags to daily time use diaries\n\n# Calculate duration and flags\ntimeuse_flags <- timeuse |>\n  mutate(\n    # Use already-parsed POSIXct columns directly (read_csv handles ISO 8601)\n    # Duration of each individual activity\n    duration_min = as.numeric(difftime(end_time, start_time, units = \"mins\"))\n  ) |>\n  group_by(pid, daily_wave) |>\n  summarise(\n    # Sum of all activity durations in this daily_wave\n    total_minutes = sum(duration_min, na.rm = TRUE),\n    has_sleep = any(str_to_lower(activity) == \"sleep\", na.rm = TRUE),\n    has_personal_care = any(\n      str_to_lower(activity) == \"personal care\",\n      na.rm = TRUE\n    ),\n    .groups = \"drop\"\n  ) |>\n  mutate(\n    quality_flag = case_when(\n      !has_sleep &\n        !has_personal_care ~ \"Low quality: no sleep and no personal care\",\n      !has_sleep ~ \"Low quality: no sleep\",\n      !has_personal_care ~ \"Low quality: no personal care\",\n      total_minutes < 1350 ~ \"Low quality: >90 minutes unaccounted for\", # 1440 - 90 = 1350\n      TRUE ~ \"Good quality\"\n    )\n  ) |>\n  select(pid, daily_wave, quality_flag)\n\n# Add flags to dataset\ntimeuse_clean <- timeuse |>\n  left_join(timeuse_flags, by = c(\"pid\", \"daily_wave\")) |>\n  arrange(pid, daily_wave, start_time)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Only include non-pilot-study participants who qualified at intake\ndat_cog <- dat_cog |>\n  filter(\n    pid %in%\n      unique(intake$pid[intake$qualified & intake$cohort != \"Platform Pilot\"]),\n    wave %in% c(1, 3, 5) # only waves with cognitive data\n  )\n\n# Take relevant variables and observations only\n# `score_final` is automatically added to all rows so distinct works here\ndat_cog <- dat_cog |>\n  filter(practice == 0, task == \"simon\") |>\n  add_count(pid, wave, name = \"n_trials\") |>\n  distinct(\n    pid,\n    wave,\n    datetime,\n    device_type,\n    n_trials,\n    score_final,\n    meanrt_final\n  )\n\ndat_cog <- dat_cog |>\n  mutate(wave = factor(wave)) |>\n  arrange(wave, pid)\n```\n:::\n\n\n\n## Write clean data files\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct path and directory\npath_out <- path(\"data\", \"clean\")\ndir_create(path_out)\n\n# Write files\nwrite_csv(intake_clean, path(path_out, \"survey_intake\", ext = \"csv.gz\"))\nwrite_csv(daily_clean, path(path_out, \"survey_daily\", ext = \"csv.gz\"))\nwrite_csv(biweekly_clean, path(path_out, \"survey_biweekly\", ext = \"csv.gz\"))\nwrite_csv(nintendo_clean, path(path_out, \"nintendo\", ext = \"csv.gz\"))\nwrite_csv(steam_clean, path(path_out, \"steam\", ext = \"csv.gz\"))\nwrite_csv(xbox_clean, path(path_out, \"xbox\", ext = \"csv.gz\"))\nwrite_csv(ios, path(path_out, \"ios\", ext = \"csv.gz\"))\nwrite_csv(android, path(path_out, \"android\", ext = \"csv.gz\"))\nwrite_csv(timeuse_clean, path(path_out, \"timeuse\", ext = \"csv.gz\"))\nwrite_csv(dat_cog, path(path_out, \"simon\", ext = \"csv.gz\"))\n```\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}