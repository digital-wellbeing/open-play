{
  "hash": "07bd07842c83d94f20795d8fc556bc1f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Clean raw data and save outputs\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\nlibrary(dtplyr)\nlibrary(fs)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::between()     masks data.table::between()\n✖ dplyr::filter()      masks stats::filter()\n✖ dplyr::first()       masks data.table::first()\n✖ lubridate::hour()    masks data.table::hour()\n✖ lubridate::isoweek() masks data.table::isoweek()\n✖ dplyr::lag()         masks stats::lag()\n✖ dplyr::last()        masks data.table::last()\n✖ lubridate::mday()    masks data.table::mday()\n✖ lubridate::minute()  masks data.table::minute()\n✖ lubridate::month()   masks data.table::month()\n✖ lubridate::quarter() masks data.table::quarter()\n✖ lubridate::second()  masks data.table::second()\n✖ purrr::transpose()   masks data.table::transpose()\n✖ lubridate::wday()    masks data.table::wday()\n✖ lubridate::week()    masks data.table::week()\n✖ lubridate::yday()    masks data.table::yday()\n✖ lubridate::year()    masks data.table::year()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\n# Load custom functions\nsource(\"R/helpers.R\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintake <- read_csv(\"data/raw/survey_intake_raw.csv.gz\", guess_max = 34000)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 34643 Columns: 88\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (60): pid, country, geo_area, panel_source, cohort, local_timezone, gen...\ndbl  (19): survey_duration, diaries_completed, panels_completed, age, height...\nlgl   (7): qualified, plays_xbox, plays_steam, plays_nintendo, plays_ios, pl...\ndttm  (2): date, nintendo_account_creation_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ndaily <- read_csv(\"data/raw/survey_daily_raw.csv.gz\", guess_max = 10000)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 18280 Columns: 46\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (27): pid, played24hr, social_gaming_4, social_gaming_5, social_gaming_...\ndbl  (17): wave, survey_duration, bangs_1, bangs_2, bangs_3, bangs_4, bangs_...\nlgl   (1): bpnsfs_failed_att_check\ndttm  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nbiweekly <- read_csv(\"data/raw/survey_biweekly_raw.csv.gz\", guess_max = 5000)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 7038 Columns: 158\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (57): pid, bangs_dup_item, positives, problematic_play, gdt_1, gdt_2, g...\ndbl  (74): wave, survey_duration, affective_valence, life_sat, self_reported...\nlgl  (16): bangs_failed_att_check, bfi_2_xs_1, bfi_2_xs_2, bfi_2_xs_3, bfi_2...\ndttm  (1): date\ndate  (3): recent_session1_date, recent_session2_date, recent_session3_date\ntime  (7): mctq_wd_bedtime, mctq_wd_sleep_onset_time, mctq_wd_wake_time, mct...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nnintendo <- read_csv(\"data/raw/telemetry_nintendo_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 756519 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): pid, title_id, operation_mode, device_type, genre, game_ref\ndbl  (2): duration, duration_nintendo\ndttm (3): session_start, session_end, time_of_pull\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nsteam <- read_csv(\"data/raw/telemetry_steam_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 1046589 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): pid, title_id, game_ref\ndbl  (4): hour, steam_id, playtime_forever, playtime_2weeks\ndttm (1): timestamp\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nxbox <- read_csv(\"data/raw/telemetry_xbox_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 4903834 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (11): pid, title_id, title_placement, publisher_id, gameplay_type, genr...\ndbl   (1): duration\ndttm  (3): session_start, session_end, time_of_pull\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nios <- read_csv(\"data/raw/telemetry_ios_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 3633 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): pid\ndbl  (1): total_gaming_minutes\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nandroid <- read_csv(\"data/raw/telemetry_android_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 2576 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): pid\ndbl  (1): total_gaming_minutes\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ntimeuse <- read_csv(\"data/raw/timeuse_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 183280 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): pid, activity, activity_other\ndbl  (1): daily_wave\ndttm (2): start_time, end_time\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ndat_cog <- read_csv(\"data/raw/cognitive_tasks_raw.csv.gz\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 247756 Columns: 38\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (7): pid, task, device_type, stim, location, resp1, resp2\ndbl  (21): wave, rt, trial_index, time_elapsed, block_trial_count, practice,...\nlgl   (9): response, timeout, success, failed_images, failed_audio, failed_v...\ndttm  (1): datetime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n## Preprocess Telemetry \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# non- and idle games lists are in helpers.R\n\n# Arrange data to calculate differences correctly\nsteam_clean <- steam |>\n  # get rid of idle games and non-games, see list in R/helpers.R\n  filter(!title_id %in% idle_games & !title_id %in% non_games) |>\n  mutate(\n    steam_id = as.character(steam_id),\n    pid = as.character(pid)\n  ) |>\n  arrange(pid, title_id, timestamp) |>\n  group_by(pid, steam_id) |>\n  mutate(\n    # Calculate time difference between consecutive polls for the same user/game\n    time_diff_hours = as.numeric(difftime(\n      timestamp,\n      lag(timestamp),\n      units = \"hours\"\n    )),\n    # Calculate playtime difference\n    playtime_diff = playtime_forever - lag(playtime_forever)\n  ) |>\n  ungroup() |>\n  # Filter out invalid calculations:\n  # - First entry for each user/game (lag is NA)\n  # - Negative playtime difference (data anomaly or reset?)\n  filter(\n    !is.na(playtime_diff),\n    playtime_diff >= 0,\n    !is.na(time_diff_hours),\n    time_diff_hours > 0\n  ) |>\n  # Calculate approximate session start and end times\n  # This approximation assumes gaming occurred immediately before the poll\n  # by subtracting the playtime duration from the poll timestamp\n  mutate(\n    minutes = playtime_diff,\n    # Approximate session start: poll timestamp minus playtime duration\n    approximate_session_start = timestamp - minutes(playtime_diff),\n    # Approximate session end: the poll timestamp (when playtime change was detected)\n    approximate_session_end = timestamp,\n    # Floor to hour for hourly aggregation\n    datetime_hour_start = floor_date(approximate_session_start, \"hour\")\n  ) |>\n  group_by(pid, steam_id) |>\n  # Calculate difference from previous session and identify new sessions\n  mutate(\n    time_since_last = as.numeric(difftime(\n      approximate_session_start,\n      lag(approximate_session_start),\n      units = \"hours\"\n    )),\n    is_new_session = ifelse(is.na(time_since_last) | time_since_last > 1.5, 1, 0),\n    session_group_id = cumsum(is_new_session)\n  ) |>\n  filter(minutes > 0, minutes <= 480) |>\n  ungroup()\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Clean Xbox data and keep only active gaming sessions\n# In particular, Xbox has a handful of overlapping sessions, potentially a result of\n# multi-device use or\n# that we need to resolve:\n# - at most one title at any instant\n# - handoff tolerance 'tol_sec' (short overlaps => later-starting title wins)\n# - ≥3 concurrent titles => drop that slice\n\nxbox_clean <- xbox |>\n  arrange(pid, session_start, session_end) |>\n  merge_adjacent_sessions(tol_sec = dseconds(60)) |>\n\n  # filter non-foreground games\n  filter(\n    duration >= 1,\n    title_placement == \"Full\", # in foreground\n    !genres %in% c(\"Shopping\", \"Video\", \"Entertainment\", \"Utilities & tools\"),\n    tolower(title_id) != \"e1924c10-9c91-4652-81b9-8ca0670e77a7\" # home screen\n  ) |>\n\n  # filter suspicious sessions\n  filter(\n    duration <= 480,\n    !is.na(session_start),\n    !is.na(session_end),\n    session_end > session_start,\n  ) |>\n  resolve_overlaps() |>\n  arrange(pid, session_start, session_end)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntol <- dseconds(60)\n\nnintendo_clean <- nintendo |>\n\n  # filter duplicate rows and merge adjacent sessions\n  distinct() |>\n  filter(duration >= 1, pid %in% intake$pid) |>\n  merge_adjacent_sessions(tol_sec = tol) |>\n\n  # filter suspicious sessions\n  filter(\n    duration <= 480,\n    !is.na(session_start),\n    !is.na(session_end),\n    session_end > session_start,\n    session_start < time_of_pull\n  ) |>\n\n  # clean up\n  arrange(pid, title_id, session_start, session_end) |>\n  select(\n    pid,\n    title_id,\n    session_start,\n    session_end,\n    duration,\n    device_type,\n    operation_mode\n  ) |>\n  as_tibble()\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Only include non-pilot-study participants who qualified at intake\ndat_cog <- dat_cog |>\n  filter(\n    pid %in%\n      unique(intake$pid[intake$qualified & intake$cohort != \"Platform Pilot\"])\n  )\n\n# Take relevant variables and observations only\n# `score_final` is automatically added to all rows so distinct works here\ndat_cog <- dat_cog |>\n  filter(practice == 0, task == \"simon\") |>\n  add_count(pid, wave, name = \"n_trials\") |>\n  distinct(\n    pid,\n    wave,\n    datetime,\n    device_type,\n    n_trials,\n    score_final,\n    meanrt_final\n  )\n\ndat_cog <- dat_cog |>\n  mutate(wave = factor(wave)) |>\n  arrange(wave, pid)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nandroid <- android |>\n  mutate(platform = \"Android\") |>\n  select(pid, platform, day_local = date, minutes = total_gaming_minutes)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nios <- ios |>\n  mutate(platform = \"iOS\") |>\n  select(pid, platform, day_local = date, minutes = total_gaming_minutes)\n```\n:::\n\n\nAs preregistered, we exclude days where total gaming time (across all platforms) exceeds 16 hours, as well as sessions that are more than 8 hours long or whose end time precedes their start time. The exclusions are applied to all relevant datasets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- Build tz map (as in your script) ---------------------------------------\ntz_map <- intake |>\n  mutate(\n    pid = as.character(pid),\n    off = offset_secs(local_timezone),\n    .keep = \"none\"\n  ) |>\n  distinct(pid, .keep_all = TRUE)\n\n# --- Hourly from sessions (Nintendo + Xbox) ---------------------------------\nsessions_telemetry <- bind_rows(\n  xbox_clean |> mutate(platform = \"Xbox\"),\n  nintendo_clean |> mutate(platform = \"Nintendo\")\n) |>\n  mutate(pid = as.character(pid)) |>\n  left_join(tz_map, by = \"pid\") |>\n  mutate(\n    start_local = session_start + off,\n    end_local = session_end + off,\n    duration_min = as.numeric(difftime(\n      session_end,\n      session_start,\n      units = \"mins\"\n    ))\n  ) |>\n  filter(\n    !is.na(session_start),\n    !is.na(session_end),\n    session_end > session_start,\n    duration_min >= 1\n  )\n\nhourly_from_sessions <- sessions_telemetry |>\n  filter(!is.na(off)) |>\n  mutate(\n    h0_local = floor_date(start_local, \"hour\"),\n    h1_local = floor_date(end_local - seconds(1), \"hour\"),\n    n_hours = as.integer(difftime(h1_local, h0_local, units = \"hours\")) + 1\n  ) |>\n  tidyr::uncount(n_hours, .remove = FALSE, .id = \"k\") |>\n  mutate(\n    hour_start_local = h0_local + hours(k - 1),\n    minutes = pmax(\n      0,\n      as.numeric(difftime(\n        pmin(end_local, hour_start_local + hours(1)),\n        pmax(start_local, hour_start_local),\n        units = \"mins\"\n      ))\n    ),\n    hour_start_utc = hour_start_local - off\n  ) |>\n  select(pid, platform, hour_start_local, hour_start_utc, minutes)\n\n# --- Hourly from Steam (already hourly) -------------------------------------\nhourly_from_steam <- steam_clean |>\n  select(pid, datetime_hour_start, minutes) |>\n  mutate(pid = as.character(pid)) |>\n  left_join(tz_map, by = \"pid\") |>\n  mutate(\n    platform = \"Steam\",\n    hour_start_utc = datetime_hour_start,\n    hour_start_local = datetime_hour_start + off\n  ) |>\n  select(pid, platform, hour_start_local, hour_start_utc, minutes)\n\nhourly_telemetry <- bind_rows(hourly_from_sessions, hourly_from_steam)\n\n# --- Daily totals (local day) incl. iOS/Android ------------------------------\ndaily_telemetry <- hourly_telemetry |>\n  mutate(day_local = as.Date(hour_start_local)) |>\n  group_by(pid, platform, day_local) |>\n  summarise(minutes = sum(minutes, na.rm = TRUE), .groups = \"drop\")\n\ndaily_ios_android <- bind_rows(\n  ios |>\n    transmute(\n      pid = as.character(pid),\n      platform = \"iOS\",\n      day_local = as.Date(day_local),\n      minutes\n    ),\n  android |>\n    transmute(\n      pid = as.character(pid),\n      platform = \"Android\",\n      day_local = as.Date(day_local),\n      minutes\n    )\n)\n\ndaily_all <- bind_rows(daily_telemetry, daily_ios_android) |>\n  group_by(pid, day_local) |>\n  summarise(total_minutes = sum(minutes, na.rm = TRUE), .groups = \"drop\")\n\nexclusion_days <- daily_all |>\n  filter(total_minutes > 16 * 60) |>\n  select(pid, day_local)\n\n# --- Apply exclusions back to each dataset ----------------------------------\nsteam_clean <- steam_clean |>\n  left_join(tz_map, by = c(\"pid\" = \"pid\")) |>\n  mutate(day_local = as.Date(datetime_hour_start + off)) |>\n  anti_join(exclusion_days, by = c(\"pid\", \"day_local\")) |>\n  select(-off, -day_local)\n\nxbox_clean <- xbox_clean |>\n  left_join(tz_map, by = \"pid\") |>\n  mutate(day_local = as.Date(session_start + off)) |>\n  anti_join(exclusion_days, by = c(\"pid\", \"day_local\")) |>\n  select(-off, -day_local)\n\nnintendo_clean <- nintendo_clean |>\n  left_join(tz_map, by = \"pid\") |>\n  mutate(day_local = as.Date(session_start + off)) |>\n  anti_join(exclusion_days, by = c(\"pid\", \"day_local\")) |>\n  select(-off, -day_local)\n\nios <- ios |>\n  mutate(pid = as.character(pid), day_local = as.Date(day_local)) |>\n  anti_join(exclusion_days, by = c(\"pid\", \"day_local\"))\n\nandroid <- android |>\n  mutate(pid = as.character(pid), day_local = as.Date(day_local)) |>\n  anti_join(exclusion_days, by = c(\"pid\", \"day_local\"))\n```\n:::\n\n\n## Preprocess Survey Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintake_clean <- intake |>\n\n  # self-reported playtime over 50 hours per week is treated as NA\n  mutate(\n    self_reported_weekly_play = if_else(\n      self_reported_weekly_play > 50 * 60,\n      NA,\n      self_reported_weekly_play\n    )\n  ) |>\n\n  # linked platforms are exported only as console (\"core\") platforms - update to include whether people linked iOS as well\n  rowwise() |>\n  mutate(\n    linked_platforms = {\n      v <- unique(trimws(strsplit(coalesce(linked_platforms, \"\"), \",\")[[1]]))\n      v <- v[v != \"\"]\n      if (pid %in% ios$pid) {\n        v <- union(v, \"iOS\")\n      }\n      if (pid %in% android$pid) {\n        v <- union(v, \"Android\")\n      }\n      na_if(paste(v, collapse = \", \"), \"\")\n    }\n  ) |>\n  ungroup()\n```\n:::\n\n\nIn both the daily and biweekly surveys, we have cases of (1) duplicate survey entries within a single day, such that only the first should be considered, (2) participants with errors in the recorded \"wave\" coding which should be recoded based on their start date, and (3) \"false starts\" that should be ignored in favor of a later start date and contiguous surveys from Day 1 to 30. We handle each of those cases here. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndaily_clean <- daily |>\n  filter(!bpnsfs_failed_att_check) |>\n  mutate(baseline_day = as.Date(date) - if_else(hour(date) < 4, 1L, 0L)) %>%\n  {\n    base <- .\n\n    best <- base |>\n      distinct(pid, ideal_baseline_day = baseline_day) |> # candidates = observed days only\n      inner_join(\n        base |> select(pid, baseline_day, date),\n        by = \"pid\",\n        relationship = \"many-to-many\"\n      ) |>\n      mutate(dw = 1L + as.integer(baseline_day - ideal_baseline_day)) |>\n      filter(between(dw, 1L, 30L)) |>\n      group_by(pid, ideal_baseline_day, dw) |>\n      slice_min(date, n = 1, with_ties = FALSE) |> # one per (pid, candidate, wave)\n      ungroup() |>\n      count(pid, ideal_baseline_day, name = \"n_valid\") |>\n      group_by(pid) |>\n      arrange(desc(n_valid), ideal_baseline_day) |>\n      slice(1) |>\n      ungroup()\n\n    base |>\n      left_join(best, by = \"pid\") |>\n      mutate(\n        original_wave = wave,\n        wave = 1L + as.integer(baseline_day - ideal_baseline_day)\n      ) |>\n      filter(between(wave, 1L, 30L)) |>\n      group_by(pid, wave) |>\n      slice_min(date, n = 1, with_ties = FALSE) |>\n      ungroup()\n  }\n\nideal <- daily_clean |>\n  distinct(pid, ideal_baseline_day)\n\ndropped <- daily |>\n  mutate(baseline_day = as.Date(date) - if_else(hour(date) < 4, 1L, 0L)) |>\n  anti_join(daily_clean |> select(pid, date), by = c(\"pid\", \"date\")) |>\n  left_join(ideal, by = \"pid\") |>\n  mutate(\n    derived_wave = if_else(\n      !is.na(ideal_baseline_day),\n      1L + as.integer(baseline_day - ideal_baseline_day),\n      NA_integer_\n    ),\n    drop_reason = case_when(\n      is.na(ideal_baseline_day) ~ \"no ideal baseline (check pid)\",\n      !between(derived_wave, 1L, 30L) ~ \"outside 1..30 after reassignment\",\n      TRUE ~ \"duplicate within (pid, wave) — later submission dropped\"\n    )\n  ) |>\n  arrange(pid, date) |>\n  select(pid, date, baseline_day, ideal_baseline_day, derived_wave, drop_reason)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbiweekly_clean <- biweekly |>\n  filter(!bangs_failed_att_check) |>\n  mutate(baseline_day = as.Date(date) - if_else(hour(date) < 4, 1L, 0L)) %>%\n  {\n    base <- .\n\n    # candidates = observed days only; score by count of valid panel waves (1..30) within ±7d of 14d grid\n    best <- base |>\n      distinct(pid, ideal_baseline_day = baseline_day) |>\n      inner_join(\n        base |> select(pid, baseline_day, date),\n        by = \"pid\",\n        relationship = \"many-to-many\"\n      ) |>\n      mutate(\n        delta_days = as.integer(baseline_day - ideal_baseline_day),\n        dpw = 1L + as.integer(round(delta_days / 14)) # derived panel wave (nearest 14d)\n      ) |>\n      filter(\n        between(dpw, 1L, 30L),\n        abs(delta_days - 14L * (dpw - 1L)) <= 7L # within ±7 days of 14d grid\n      ) |>\n      group_by(pid, ideal_baseline_day, dpw) |>\n      slice_min(date, n = 1, with_ties = FALSE) |>\n      ungroup() |>\n      count(pid, ideal_baseline_day, name = \"n_valid\") |>\n      group_by(pid) |>\n      arrange(desc(n_valid), ideal_baseline_day) |>\n      slice(1) |>\n      ungroup()\n\n    base |>\n      left_join(best, by = \"pid\") |>\n      mutate(\n        original_wave = wave,\n        delta_days_final = as.integer(baseline_day - ideal_baseline_day),\n        derived_wave = 1L + as.integer(round(delta_days_final / 14)),\n        wave = derived_wave\n      ) |>\n      filter(\n        between(wave, 1L, 30L),\n        abs(delta_days_final - 14L * (wave - 1L)) <= 7L # enforce validity post-assign\n      ) |>\n      group_by(pid, wave) |>\n      slice_min(date, n = 1, with_ties = FALSE) |>\n      ungroup() |>\n      select(\n        ideal_baseline_day,\n        baseline_day,\n        date,\n        original_wave,\n        derived_wave,\n        wave,\n        everything()\n      )\n  }\n```\n:::\n\n\n## Write clean data files\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Construct path and directory\npath_out <- path(\"data\", \"clean\")\ndir_create(path_out)\n\n# Write files\nwrite_csv(intake_clean, path(path_out, \"survey_intake\", ext = \"csv.gz\"))\nwrite_csv(daily_clean, path(path_out, \"survey_daily\", ext = \"csv.gz\"))\nwrite_csv(biweekly_clean, path(path_out, \"survey_biweekly\", ext = \"csv.gz\"))\nwrite_csv(nintendo_clean, path(path_out, \"nintendo\", ext = \"csv.gz\"))\nwrite_csv(\n  steam_clean |>\n    select(\n      pid,\n      timestamp,\n      date,\n      hour,\n      title_id,\n      steam_id,\n      playtime_forever,\n      playtime_2weeks,\n      minutes,\n      game_ref\n    ),\n  path(path_out, \"steam\", ext = \"csv.gz\")\n)\nwrite_csv(xbox_clean, path(path_out, \"xbox\", ext = \"csv.gz\"))\nwrite_csv(ios, path(path_out, \"ios\", ext = \"csv.gz\"))\nwrite_csv(android, path(path_out, \"android\", ext = \"csv.gz\"))\nwrite_csv(dat_cog, path(path_out, \"simon\", ext = \"csv.gz\"))\nwrite_csv(timeuse, path(path_out, \"timeuse\", ext = \"csv.gz\"))\n```\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}